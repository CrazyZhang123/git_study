---
created: 2024-11-04T21:21
updated: 2025-02-22T12:31
---

### Sophisticated lnput
复杂的输入

- a vector    ----  数值或类别
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104212338.png)
- 一组向量==(可变长==)  ----  数值或类别
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104212436.png)


#### 输入类型
##### nlp中
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104215032.png)

- 独热编码 one hot
- embedding 对 one hot进行压缩


##### 音频
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104215117.png)
采样方式有很多
**1s就是100 frame 帧**

##### 图 graph


![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104215506.png)


#### 输出类型

- 每个向量都要有对应的label 标签。
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104224238.png)
- 整个序列生成一个 label

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104224348.png)

- seq2seq 输出不确定长度label
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104224443.png)


#### 每个向量都要有对应的label 标签。 就是 sequence labeling

全用全连接层就会出现相同的输入但是输出要不一样，但全连接层无法做到

- FC考虑周围的向量 设置window的大小， 可以解决一部分问题。
- 但是如果输入和输出的长度不一致，window的大小无法确定，就无法继续训练了。
- 换一种考虑更多context上下文的方式
--> 引入自注意力机制

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104225051.png)
- 先将整个文本输入到 self-attention里面(考虑整个上下文)，生成对应的向量，再输入到FC中去标记。
- self-attention 可以多次使用。
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104225248.png)
paper: [attention is all you need](https://arxiv.org/abs/1706.03762)
- 他不是最早提出来的，他是发扬光大的

### self attention 原理

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104225951.png)

##### 1. 找到和a1关联relevant的大小 a

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104230151.png)
- **Dot-product** :左边最常用，两个权重矩阵和对应的矩阵相乘后，然后点乘的到  最终的关联 relevant a
- **additive**: 乘完后，矩阵相加过tanh激活函数再过transformer得到 relevant a

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104230701.png)
- 
- 可以不用 softmax ，但最常使用的是 softmax

##### 2. 基于注意力分数抽取信息
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104230941.png)
- 计算 b

![image.png|508](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104232033.png)

##### QKV来源
==q,k,v都是a去乘以矩阵得到的，这就是为什么叫自注意力机制==

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104232358.png)

##### A矩阵——注意力矩阵
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104232624.png)

##### O矩阵  所有的b值
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104232858.png)

##### self attention 参数就是 qkv矩阵

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104233048.png)


##### Multi-head self-attention 多头注意力机制

- 来源：==只用一个q无法完整的表达相关性 ，所以引入多个q，这就是多头==
- 其他计算和前面一样

![image.png|487](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104233410.png)



![image.png|344](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241104233521.png)
bi,1 bi,2接在一起，再通过一个transformer形成O矩阵，形成真正的bi


### Positional Encoding 位置编码

#### 为什么自注意力机制需要位置编码

**在一个句子里，各个词的前后关系对这个句子的意义是有影响的，这种影响不仅体现在语法方面，而且还体现在语义方面。**

从上一小节自注意力机制简介中，我们知道当一个序列输入进一个自注意力模块时，由于序列中所有的 Token 是同时进入并被处理的，如果不提供位置信息，那么这个序列的相同的 Token 对自注意力模块来说就不会有语法和语义上的差别，他们会产生相同的输出。比如，图 1.2 里的词 “我” 出现 2 次，它们的初始表达向量是一样的，如果不加入位置信息，那么自注意力模块产生的对它们的关注度是一样的，或者模型产生的新的表达式一样的。所以，我们需要在输入序列里人为加入每个 token 的位置信息。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241109163942.png)

        图 1.2 自注意力机制本身并不理会输入序列里各个词的位置

###### 1.2 位置编码

为了给自注意力模块加入位置编码，我们大体有两中选择：

- 想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法。

- 想办法微调一下 Attention 结构，使得它有能力分辨不同位置的 Token，这构成了相对位置编码的一般做法。

- No position information in self-attention. ==目前的注意力机制缺少对位置信息的收集==
- Each position has a unique positional vector  $e^i$
- **handcrafted** 手工设置的 paper: attention is all you need.
- **learned from paper**
- 尚待研究！！
![image.png|229](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241109162450.png)![image.png|258](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241109162521.png)
#### 很多生成位置编码方式
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110104013.png)

#### many applications
##### nlp领域
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110104200.png)
- transformer  https://arxiv.org/abs/1706.03762
- bert https://arxiv.org/abs/1810.04805

##### speech领域 语音领域
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110104806.png)
- 思路是 如果按照之前的方式，可能我们需要计算的注意力矩阵维度就会太大，需要资源太多，耗时长。
- 因此我们计算注意力矩阵A‘就不用关注所有的单词，关注一个小范围就好，这个范围的大小取决于对问题的理解，也就是 **truncated Self-attention 截断自注意力机制**

##### image 领域
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110105301.png)

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110105413.png)
- Self-Attention GAN https://arxiv.org/abs/1805.08318
- DEtection Transformer (DETR) https://arxiv.org/abs/2005.12872

##### Graph 图领域

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110111646.png)

- 考虑边 edge: 只计算连接的节点。
- This is one type of Graph Neural Network (GNN).


##### self attention vs CNN
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110105821.png)

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110111741.png)


![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110105839.png)
- self-attention 可以设置特定的参数，可以转换为CNN
- On the Relationship between Self-Attention andConvolutional Layers
https://arxiv.org/abs/1911.03584

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110110147.png)
- An lmage is Worth 16x16 Words: Transformers for lmageRecognition at Scale
https://arxiv.org/pdf/2010.11929;pdf

##### self attention vs RNN
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110110704.png)

- RNN串行，Attention并行
- RNN较难考虑前面的信息，Attention很容易考虑前面的信息，因为QKV计算的时候，都会考虑
Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
https://arxiv.org/abs/2006.16236

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110111951.png)

#### more
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110111911.png)


### transformer

#### Seq2seq
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110113748.png)
- 由于很多语言没有文字，所以有的需要直接翻译为我们需要的语言

##### Hokkien 台语
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110114005.png)
- 输入就是台语，输出就是对应的中文去训练。
- 不管背景音乐，噪声，闽南语的音素
To learn more: https://sites.google.com/speech.ntut.edu.tw/fsw/home/challenge-2020

##### Text-to-Speech(TTS) Synthesis 
从文本到语音
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110114827.png)

##### seq2seq for chatbot
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110114955.png)

###### Most Natural Language Processing applications ...

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110115238.png)

- seq2seq可以做，但是不如特制模型效果好，就像刀可以干很多事情，切菜，切水果，但是大部分的事情都有更适合的模型
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110115349.png)

##### Seq2seq for Syntactic Parsing 
用于句法分析的Seq2seq
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110115854.png)
- 把语句转为对应的格式，就是树。
- Paper   Grammar as a Foreign Language
https://arxiv.org/abs/1412.7449

##### Seq2seq for Muti-label classification
**muti-label  (每个东西属于不止一个类)**
**Seq2seq for Muti-class classification (每个东西只能属于一个类)**
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110120404.png)
https://arxiv.org/abs/1909.03434
https://arxiv.org/abs/1707.05495

##### Seq2seq for Object Detection
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110120725.png)


![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110120942.png)

- seq2seq 主要包括编码和解码
#### Encoder  编码

- 输入向量，输出向量
- RNN,CNN都可以
![image.png|539](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110124340.png)


- attention 用的是 多头注意力机制
- Add & Norm 是 Residual + Layer norm
- Feed Forward理解  [[对Transformer中FeedForward层的理解]]

$$
FFN(x) = \mathrm{ReLu}(xW_{1}+b_{1})W_{2}+b_{2}
$$

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110123020.png)

##### 原来的transformer


![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110123255.png)
- 输入+输出 -> residual
- layer norm [[【深度学习】BatchNorm、LayerNorm-CSDN博客]]
- 原始的结构不是最优的

##### To Learn more..
![image.png|540](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110124340.png)

- On Layer Normalization in theTransformer Architecture
https://arxiv.org/abs/2002.04745
 PowerNorm: Rethinking BatchNormalization in Transformers https://arxiv.org/abs/2003.07845

#### decoder 解码
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110171325.png)

##### Autoregressive(AT)  自回归  decoder

speech recognition as example

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110172825.png)

- decoder目的是产生输出
- encoder的结果输出->读取到Decoder一部分输入

> 输入：
- 标识BEGIN (special token) 代表开始
	- 是 BOS(begin of sentence),这里明确写出来是 BEGIN
	- 用one-hot编码，一个维度是1其他维度是0

> 输出：
- 输出向量的大小就是vocabulary size 词表的长度
	- ==vocabulary 词表是你想要输出的所有词或者单元的集合。==
- 处理逻辑：
	- 先将BEGIN作为输入，结合Encoder的输出，得到第一个向量，通过softmax归一化后，输出概率最大的维度对应的中文。
	- 这个输出再次作为输入传到Decoder的输入，得到输出再softmax依此类推。
	- 
```ad-note
问题：Decoder得到的某一个错误输出也会影响后面值的生成！
这个后面会单独解决！！
```

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110173354.png)

##### Encoder vs Decoder

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110173732.png)

遮住第二个多头注意力的模块的相同之处：
- 整个结构相同：
	- embedding + position enconding 
	- 然后多头 + Add & Norm
	- Feed Forward +  Add & Norm
不同:
- Decoder的多头是masked的(掩码)
- ==也就是计算b的时候，只能考虑和自己序号及以前的输入，不用考虑所有的输入==
	- b3考虑 a1,a2,a3
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110174111.png)
	- 具体计算：

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110175326.png)
	- ==原因：因为输入是一个一个产生的，在计算b2的时候，还没有a3==

##### how to stop 
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110175612.png)
- ==可以使用stop token，和begin token可以是一个符号，也可以是不同的符号。==
- ==通过在词汇表中添加对应的符号，用来标识输出的开始和结束。==
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110175849.png)
![image.png|421](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110175906.png)


##### Non-autoregressive(NAT) 非自回归

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110181015.png)
怎么样结束
- 预测长度
- 忽略end后面的输出


更多资料：
https://youtu.be/jvyKmU4OM3c


#### Encoder-Decoder

**Transformer**
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110181339.png)

##### Cross attention

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110181843.png)

- q是通过masked 多头注意力机制产生的结果，考虑了包括自己序号之前的输入，然后通过softmax后得到的输出q
- k,v来自Encoder的输出
- ==Cross attentions是kv来自Encder,q来自Decoder然后再进行多头注意力机制的过程就是交叉注意力机制==
- 得到的v'就是输出然后进入FC层
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110182411.png)

##### seq2seq做的
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110182600.png)

##### cross attention 交叉的位置
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110182956.png)


#### Train
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110183519.png)

- 核心：计算Ground truth 真实数据和Decoder输出的**最小交叉熵损失**，本质和分类一样。
- **Teacher Forcing: 有标准答案作为输入的方法。**

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110183723.png)

##### Copy Mechanism 复制机制

- 定义：需要把部分结果复制到输出的机制
- 应用：**chat-bot，聊天机器人**，sumarization摘要
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110184234.png)

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110184300.png)

- 需要的数据量得有百万篇，几万篇太少
https://arxiv.org/abs/1704.04368

- 最早从输入复制东西的模型就是 Pointer Network
- Pointer Network
https://youtu.be/vdOyqNQ9aww

- Incorporating Copying Mechanism in Sequence-to-Sequence Learning
https://arxiv.org/abs/1603.06393

##### Guided Attention
ln some tasks, input and output are monotonically aligned.For example, speech recognition,TTS, etc.

tts as example

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110184840.png)
- 大部分情况都蛮好的，可能会出现一些奇怪的问题，输入2-4次发财，声音还有抑扬顿挫，各个发财不一样，但是输出一次发财，就有财了，原因是语料库短的太少了

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110195301.png)

**解决方法：**
Monotonic Attention
Location-aware attention

##### Beam Search 束搜索
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110195644.png)
Not possible to check all the paths .  -> Beam Search [[Beam Search解码-CSDN博客]]

- Beam Search
Beam Search（束搜索）是一种在搜索空间中寻找最优解的算法。它常用于自然语言处理任务中，如机器翻译和语言生成。Beam Search通过在每个时间步选择概率最高的一组候选解来进行搜索，以此来寻找最有可能的解。

Beam Search的原理如下：

（1）首先，根据模型的输出概率分布，选择概率最高的K个候选解作为初始解集。

（2）在每个时间步，对于每个候选解，根据模型的输出概率分布，生成K个新的候选解。

（3）对于生成的新候选解，根据其得分（通常是概率的对数）进行排序，并选择得分最高的K个候选解作为下一步的候选解集。

（4）重复步骤2和步骤3，直到达到指定的搜索深度或满足终止条件为止。

（5）最后，从最终的候选解集中选择得分最高的解作为最终的输出。


- Greedy Search（贪心搜索）
贪心算法在翻译每个字的时候，直接选择条件概率最大的候选值作为当前最优。


##### Sampling 抽样

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110200040.png)

- Beam Search也不总是好的，左侧，用Pure Search可能更好，根据不同的问题适用不同的方法。
- ==TTS需要随机性==
- Accept that nothing is perfect.True beauty lies inthe cracks of imperfection.。

##### 优化评价指标 optimizing Evalation Metrics

[[BLEU score 是什么评估指标]]
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110200946.png)

- BLEU无法微分，不好做Loss,-> 使用RL去做。
- **大概思路**
	- Decoder 当成 Agent
	- BLEU Score当成是 RL的reward
- paper https://arxiv.org/abs/1511.06732

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110201436.png)



- 训练的时候，看到是输出都是正确的
- 测试的时候，Decoder可能得到错误的输出，会影响后续
- 这就是exposure bias
- 解决方法：-> scheduled sampling
	- 给训练的时候加一些错误信息，这样反而训练效果更好了。

##### scheduled sampling

==会影响transformer的并行能力==
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241110201538.png)
