---
created: 2025-01-18T13:04
updated: 2025-01-26T00:14
---

# （一）Bert简介

## 1 . 自监督学习

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/20250118130624.png" alt="image.png" style="zoom: 50%;" />

**监督学习(supervised)**：给定输入x，通过model得到预测值y,和标准答案$\hat y$，这种有答案的学习叫监督学习。

**自监督学习(self-supervised)**: 由于没有label也就是输出，将输入x分为输入$x'$和输出(label)$x''$，去训练模型。

Yann LeCun说，这个叫自监督学习，不应该叫无监督学习，因为无监督学习也是一个大的范畴，比如聚类等等。

> 上面的翻译：
>
> l now call it "self-supervised learning". because "unsupervised" is both a loaded(意味深长) and confusing(令人困惑,难以理解的) term(词,术语).
>
> ln self-supervised learning, the system learns to predict part of its input from other parts of it input. In other words a portion of the input is used as a supervisory signal to a predictor fed with the remaining portion of the input.
>
> 我现在称之为“自我监督学习”。因为“无监督”是一个既复杂又令人困惑的术语。
> 在自监督学习中，系统学习从其输入的其他部分预测其输入的一部分。换句话说，输入的一部分被用作预测器的监控信号，预测器由输入的剩余部分馈送。



Bert可以做的事情是 输入一排向量，输出一排向量，维度一样。

Bert会有masking input操作，还有next sentence prediction

### Masing Input

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118140337573.png" alt="image-20250118140337573" style="zoom: 50%;" />

1> mask掩码是随机的，策略有2种：

- 使用特殊的token，文本中没有的
- 随机替换为其他token

2> mask词元输入后的输出记为$y''$,通过Linear层也就是乘以矩阵，再通过softmax层进行归一化,得到分布得到预测结果$y'$，然后我们把mask原理的值y和$y'$使用分类的目标函数最小交叉熵(minimize cross entropy)

### Next Sentence Prediction

![image-20250118181801532](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118181801532.png)

NSP任务

NSP样本如下:
1．从训练语料库中取出两个连续的段落作为正样本
2 .  从不同的文档中随机创建一对段落作为负样本

- 将sentence1和sentence2训练后，把CLS对应的输出结果通过线性层，继续来分类判断2个句子是不是相接。
- 后面研究有说法NSP任务 not helpful—没有效果，有些简单,随机选择的和正常顺序的句子应该差别很大。 **(RoBerta)**
- 给出优化版的方法— **SOP(sentence order prediction)**，步骤如下：
  - 将两个按顺序的句子S1,S2作为正输入，S2,S1作为负输入，作为训练任务，代替Bert的NSP任务。
  - 作用：用来判断两个句子是否是按顺序的
  - 用在: ALBERT

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118184004190.png" alt="image-20250118184004190" style="zoom:50%;" />

#### 预训练Pre-train

Bert真正有用的预训练任务应该是Masked token prediction,也就是Bert对做完型填空任务很擅长，NSP任务作用不大。

预训练是**为了让模型在见到特定任务数据之前，先通过学习大量通用数据来捕获广泛有用的特征，从而提升模型在目标任务上的表现和泛化能力。**

#### 微调 fine-tune

> **什么是预训练和微调?**
>
> - 你需要搭建一个网络模型来完成一个特定的图像分类的任务。首先，你需要随机初始化参数，然后开始训练网络，不断调整参数，直到网络的损失越来越小。在训练的过程中，一开始初始化的参数会不断变化。当你觉得结果很满意的时候，你就可以将训练模型的参数保存下来，以便训练好的模型可以在下次执行类似任务时获得较好的结果。这个过程就是 pre-training。
> - 之后，你又接收到一个类似的图像分类的任务。这时候，你可以直接使用之前保存下来的模型的参数来作为这一任务的初始化参数，然后在训练的过程中，依据结果不断进行一些修改。这时候，你使用的就是一个 pre-trained 模型，而过程就是 fine-tuning。
>
> 所以，**预训练 就是指预先训练的一个模型或者指预先训练模型的过程；微调 就是指将预训练过的模型作用于自己的数据集，并使参数适应自己数据集的过程。**

经过微调后的bert模型就可以处理下游任务：

- 我们所关注的任务，不仅仅是完型填空任务。
- **我们需要一些有标签的数据**，就可以使bert经过微调处理各式各样的任务。

### GLUE

**General Language Understanding Evaluation (GLUE) 通用语言理解评估**

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118184203902.png" alt="image-20250118184203902" style="zoom:50%;" />

GLUE任务集，将模型如bert针对9个任务进行微调，得到9个模型，通过他们的评价指标如准确率/F1分数来评价模型的好坏。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118185322590.png" alt="image-20250118185322590" style="zoom:50%;" />

**黑色线是人类的水平。**从图中可以看到，较早的时间时，人类在大部分的GLUE任务上比训练的模型要好，慢慢的训练的模型就不断提升，在越来愈多的任务表现超过人类，有bert,gpt等等

### Use Bert  Case 1

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118190053860.png" alt="image-20250118190053860" style="zoom:50%;" />

case1的情况：

- data: 有标签的数据集，以情感分析为例，需要给出大量的label是正面还是负面的输入和输出数据作为模型训练的数据集。
- input: sequence,也就是序列
- output: class 也就是分类结果
- example: sentiment analysis(情感分析)，到底是正面的还是负面的。
- 结构: BERT+Linear+softmax
  - BERT作为预训练的产物，然后在CLS的输出部分接上线性层和softmax层，得到分类结果。
  - Bert部分: 参数初始化采用预训练结果的权重等参数(效果好,见下图)。
  - Linear层：随机初始化权重等参数。
  - 整个模型需要使用上面说的有label的data进行训练。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118190743055.png" alt="image-20250118190743055" style="zoom:50%;" />

- bert是先unsupervised无监督的预训练学习广泛的特征，然后在小数据集上进行微调这部分就是supervised监督训练，需要的是有label的小数据集。

### Use Bert Case 2

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118193653491.png" alt="image-20250118193653491" style="zoom:50%;" />

- input一个句子和output长度一样，比如POS tagging 词性标记任务。
- 初始化：线性层随机初始化参数，bert使用pre-train得到的参数
- 结构：每个output除了CLS的，都接线性层和softmax层进行分类，得到输出。
- 微调过程：有标签的数据输入后，经过bert得到输出结果，然后去分类。

### Use Bert Case 3

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118194706473.png" alt="image-20250118194706473" style="zoom:50%;" />

- input 2个句子，output一个分类结果，比如**Natural Language Inference(NLI) 自然语言推断**
- 详细解释这个任务
  - **premise(前提)**，**Hypothesis(假设)**
  - **能不能通过前提推出假设，两个的关系**是矛盾(contradiction 矛盾，对立)的，还是蕴含的(entailment),还是中性的(neutral 中立的，中性的)
  - 示例：premise: A person on a horse jumps over a broken down airplane；hypothesis: A person is at a diner.
  - 经过model,应该得到 contradiction矛盾的

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118195612005.png" alt="image-20250118195612005" style="zoom:50%;" />



### Use Bert Case 4

![image-20250118195933947](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118195933947.png)

- 针对Extraction-based Question Answering(QA),有限制的QA，问题的答案，就在输入的段落里面
- Document和Query都是一个列表，d,q代表对应的内容，经过QA Model后输出整数s和e,output是Document中内容的索引，代表Query答案索引是Document中[s,e]，因此Answer是 [ds, ... ,de]

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118201011390.png" alt="image-20250118201011390" style="zoom:50%;" />

- 推理结果的过程：
  - 将question和document作为两个部分输入到bert,使用[SEP]分隔，理论上Bert的输入无限长，但实际上这里最长512，太差计算量太大。
  - **开始索引startIndex**，使用document的向量和它的输出向量做内积(inner production)，再通过softmax层得到startIndex结果,即答案在document的开始索引。
  - **输出索引endIndex**，使用Query的向量和它的输出向量做内积(inner production)，再通过softmax层得到endIndex结果,即答案在document的结束索引。
  - 最终结果就是document索引从[s,e]的内容。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118202057496.png" alt="image-20250118202057496" style="zoom:50%;" />

单独训练很困难，李宏毅老师组训练了一个用TPU跑1M轮，跑了8天

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118202357673.png" alt="image-20250118202357673" style="zoom:50%;" />

When does BERT know Pos tagging,syntactic parsing, semantics?
伯特什么时候知道词性标记、句法分析、语义的？

结论违反直觉(counterintuitive)! 

![image-20250118210335012](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118210335012.png)

corrupt 腐败的，贪污的，有错误的

mask的方法

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118210457339.png" alt="image-20250118210457339" style="zoom:50%;" />

T5比较了很多的mask方法

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118210629421.png" alt="image-20250118210629421" style="zoom:50%;" />

- 数据集 C4 有7T

## 2 .Bert奇闻异事

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118211047310.png" alt="image-20250118211047310" style="zoom:50%;" />

经过bert的输入结果的向量，相同语义token的向量结果接近，**bert可以考虑上下文信息**，比如吃苹果的果和苹果手机的果向量不一样，

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118211619689.png" alt="ss" style="zoom:50%;" />

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118211718882.png" alt="image-20250118211718882" style="zoom:50%;" />

使用了10个句子的果(前5个果是吃的苹果，后5个是手机苹果)的emb进行计算相似度，发现前五个相似度接近，后五个相似度接近，之间的相似度差别很大，**说明Bert理解了上下文语义，各个词之间不是相互独立的**

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118212408242.png" alt="image-20250118212408242" style="zoom:50%;" />

也许bert预测就是使用上下文抽取的精华来预测mask的向量。

![image-20250118212656307](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118212656307.png)

DNA的分类训练

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118214832344.png" alt="image-20250118214832344" style="zoom:50%;" />

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118214946786.png" alt="image-20250118214946786" style="zoom:50%;" />

bert的初始化参数不仅仅和语义有关，做蛋白质，DNN，音乐分类，效果依然好！——Bert很强大

## Learn More

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118215115835.png" alt="image-20250118215115835" style="zoom:50%;" />

## Multui-BERT

![image-20250118215157417](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118215157417.png)

#### Zero-shot Reading Comprehension

Training on the sentences of 104 languages

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118215326194.png" alt="image-20250118215326194" style="zoom:50%;" />

英文训练，中文居然可以测试

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118215537892.png" alt="image-20250118215537892" style="zoom:50%;" />

pre-train 104种语言，**在英文上微调QA后，直接去测试中文问答QA**居然效果很好，和**纯用中文微调训练和测试F1分数相差不是很大**。

Cross-lingual Alignment4

![image-20250118215958067](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118215958067.png)



<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118221434886.png" alt="image-20250118221434886" style="zoom:50%;" />

Multi-Bert可以将各个语言的词相对应，并且随着文本量的上升，Multi-Bert的效果会很好，对齐的很不错，图中李宏毅老师组的同学200k的文本量怎么调效果都不好，当文本量达到了1000k后训练效果很好。

![image-20250118222053282](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118222053282.png)

没有抹掉不同语言的信息，他们的向量还是不一样的。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118222147808.png" alt="image-20250118222147808" style="zoom:50%;" />

所有英文的emb平均起来A，和中文的emb平均起来B，有一个差距向量A-B，把需要输入的英文向量emb之后得到c，加上得到的差距向量A-B，再次作为输入，去做填空题的效果如下图

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118222547686.png" alt="image-20250118222547686" style="zoom:50%;" />

感觉可以找到中英文的语义空间向量差别，这样可以得到还不错的翻译——Unsupervised token-level translation(无监督token级翻译)，尽管multi-Bert可以把不同语言相同语义的emb向量进行对齐，但是仍有差别。

## GPT

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118224110855.png" alt="image-20250118224110855" style="zoom:50%;" />

架构: decoder-only，给一个输入然后不断自回归预测下一个值，相当于transformer里面的decoder部分，然后通过线性层，softmax,然后通过cross entropy交叉熵进行训练。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118224405622.png" alt="image-20250118224405622" style="zoom:50%;" />

gpt具有生成能力，想起独角兽是因为有一个例子是生成独角兽新闻，测试可以看到

### How to use GPT ?

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118224545013.png" alt="image-20250118224545013" style="zoom:50%;" />

gpt可以和bert通过linear层，然后去做微调，有可能是gpt太大了微调很困难，也有可能为了和bert不一样。

所以使用方法更加狂，和人类更加接近! 模仿上面的过程

#### task desciption-example-prompt

任务描述-例子-prompt(提示)

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118223945178.png" alt="image-20250118223945178" style="zoom:50%;" />

和bert填空不一样，使用这种结构，完全没有专门学习过这样的翻译任务，只是学习了不断生成结果!

![image-20250118224043659](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118224043659.png)

这是gpt在不同规模的大小模型进行42项任务的测试，随着参数量增加效果逐渐变好。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20250118225118526.png" alt="image-20250118225118526" style="zoom:50%;" />，

自监督的方法不一定只能用于NLP，还有语音和cv
