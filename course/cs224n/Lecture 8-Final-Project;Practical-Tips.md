## Lecture Plan

![image-20251029195212804](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029195212804.png)

1. **Attention revisited [12 mins]**：回顾注意力机制 —— 巩固这一 NLP 核心技术的原理，为后续项目或研究奠定基础。
2. **Final project types and details; assessment revisited [15 mins]**：讲解期末项目的类型、细节及考核要求 —— 明确项目方向和评分标准，助力学生规划实践。
3. **Finding research topics; a couple of examples [18 mins]**：指导如何寻找研究课题并举例 —— 培养学生的科研选题能力，从 “做项目” 过渡到 “做研究”。
4. **Finding data [10 mins]**：讲解数据获取方法 —— 解决科研中 “数据从哪来” 的关键问题，保障项目和研究的可落地性。
5. **Doing your research [15 mins]**：阐述科研开展流程 —— 从选题到实验再到结论，梳理完整的科研逻辑。
6. **Reading Comprehension/Question Answering brief intro [10 mins]**：简要介绍阅读理解与问答任务 —— 拓展 NLP 知识边界，为后续学习或研究提供新方向。

![image-20251029200027270](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029200027270.png)



![image-20251029200334536](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029200334536.png)


### 
- 我们有编码器隐藏状态 $h_1, \dots, h_N \in \mathbb{R}^h $
- 在时间步 $t $，我们有解码器隐藏状态 $s_t \in \mathbb{R}^h $
- 我们得到该时间步的注意力分数 $e^t $：$e^t = [s_t^T h_1, \dots, s_t^T h_N] \in \mathbb{R}^N $
- 我们对其进行 softmax 操作以得到该时间步的注意力分布 $\alpha^t $（这是一个概率分布，总和为 1）：$\alpha^t = \text{softmax}(e^t) \in \mathbb{R}^N $
- 我们==使用 $\alpha^t $ 对编码器隐藏状态进行加权求和==，得到注意力输出 $a_t $：$a_t = \sum_{i=1}^N \alpha_i^t h_i \in \mathbb{R}^h $
- 最后，我们将注意力输出 $a_t $ 与解码器隐藏状态 $s_t $ 拼接，然后按照无注意力机制的 seq2seq 模型的流程继续处理：$[a_t; s_t] \in \mathbb{R}^{2h} $


#### 解释
这部分内容详细阐述了**注意力机制在 Seq2Seq 模型中的数学实现流程**，核心是让解码器在每一步生成时，动态地关注编码器中与当前生成最相关的信息，具体步骤如下：
1. **定义基础状态**：编码器输出一系列隐藏状态 $h_1, \dots, h_N $（维度为 $\mathbb{R}^h $），解码器在时间步 $t $ 有隐藏状态 $s_t $（维度也为 $\mathbb{R}^h $）。
2. **计算注意力分数**：通过解码器隐藏状态 $s_t $ 与每个编码器隐藏状态 $h_i $ 的**点积操作**（$s_t^T h_i $），得到注意力分数向量 $e^t $（维度为 $\mathbb{R}^N $）。这个分数衡量了解码器当前状态与编码器各位置信息的“相关性强弱”。
3. **生成注意力分布**：对注意力分数 $e^t $ 应用 **softmax 函数**，将其转化为概率分布 $\alpha^t $（维度为 $\mathbb{R}^N $）。softmax 确保所有分数被归一化，且总和为 1，这样 $\alpha_i^t $ 就可以理解为“解码器在时间步 $t $ 对编码器第 $i $ 个隐藏状态的关注权重”。
4. **计算注意力输出**：==用注意力分布 $\alpha^t $ 对编码器隐藏状态进行**加权求和**，得到注意力输出 $a_t $（维度为 $\mathbb{R}^h $）==。这一步是“注意力”的核心体现——权重高的编码器状态会在 $a_t $ 中占据更大比重，从而让解码器聚焦于关键信息。
5. **后续处理**：将注意力输出 $a_t $ 与解码器当前隐藏状态 $s_t $ **拼接**（维度变为 $\mathbb{R}^{2h} $），再输入到后续的网络层（如全连接层），继续完成 Seq2Seq 模型的解码生成过程。

通过这一系列操作，注意力机制让解码器不再“盲目”依赖固定的编码器汇总信息，而是能在每一步动态选择最相关的输入内容，从而提升翻译、文本生成等任务的效果。

### Attention is great

![image-20251029201446039](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029201446039.png)

- 注意力机制很棒
- 注意力机制**显著提升神经机器翻译（NMT）的性能**
  - 它能让解码器聚焦于源文本的特定部分，这非常有用
- 注意力机制提供了更“类人”的机器翻译（MT）过程模型
  - **在翻译时，你可以回头查看源语句，而无需记住全部内容**
- 注意力机制解决了瓶颈问题
  - **它允许解码器直接查看源文本，绕过瓶颈**
- 注意力机制有助于缓解梯度消失问题
  - **为远距离状态提供了捷径**
- 注意力机制提供了一定的**可解释性**
  - 通过检查注意力分布，我们可以看到解码器当时在关注什么
  - 我们免费得到了（软性的）对齐！
  - 这很酷，因为我们从未显式地训练过一个对齐系统
  - 网络自己就学会了对齐

### There are several attention variations 

有很多attention的变体

![image-20251029201656417](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029201656417.png)

### 提取文字
- There are several attention variants
- We have some values $ h_1, \dots, h_N \in \mathbb{R}^{d_1} $ and a query $ s \in \mathbb{R}^{d_2} $
- Attention always involves:
  1. Computing the attention scores $ e \in \mathbb{R}^N $
     - There are multiple ways to do this
  2. Taking softmax to get attention distribution $ \alpha $: $ \alpha = \text{softmax}(e) \in \mathbb{R}^N $
  3. Using attention distribution to take weighted sum of values: $ \boxed{a} = \sum_{i=1}^N \alpha_i h_i \in \mathbb{R}^{d_1} $
- thus obtaining the attention output $ \boxed{a} $ (sometimes called the context vector)



- 存在几种注意力机制的变体
- 我们有一些**值（values）** $ h_1, \dots, h_N \in \mathbb{R}^{d_1} $ 和一个**查询（query）** $ s \in \mathbb{R}^{d_2} $
- 注意力机制始终包含以下步骤：
  1. 计算注意力分数 $ e \in \mathbb{R}^N $
     - 有多种实现方式
  2. 对其进行 softmax 操作以得到注意力分布 $ \alpha $：$ \alpha = \text{softmax}(e) \in \mathbb{R}^N $
  3. 使用注意力分布对“值”进行加权求和：$ \boxed{a} = \sum_{i=1}^N \alpha_i h_i \in \mathbb{R}^{d_1} $
- 从而得到注意力输出 $ \boxed{a} $（有时也称为上下文向量）


#### 解释
这部分内容是对**注意力机制通用框架**的抽象总结，适用于各类注意力变体，核心逻辑如下：
1. **定义核心元素**：
   - “值（values）” $ h_1, \dots, h_N $：是注意力机制要“关注”的对象集合，维度为 $ \mathbb{R}^{d_1} $；
   - “查询（query）” $ s $：是触发注意力计算的“线索”，维度为 $ \mathbb{R}^{d_2} $。
2. **三步核心流程**：
   - **计算注意力分数**：根据“查询”和每个“值”的关联程度，计算出分数向量 $ e $（维度 $ \mathbb{R}^N $）。这里强调“有多种实现方式”，==比如点积、加性模型、缩放点积等，都是不同的分数计算手段==。
   - **生成注意力分布**：对分数 $ e $ 应用 softmax 函数，得到概率分布 $ \alpha $（维度 $ \mathbb{R}^N $）。softmax 确保权重归一化，让 $ \alpha_i $ 表示对第 $ i $ 个“值”的关注比例。
   - **加权求和得到输出**：用注意力分布 $ \alpha $ 对所有“值”进行加权求和，得到注意力输出 $ \boxed{a} $（维度 $ \mathbb{R}^{d_1} $，也叫上下文向量）。这一步是“注意力”的最终体现——将分散的“值”按重要性聚合为一个向量。

这个框架是各类注意力变体（如自注意力、多头注意力等）的底层逻辑，**不同变体的差异主要体现在“注意力分数的计算方式”上**，而三步流程的核心结构是通用的。

### Attention variants

注意力分数的变体

![image-20251029203338648](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029203338648.png)

- Attention variants
  - You’ll think about the relative advantages/disadvantages of these in Assignment 4!
- There are several ways you can compute $ e \in \mathbb{R}^N $ from $ h_1, \dots, h_N \in \mathbb{R}^{d_1} $ and $ s \in \mathbb{R}^{d_2} $:
  - **Basic dot-product attention**: $ e_i = s^T h_i \in \mathbb{R} $
    - Note: this assumes $ d_1 = d_2 $. This is the version we saw earlier.
  - **Multiplicative attention**: $ e_i = s^T W h_i \in \mathbb{R} $ [Luong. Pham, and Manning 2015]
    - Where $ W \in \mathbb{R}^{d_2 \times d_1} $ is a weight matrix
  - **Reduced rank multiplicative attention**: $ e_i = s^T (U^T V) h_i = (U s)^T (V h_i) $
    - For low rank matrices $ U \in \mathbb{R}^{k \times d_2}, V \in \mathbb{R}^{k \times d_1}, k \ll d_1, d_2 $
  - **Additive attention**: $ e_i = v^T \tanh(W_1 h_i + W_2 s) \in \mathbb{R} $ [Bahdanau, Cho, and Bengio 2014]
    - Where $ W_1 \in \mathbb{R}^{d_3 \times d_1}, W_2 \in \mathbb{R}^{d_3 \times d_2} $ are weight matrices and $ v \in \mathbb{R}^{d_3} $ is a weight vector.
    - $ d_3 $ (the attention dimensionality) is a hyperparameter
    - “Additive” is a weird/bad name. It’s really using a neural net layer.
- More information: “Deep Learning for NLP Best Practices”, Ruder, 2017. http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
  “Massive Exploration of Neural Machine Translation Architectures”, Britz et al, 2017, https://arxiv.org/pdf/1703.03906.pdf


#### 翻译
- 注意力机制变体
  - 你会在作业 4 中思考这些变体的相对优缺点！
- 有几种方法可以从 $ h_1, \dots, h_N \in \mathbb{R}^{d_1} $ 和 $ s \in \mathbb{R}^{d_2} $ 计算出 $ e \in \mathbb{R}^N $：
  - **基本点积注意力**：$ e_i = s^T h_i \in \mathbb{R} $
    - 注意：这假设 $ d_1 = d_2 $。这是我们之前见过的版本。
  - **乘法注意力**：$ e_i = s^T W h_i \in \mathbb{R} $ [Luong, Pham, and Manning 2015]
    - 其中 $ W \in \mathbb{R}^{d_2 \times d_1} $ 是权重矩阵
  - **低秩乘法注意力**：$ e_i = s^T (U^T V) h_i = (U s)^T (V h_i) $
    - 适用于低秩矩阵 $ U \in \mathbb{R}^{k \times d_2}, V \in \mathbb{R}^{k \times d_1}, k \ll d_1, d_2 $（$ k $ 远小于 $ d_1, d_2 $）
  - **加性注意力**：$ e_i = v^T \tanh(W_1 h_i + W_2 s) \in \mathbb{R} $ [Bahdanau, Cho, and Bengio 2014]
    - 其中 $ W_1 \in \mathbb{R}^{d_3 \times d_1}, W_2 \in \mathbb{R}^{d_3 \times d_2} $ 是权重矩阵，$ v \in \mathbb{R}^{d_3} $ 是权重向量。
    - $ d_3 $（注意力维度）是一个超参数
    - “加性”是个奇怪/不太恰当的名字。它实际上是在使用一个神经网络层。
- 更多信息：“自然语言处理深度学习最佳实践”，Ruder，2017。http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
  “神经机器翻译架构的大规模探索”，Britz 等人，2017，https://arxiv.org/pdf/1703.03906.pdf


### 解释
这部分内容介绍了**注意力分数计算的四种常见变体**，核心差异在于“如何衡量查询 $ s $ 和值 $ h_i $ 之间的相关性”，具体分析如下：

1. **基本点积注意力**
   - 公式：$ e_i = s^T h_i $
   - 特点：直接对查询和值做**点积**，计算高效。但要求查询和值的维度 $ d_1 = d_2 $，否则无法运算。是最基础的注意力分数计算方式（如之前讲解的 Seq2Seq 注意力就属于这种）。

2. **乘法注意力**
   - 公式：$ e_i = s^T W h_i $
   - 特点：引入权重矩阵 $ W \in \mathbb{R}^{d_2 \times d_1} $，**打破了“$ d_1 = d_2 $”的限制，让查询和值可以是不同维度。通过矩阵乘法灵活建模两者的关联，是点积注意力的一般化形式。**

3. **低秩乘法注意力**
   - 公式：$ e_i = s^T (U^T V) h_i = (U s)^T (V h_i) $
   - 特点：对乘法注意力进一步优化，**引入低秩矩阵 $ U $ 和 $ V $（$ k \ll d_1, d_2 $）。通过“降维 + 点积”的方式，在保证表达能力的同时，大幅减少计算量和参数数量，适合高维场景。**

4. **加性注意力**
   - 公式：$ e_i = v^T \tanh(W_1 h_i + W_2 s) $
   - 特点：通过**神经网络层**（先将查询和值分别线性变换后相加，再经过 tanh 激活，最后与权重向量 $ v $ 点积）来建模相关性。它不要求查询和值维度一致，且理论上可以捕捉更复杂的非线性关系。但计算复杂度比点积类注意力高，“加性”的命名源于“$ W_1 h_i + W_2 s $”这一操作，**实际本质是神经网络的非线性变换。**


这些变体各有优劣：点积类计算高效但维度受限；加性表达能力强但计算稍慢。实际应用中需根据任务场景（如维度匹配、计算资源、模型复杂度）选择合适的注意力分数计算方式。

### Attention is a general Deep Learning technique 

注意力是一种通用的深度学习技术

![image-20251029203618984](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029203618984.png)

- 我们已经看到，注意力是改进机器翻译序列到序列（seq2seq）模型的有效方法。
- 然而：你可以在**多种架构**（不仅限于 seq2seq）和**多种任务**（不仅限于机器翻译）中使用注意力机制
- 注意力的**更通用定义**：
  - ==给定一组向量 “值（values）” 和一个向量 “查询（query）”，注意力是一种根据查询来计算 “值” 的加权和的技术==。
- 我们有时会说 “查询对值进行关注（the query attends to the values）”。
- 例如，在 seq2seq + 注意力的模型中，每个解码器隐藏状态（查询）会对所有编码器隐藏状态（值）进行关注。
  - “the query attends to the values” 是领域内对注意力过程的通俗描述，即 “查询在‘关注’这些值”，对应着从 “计算关联度” 到 “加权求和” 的完整逻辑。以 seq2seq + 注意力的场景为例，解码器的每个隐藏状态作为 “查询”，会去关注编码器的所有隐藏状态（“值”），从而动态捕捉源文本中与当前生成最相关的信息 —— 这一过程在其他任务中可类比迁移（比如图像识别中，“查询” 可以是图像的局部特征，“值” 可以是全局特征集合，通过注意力捕捉局部与全局的关联）。



![image-20251029204113342](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029204113342.png)

- 直觉理解：
  - 加权和是对 “值” 中包含信息的**选择性总结**，其中查询决定了要聚焦哪些 “值”。
  - 注意力是一种根据另一种表示（查询），为任意一组表示（值）获取**固定大小表示**的方法。
- 结论：
  - 注意力已成为深度学习模型中强大、灵活、通用的 “指针与内存操作” 方式。它是 2010 年之后出现的新思想！

这部分内容从**直觉层面和宏观价值**阐述了注意力机制：

1. 直觉理解（Intuition）

   - 从信息处理角度，==注意力的加权和不是对所有 “值” 的无差别汇总==，而是**选择性地提炼**—— 由 “查询” 决定哪些 “值” 更重要，最终的加权和是对关键信息的浓缩总结。
   - 从表示学习角度，==无论 “值” 的集合是何种形式（长度、维度可变），注意力都能输出一个**固定维度的表示**，且这个表示由 “查询” 动态调控==。这一特性让它能适配各种输入形式，成为不同模块间的 “信息桥梁”。

2. 结论（Upshot）

   **把注意力类比为 “指针与内存操作”，是因为它能像指针一样 “指向” 关键信息（内存），并对这些信息进行 “操作”（加权汇总）。这种能力让它在深度学习中极具通用性和灵活性，**成为 2010 年后推动模型性能突破的核心技术之一（如 Transformer 架构完全基于注意力，在 NLP、CV 等领域掀起革命）。

## 2、Course work and grading policy

**课程作业与评分政策**

![ ](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029204748829.png)

- 2. 课程作业与评分政策

  - 5 次一周期限的作业：6% + 4×12% = 54%
  - 期末默认或定制课程项目（1-3 人）：43%
    - 项目提案：5%；里程碑：5%；总结段落 + 图片：3%；报告：30%
  - 课堂参与：3%
    - 嘉宾演讲、Ed 平台互动、课程评估、学习贡献等 —— 详见课程网站！
  - 迟交政策
    - 6 天免费迟交额度；之后每天扣 10%；每项作业最多迟交 3 天
  - 协作政策：请阅读课程网站和荣誉准则！
    - 项目中：可使用现有代码 / 资源，但必须记录，且评分基于你的 “增值贡献”
    - 若为多人项目：需包含每位团队成员工作的简要说明
      - 几乎所有情况下，每位团队成员得分相同，但在严重违规情况下，我们保留差异化评分的权利

### The Final Project

![image-20251029205209914](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029205209914.png)



- **期末项目**
  - 关于期末项目（FP），你可以选择以下两种方式之一：
    - 完成默认项目，即 SQuAD 问答任务（包含 2 个子变体）
      - 开放性较强且入门较容易；对大多数人来说是不错的选择
    - 提出定制化的期末项目，且需获得我们的批准
      - 你将从导师（助教/教授/博士后/博士）那里获得反馈
  - **你可以以 1-3 人的团队形式开展项目**
    - 较大规模的团队项目或用于多门课程的项目，规模应更大，且通常需要探索更多模型或任务
  - 你可以为项目使用任何编程语言/框架
    - 不过我们预计你们大多数人会继续使用 PyTorch
    - 而且我们默认期末项目的起始代码是基于 PyTorch 的

### Custom Final Project

![image-20251029205311034](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029205311034.png)

- 定制化期末项目
  - 我很乐意和大家讨论期末项目，但有个小问题：我只有一个人……
  - 查看助教在定制化期末项目上的专长：
    - http://web.stanford.edu/class/cs224n/office_hours.html#staff
  - （下方表格为助教办公时间及研究专长安排）
    - 周一：Chris Manning（自然语言处理多数领域，生成对抗网络和强化学习方面稍弱）；Chris Waites；Rui Wang（强化学习、通用深度学习）；Akshay Smit（深度学习的生物医学应用）；Angelica Sun
    - 周二：Gita Krishna；Megan Leszczynski（命名实体消歧、机器学习系统）；Mandy Lu；Yuyan Wang（自然语言生成）
    - 周三：Lauren Zhu（深度学习、机器翻译、问答 / 默认期末项目）；Anna Yang（医疗聊天机器人、人机交互）；Alvin Hou（迁移学习、问答 / 默认期末项目）；Andrew Wang（图机器学习、计算社会科学的自然语言处理）
    - 周四：Rachel Gardner（视觉与语言、BERT、含噪用户文本、机器人学）；Shikhar Murty（快速适应、组合性、常识）；Davide Giovanardi（语言模型、迁移学习、元学习）；Zihan Wang（深度学习、机器人学、元学习）；Prerna Khullar
    - 周六：Elissa Li（语用学 / 语用推理的深度学习）；Rui Yan（深度学习及其在生物医学中的应用）；Daniel Do；Dilara Soylu；Lingjue Xie（深度学习、搜索引擎、默认期末项目）

### The Default Final Project

![image-20251029205510619](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029205510619.png)

**默认期末项目**

- 现在网上已有两份关于它的讲义！

- 两个问答（QA）任务变体

  1. **从头构建适用于 SQuAD 的文本问答架构**

  - 斯坦福问答数据集：https://rajpurkar.github.io/SQuAD-explorer/
  - 提供 PyTorch 起始代码。尝试 SQuAD 2.0（包含不可回答的问题）

  2. **构建能在不同问答数据集 / 领域工作的鲁棒问答系统**

  - 在 SQuAD、NewsQA 和 Natural Questions 上训练；测试集为 DuoRC、Race 和 RC 的 ZSRE
  - 起始点是大型预训练语言模型（DistilBERT）；**主要研究鲁棒性方法**

- 我们将在课程后期（第 6 周）讨论问答。示例：

  文本（T）：[Bill] Aiken 被墨西哥电影女演员 Lupe Mayorga 收养，在邻近的马德拉镇长大，他的歌曲记录了他儿时看到的移民农场工人所面临的艰辛。

  问题（Q）：Bill Aiken 是在哪个城镇长大的？

  答案（A）：Madera [但谷歌的 BERT 给出 < 无答案 >！]

这部分内容详细介绍了课程的**默认期末项目（问答任务）**，分为两个方向：

- 第一个方向聚焦 “**从零构建 SQuAD 问答架构**”，基于经典的斯坦福问答数据集，提供 PyTorch 起始代码，还涉及更具挑战性的 SQuAD 2.0（含不可回答问题），适合想从基础入手理解问答模型架构的学生。
- 第二个方向侧重 “**鲁棒性问答系统**”，需在多数据集（SQuAD、NewsQA 等）上训练并测试，基于预训练语言模型 DistilBERT 优化鲁棒性，适合对模型泛化能力和跨领域表现感兴趣的学生。

![image-20251029210205618](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029210205618.png)

**何选择默认期末项目？**

- 如果你：
  - 科研经验有限，不清楚自己想做什么，或者需要指导、明确目标…… 甚至需要排行榜
- 那么：
  - 选择默认期末项目！
  - 很多人都应该选择它！
- 考量因素：
  - 两个默认期末项目变体为你提供了大量指导、框架支持和明确的目标方向
  - 成功的路径不是去做那些和默认期末项目能实现的成果相比显得很平庸的事情。（过往数据：约一半的学生会选择默认期末项目。）

![image-20251029210434632](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029210434632.png)

- 为何选择定制化期末项目？
  - 如果你：
    - 有令自己兴奋的科研项目（可能已经在开展），且该项目实质性涉及人类语言和神经网络
    - 想要独立尝试做一些不同的事情
    - 对问答之外的领域（涉及人类语言材料和深度学习）感兴趣
    - 想要更深入地体验定义研究目标、寻找数据和工具、探索有趣的研究方向以及评估成果的完整过程
  - 那么：
    - 选择定制化期末项目！



![image-20251029210606426](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029210606426.png)

- 项目提案 —— 所有人必做，占比 5%

  1. **为你的课题寻找一篇相关的（关键）研究论文**
     - 对于默认期末项目（DFP），我们会提供一些建议，但你也可以在其他地方寻找有趣的问答 / 阅读理解研究成果
  2. 撰写该研究论文的总结，以及你从中提炼出的希望在自己项目中运用的关键思想
  3. 阐述你计划开展的工作，以及将如何在期末项目中实现创新
     - 建议设定一个中期里程碑
  4. **按需描述，尤其针对定制化项目**：
     - 项目计划、相关现有文献、你将使用 / 探索的模型类型；你将使用的数据（及获取方式），以及你将如何评估项目是否成功

  - 篇幅要求：3-4 页
  - 提交截止时间：2 月 16 日（周二）下午 4:30，通过 Gradescope 提交



![image-20251029210800124](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029210800124.png)

#### 翻译
- **项目提案——所有人必做，占比5%**
  1. 如何批判性地思考一篇研究论文
     - 研究论文综述的评分主要是**总结性的（summative）**
     - 该论文的**新颖贡献或观点**是什么？
     - 使其有效的因素是否具有**一般性和可复用性**？
     - 他们的工作是否存在缺陷或巧妙的细节？
     - **它与同类主题的其他论文如何契合**？
     - 它是否能引发关于进一步尝试或不同方向的好问题？
  2. **如何做好你的项目提案**
     - 项目提案的评分主要是**形成性的（formative）**
     - 你需要有一个**整体合理的想法**（！）
     - 但大多数不完善的项目计划都在具体细节上存在不足：
       - 你是否有**优质数据或切实可行的收集计划**？
       - 你是否有切实可行的工作评估方式？
       - 你是否有**合适的基准模型或拟开展的消融研究**用于对比？


#### 分析
这部分内容从**学术思维和提案质量**两个维度，对项目提案的要求进行了深化：
- 对于“研究论文的批判性思考”，强调不仅要总结论文，更要从“新颖性、可复用性、缺陷与细节、领域契合度、衍生问题”等角度分析，培养学生的学术批判和迁移能力。
- 对于“项目提案的完成质量”，明确其评分是“形成性”的（以指导改进为主），同时指出提案常见的不足点——数据、评估方式、基准/消融研究的缺失。这些要求确保学生的项目从构思阶段就具备“可行性、可评估性、可比性”，为后续研究奠定扎实基础。

### Project Milestone - from everyone 5%

![image-20251029212444150](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029212444150.png)

- 项目里程碑 —— 所有人必做，占比 5%
  - 这是一份进度报告
  - 此时你应该完成了超过一半的工作！
  - 描述你已开展的实验
  - 描述你已获得的初步结果
  - 描述你计划如何利用剩余时间
  - 除某些特殊类型的项目外，到该截止日期时，你应已实现了部分系统并呈现一些初步实验结果
  - 提交截止时间：3 月 2 日（周二）下午 4:30，通过 Gradescope 提交

### Project writeup

writeup 写作，报告

项目报告

![image-20251029213230323](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029213230323.png)

- **项目报告撰写**
  - 报告质量对你的成绩非常重要！
  - **可以参考去年（或 2019 年）的优秀获奖作品作为范例**
  - 报告结构包含：
    - 摘要与引言
    - 相关工作
    - 模型（两部分）
    - 数据
    - 实验
    - 结果
    - 分析与结论

### Much of today's info is relevant ... for everybody

今天的大部分信息和所有人都相关。

![image-20251029213701708](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029213701708.png)

- **今天的大部分信息对所有人都很重要**

  - 从宏观层面

    - **了解一些做研究的方法是有益的！**

  - 从实际层面

    - 我们会涉及：
      - **基准模型（Baselines）**
      - **基准测试（Benchmarks）**
      - **评估（Evaluation）**
      - **错误分析（Error analysis）**
      - **论文写作（Paper writing）**

    这些内容对默认期末项目（DFP）来说也都是很有必要了解的！

![image-20251029213930435](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029213930435.png)

寻找研究课题

所有科学领域的两个**基本出发点(basic starting points)**：

- [钉子型] 从一个**感兴趣的（领域）问题入手**，尝试找到比现有已知 / 使用方法更优的解决方式
- [锤子型] 从一种**感兴趣的技术方法 / 手段入手**，探索其拓展、改进的有效路径，或寻找新的应用场景

#### 分析

这部分内容是对**科研课题选择逻辑**的经典分类：

- “钉子型” 以**问题为导向**，聚焦领域中待解决的痛点，**核心是** “解决已有问题的更优方案”；

- “锤子型” **以技术为导向**，聚焦方法本身的迭代或跨界应用，**核心是** “让技术产生更多价值”。

  这种分类帮助科研入门者清晰梳理课题来源，无论是从实际问题出发还是从技术创新出发，都能找到明确的研究起点，是科研思维培养的基础框架。



![image-20251029214226776](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029214226776.png)

“**exhaustive**” 可翻译为**“详尽的”“彻底的”**或**“全面的”**，常用来描述对某事物进行了充分、无遗漏的研究、分析或列举。例如 “an exhaustive survey” 可译为“一项详尽的调查”，“exhaustive analysis” 可译为“彻底的分析”。

“**exhausted**” 主要有两个含义： 

- 作为形容词，意为**“疲惫的；耗尽的”**，比如 “I feel exhausted after a long day.”（漫长的一天后我感到很疲惫），“exhausted resources”（耗尽的资源）。 
- 作为动词过去式/过去分词，是**“exhaust”（使疲惫；耗尽）的变形**，例如 “He exhausted himself by working too hard.”（他因工作太努力而精疲力竭）。

**项目类型**

这并非详尽列表，但大多数项目属于以下类型之一：

1. 找到**感兴趣的应用 / 任务**，探索如何有效解决它，通常使用现有模型
   - 可以是实际场景中的任务，或现有 Kaggle / 竞赛 / 共享任务
2. **实现复杂的神经架构**，并在数据上展示其性能
3. 提出**新的或变体的神经网络模型**，探索其实证效果
4. 分析类项目。分析模型行为：**它如何表示语言知识，能处理哪些现象或会犯哪些错误**
5. **罕见的理论类项目**：展示模型类型、数据或数据表示的有趣且非平凡的性质

#### 分析

这部分内容对**科研项目的类型**进行了分类梳理，覆盖了从 “应用落地” 到 “理论探索” 的全维度：

- 类型 1 侧重**任务驱动的应用探索**，利用现有模型解决实际或竞赛任务，门槛相对较低，适合从具体问题切入；
- 类型 2 和 3 聚焦**模型本身的实现与创新**，前者是复现复杂架构并验证效果，后者是提出新模型或变体，属于方法驱动的研究；
- 类型 4 是**模型行为分析**，通过拆解模型的语言知识表示、能力边界和错误模式，深化对模型的理解；
- 类型 5 是**理论性研究**，探索模型、数据的深层性质，属于科研中偏理论的小众但高价值方向。

这种分类为学生选择项目方向提供了清晰的参考框架，不同类型对应不同的研究目标和能力要求，帮助学生根据自身兴趣和基础定位项目类型。



![image-20251029223739135](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029223739135.png)



- **深度诗歌：用于莎士比亚十四行诗生成的词级和字符级语言模型**
  作者：Stanley Xie、Ruchir Rastogi、Max Chang
  - **门控LSTM**
    你的青春年华与容颜，他的形貌是否将遮掩？
    如今所有鲜活的美丽，我的爱在此处
    时光终将问候，将一切遗忘，如同终将消逝，
    但在最虔诚的敬拜中，他的荣耀逝去。
  - 图1：门控LSTM的架构


### 分析
这是一篇关于**用深度学习生成莎士比亚风格十四行诗**的研究工作：
- 模型采用**门控LSTM架构**，融合了“词嵌入查找”和“字符级LSTM”的信息，通过门控机制平衡词级和字符级特征，再经词级LSTM和Softmax层生成下一个词（$w_{t+1}$）。
- ==应用场景聚焦于文学创作领域，尝试让模型学习莎士比亚十四行诗的语言风格、韵律和语义模式，生成具有相似风格的诗歌。==这种跨领域的尝试体现了自然语言处理技术在艺术创作中的潜力，也为研究“模型如何捕捉复杂文学风格”提供了案例。



![image-20251029223937152](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029223937152.png)

- **可微分神经计算机的实现与优化**
  作者：Carol Hsin
  计算与数学工程专业研究生

  我们基于2016年10月发表的可微分神经计算机（DNC）论文[1]，在bAbI数据集[25]和神经图灵机论文[12]中描述的复制任务上，实现并优化了DNC。本文将通过记录我们的DNC实现方法以及优化DNC时遇到的挑战，帮助读者更好地理解这一新颖且极具潜力的架构。

  （下方为DNC细胞架构图）
  - 输入：$x_1$ 至 $x_T$
  - 模块：神经网络控制器（Sec.3.2）、记忆模块（Sec.3.3）、最终输出（Sec.3.4）
  - 状态（State）、$\xi_t$、$\nu_t$、$r_t^i$
  - 输出：$y_1$ 至 $y_T$


### 分析
这是一篇关于**可微分神经计算机（DNC）**的研究工作：
- DNC是一种融合神经网络与记忆模块的架构，旨在让模型具备类似计算机的“读写记忆”能力。文中基于经典论文复现并优化了DNC，在bAbI（问答推理数据集）和复制任务上验证其性能。
- 架构图展示了DNC细胞的核心流程：输入序列经神经网络控制器决策，通过记忆模块进行读写操作，最终输出结果。这种设计让模型能处理需要长期记忆和复杂推理的任务，是探索“神经-符号结合”“类脑计算”的重要尝试，为提升模型的推理和记忆能力提供了新路径。



![image-20251029224052468](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029224052468.png)

- **通过增强损失函数改进学习效果**
  作者：Hakan Inan、Khashayar Khosravi
  邮箱：inanh@stanford.edu、khosravi@stanford.edu

  我们针对知名的循环神经网络语言模型（RNNLM）提出两项改进。首先，**我们利用词嵌入矩阵将RNN输出投影到输出空间，在提升性能的同时大幅减少了自由参数的数量。**其次，我们不再仅仅最小化预测分布与“独热”目标分布之间的标准交叉熵损失，而是额外最小化一个考虑目标词与其他词之间固有度量相似性的损失项。通过在Penn Treebank数据集上的实验，我们证明：（1）与相同网络规模的先前模型相比，我们提出的模型实现了显著更低的平均词困惑度；（2）通过使用比先前最佳工作少得多的参数，达到了新的技术水平。

  该论文于2017年发表于ICLR会议。


### 分析
这是一篇发表于ICLR 2017的**循环神经网络语言模型（RNNLM）改进研究**：
- 核心创新在于“参数高效化”和“损失函数增强”：利用词嵌入矩阵减少参数数量，同时在损失函数中引入“词间度量相似性”项，让模型学习时不仅关注目标词，还能捕捉词与词之间的语义关联。
- 实验在Penn Treebank数据集上验证了效果，既降低了困惑度（语言模型的核心指标），又减少了参数规模，实现了“性能提升+效率优化”的双重目标，为RNNLM的优化提供了新的思路，也体现了“损失函数设计”在模型改进中的关键作用。

![image-20251029224111556](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029224111556.png)

- **Word2Bits - 量化词向量**
  作者：Maximilian Lam
  邮箱：maxlam@stanford.edu

  **摘要**
  词向量需要大量的内存和存储空间，这对手机、GPU等资源受限的设备构成了挑战。**我们通过在Word2Vec中引入量化函数，证明可以学习到每个参数仅用1-2位的高质量量化词向量。此外，我们还证明，使用量化函数训练可以起到正则化的作用。**我们在英文维基百科（2017）上训练词向量，并在标准的词相似度、类比任务以及问答任务（SQuAD）上进行评估。**我们的量化词向量不仅比全精度（32位）词向量节省8-16倍的空间，还在词相似度任务和问答任务上表现更优。**


### 分析
这是一篇关于**词向量量化优化**的研究工作：
- ==核心问题是解决传统词向量“内存占用大”的痛点，提出在Word2Vec中引入量化函数，将词向量参数压缩到1-2位/参数，同时保持甚至提升性能。==
- 创新点在于**“量化即正则化”**的发现，让量化后的词向量在节省8-16倍存储空间的同时，在词相似度、类比和SQuAD问答任务上超过全精度词向量。这种“压缩+提效”的设计，为资源受限设备上的自然语言处理应用提供了可行方案，体现了模型轻量化与性能优化的平衡思路。



### How to find an interesting place to start?

**如何找到有趣的研究起点？**

![image-20251029224418514](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029224418514.png)

### 翻译
- **如何找到有趣的研究起点？**
  - 查阅ACL论文集获取自然语言处理（NLP）论文：
    - https://www.aclweb.org/anthology/
  - 也可查阅主要机器学习会议的在线论文集：
    - NeurIPS https://papers.nips.cc、ICML、ICLR
  - 参考过往cs224n课程项目：
    - 查看课程官网
  - 查阅在线预印本服务器，尤其是：
    - https://arxiv.org
  - 更好的方式：从现实世界中寻找有趣的问题！
    - Hal Varian：《如何在业余时间构建经济模型》
      - https://people.ischool.berkeley.edu/~hal/Papers/how.pdf

![](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029225004819.png)

- 如何找到有趣的研究起点？

  - 由斯坦福毕业生、知名 cs231n 课程讲师 Andrej Karpathy 开发的 Arxiv Sanity Preserver：

    - [http://www.arxiv-sanity.com](http://www.arxiv-sanity.com/)

  - 展示了过去一天在 Twitter 上被提及的热门论文：

    - 《塑造叙事弧：协作对话的信息论方法》

      

      作者：Kory W. Mathewson 等，1901.11528v1，涉及领域：cs.HC、cs.AI 等

      摘要：研究设计能与人类协作对话生成创意叙事的智能体，通过信息论方法让智能体推理对话中的 “宇宙信息”，提升对话的创意性与连贯性，实验显示该方法显著提升了电影对话的预测准确率，且专业戏剧演员更偏好与该模型增强的智能体互动。

    - 《学习与评估通用语言智能》

      作者：Dani Yogatama 等，1901.11373v1，涉及领域：cs.LG、cs.CL 等

![image-20251029225450156](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029225450156.png)

- **想要在某领域超越当前最优水平？**
  - 一些整合了前沿技术信息的优质新网站
    - 但并非完全准确
    - https://paperswithcode.com/sota
    - https://nlpprogress.com/
  - 特定任务 / 主题的前沿榜单，例如：
    - https://gluebenchmark.com/leaderboard/
    - https://www.conll.org/previous-tasks/
  - （右侧展示机器翻译领域的前沿排行榜，包含数据集、最佳方法、论文标题、论文及代码链接等信息，如 WMT2014 英法翻译任务的最佳方法是 “Transformer Big + BT”，对应论文《Understanding Back-Translation at Scale》等）

![image-20251029225820539](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029225820539.png)



- **寻找研究课题**
  - 图灵奖得主、斯坦福大学计算机科学荣誉退休教授Ed Feigenbaum 建议遵循其导师——人工智能先驱、图灵奖与诺贝尔奖得主Herb Simon 的建议：
    - “**如果看到一个很多人在研究的领域，就去别的地方。**”
  - **但该去哪里呢？**冰球传奇Wayne Gretzky 说：
    - “我滑向冰球要去的地方，而不是它去过的地方。”


### 分析
这部分内容用**跨界类比**阐释了科研选题的策略：
- Herb Simon的建议强调“避开过度拥挤的研究领域”，避免陷入同质化竞争，鼓励寻找尚未被充分探索的方向；
- Wayne Gretzky的类比则突出“**前瞻性**”，即要预判领域的发展趋势，聚焦有潜力的前沿方向。两者结合，为科研者提供了“避红海、追蓝海”的选题逻辑——既不盲目跟风热门领域，又能通过趋势预判找到具有长期价值的研究课题，体现了科研中“差异化”与“前瞻性”的双重重要性。

### Old Deep Learning(NLP), new Deep Learning NLP

**自然语言处理中的 “旧深度学习” 与 “新深度学习”**

![image-20251029230002658](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029230002658.png)

- 在深度学习复兴的早期（2010-2018 年），**大部分工作聚焦于定义和探索更优的深度学习架构**。
- 典型论文方向：
  - 我可以通过不仅常规使用注意力机制，还引入 “复制注意力” 来改进摘要系统 —— **即通过额外的注意力计算和概率门，直接将输入中的词复制到输出中。**
- 这也是很多优秀的 CS 224N 课程项目的研究思路。
- 在 2019-2021 年，这种研究思路已式微。
  - 当然，这么说有些绝对，但确实变得困难且罕见了。
- 总体而言，**大部分工作转向下载大型预训练模型并在此基础上开展研究。**
  - 核心动作是**微调**，或**先进行领域适配再微调，诸如此类。**

#### 分析

这部分内容梳理了**自然语言处理领域深度学习研究范式的演变**：

- 早期（2010-2018）以 **“架构创新”** 为核心，研究者通过设计新的网络结构（如带复制注意力的摘要模型）来提升任务性能，CS 224N 的课程项目也多遵循这一思路；
- 后期（2019-2021 后）转向 **“预训练 + 微调”** 的范式，研究者不再从头设计架构，而是基于大型预训练模型（如 BERT、GPT 系列），通过微调或领域适配来解决具体任务。

这种演变反映了领域从 “追求架构突破” 到 “利用大规模预训练模型的迁移能力” 的趋势，也体现了科研范式随技术发展的迭代 —— 当预训练模型的性能天花板足够高时，“站在巨人肩膀上微调” 成为更高效的研究路径。

![image-20251029230223733](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029230223733.png)

**2021 年自然语言处理…… 推荐用于所有实际项目😊**

- `pip install transformers` # 由 Huggingface 提供🥳
- 这不是可直接运行的代码，但能体现大致思路……



![image-20251029231258280](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231258280.png)

- **2021年自然语言处理热门领域**
  如今很多令人兴奋的研究方向都围绕着预训练模型生态展开：
  - **模型评估与改进（非准确率导向）**
    - 领域迁移鲁棒性：研究模型在不同领域数据间的适应能力。
    - 模型整体鲁棒性评估：可参考项目平台https://robustnessgym.com，适合作为课程期末项目探索。
  - **预训练模型的实证研究**：分析大型预训练模型到底学到了什么知识。
  - **低数据场景下的知识迁移**：探索如何在数据稀缺的任务中，从大型模型中提取知识并实现高性能（如迁移学习）。
  - **大型模型的偏见、可信度与可解释性**：==关注模型的伦理与可解释性问题。==
  - **数据增强**：研究如何通过数据扩充提升模型性能。
  - **低资源语言处理**：针对缺乏数据的小众语言开展研究。
  - **长尾稀有样本性能优化**：提升模型在罕见案例上的表现。

  这些方向体现了2021年NLP领域从“追求单一准确率”向“关注模型泛化、可解释性、伦理及实际场景适配”的转变，反映了领域在技术成熟后对落地性、鲁棒性和社会责任的重视。

![image-20251029231316902](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231316902.png)



#### 一、模型的缩放研究
- **大型模型构建**：以GPT-2、GPT-3为代表的超大模型技术门槛极高，不适合cs224n课程项目。
- **小型高性能模型构建**：是极具价值的研究方向，可作为课程项目选题，具体技术包括：
  - **模型剪枝**：通过移除冗余参数压缩模型，例如参考论文《[对应链接论文]》。
  - **模型量化**：降低参数精度以减少存储和计算消耗，例如参考论文《[对应链接论文]》。
  - **低资源问答**：探索在6GB甚至500MB资源限制下的问答任务性能，可参考项目https://efficientqa.github.io。

#### 二、高级功能的探索
- 聚焦在小规模数据和问题上实现**组合性、系统性泛化、快速学习（如元学习）**等高级能力，相关研究案例包括：
  - **BabyAI**：面向智能体学习的简化环境，论文链接https://arxiv.org/abs/2007.12770。
  - **gSCAN**：用于评估系统性泛化能力的基准，论文链接https://arxiv.org/abs/2003.05161。

这些方向体现了NLP领域在“模型效率”和“智能本质”两个维度的探索：一方面通过剪枝、量化等技术让模型在资源受限场景下可用；另一方面通过小数据场景下的高级能力研究，探索人工智能的泛化和学习本质，为课程项目提供了兼具实用性和学术价值的选题参考。



![image-20251029231557932](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231557932.png)

#### 一、数据获取的三类方式
| 方式                    | 具体说明                                                     | 价值与意义                                                   |
| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 自主收集数据            | 可使用无监督数据、小量标注数据，或从带点赞、评分的网站获取标注（如社交平台的互动数据） | 能直面机器学习/自然语言处理在真实场景中的挑战，培养数据工程能力 |
| 利用既有项目/公司数据   | 若有研究项目或公司的现成数据，可在提供数据样本的前提下使用   | 适合资源有限时快速开展研究                                   |
| 使用现有 curated 数据集 | **采用前人整理的数据集**                                     | 可快速启动项目，且有明确的前人工作和基准模型可参考对比       |

这些方式覆盖了**从“完全自主”到“依赖既有资源”的全路径，体现了科研中数据获取的灵活性**——既鼓励探索真实场景的数据挑战，也支持借助现有资源高效开展研究，为不同背景的研究者提供了适配的策略。

![image-20251029231616955](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231616955.png)



#### 一、语言数据联盟（Linguistic Data Consortium, LDC）
- **资源链接**：https://catalog.ldc.upenn.edu/
- **斯坦福访问方式**：通过https://linguistics.stanford.edu/resources/resources-corpora注册获取权限（仅限斯坦福用途）。
- **数据类型**：包含树库、命名实体、共指数据、干净的新闻文本、带转录的语音数据、机器翻译平行语料等。
- **使用建议**：可浏览其目录查找所需数据，但需注意仅限斯坦福相关研究使用。

#### 二、LDC的价值与意义
LDC是自然语言处理领域的权威数据资源库，提供了大量经过整理和标注的高质量语料，覆盖句法、语义、语音、机器翻译等多个方向。例如其“Top Ten Corpora”中的TIMIT语音库、OntoNotes语义标注库等，都是领域内的经典基准数据，为科研和教学提供了重要支撑，帮助研究者快速开展实验并与前人工作对比。



![image-20251029231753851](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231753851.png)

#### 一、机器翻译资源
- **资源链接**：http://statmt.org
- **重点资源**：各类WMT共享任务（国际机器翻译评测任务）。

#### 二、statmt.org的价值与意义
该网站是统计机器翻译领域的权威资源平台，聚焦于“通过大量平行文本学习翻译”的统计机器翻译研究。其提供的资源包括：
- **经典文献**：如Brown等人的《统计机器翻译的数学基础》、Kevin Knight的《统计机器翻译手册》等，是领域入门的理论基石。
- **工具与数据**：Moses翻译系统、新闻评论语料库等，支持研究者快速开展实验。
- **评测任务**：WMT系列共享任务是机器翻译领域的权威 benchmark，每年吸引全球研究者参与，推动了机器翻译技术的迭代，例如从统计方法到神经机器翻译的演进。

这些资源为机器翻译的科研、教学和工程实践提供了从理论到数据、从工具到评测的全链路支持，是领域内不可或缺的知识枢纽。



![image-20251029231808118](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231808118.png)



#### 一、依存分析资源：通用依存（Universal Dependencies, UD）
- **资源链接**：https://universaldependencies.org
- **UD的定义与规模**：UD是一个跨语言语法标注框架，由超200名贡献者共同维护，涵盖70多种语言的100多个树库。

#### 二、UD的价值与意义
UD为依存分析研究提供了**跨语言统一的标注标准**，让不同语言的句法分析可以在同一框架下对比和研究。其资源包括：
- **标注指南与工具**：提供详细的标注规范和处理工具，支持研究者开展数据标注和模型训练。
- **树库查询与下载**：通过SETS、PML Tree Query等平台可在线查询树库，也可直接下载用于实验。
- **社区与交流**：支持邮件列表订阅和GitHub议题讨论，方便研究者交流标注问题和技术细节。

这种标准化的资源体系，极大降低了跨语言句法分析的研究门槛，推动了依存分析在多语言场景下的发展，是自然语言处理句法领域的核心基础设施之一。

![image-20251029231935415](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231935415.png)

![image-20251029231943998](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231943998.png)

![image-20251029231957825](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029231957825.png)



#### 一、数据资源的补充渠道
| 渠道         | 具体说明                                                     | 价值                                  |
| ------------ | ------------------------------------------------------------ | ------------------------------------- |
| Kaggle       | 知名数据科学竞赛平台，提供大量开源数据集                     | 覆盖多领域NLP任务，适合实战研究       |
| 研究论文     | 参考论文中使用的数据集                                       | 可复现前人实验，开展对比研究          |
| 数据集列表   | 如https://machinelearningmastery.com/datasets-natural-language-processing/、https://github.com/niderhoff/nlp-datasets | 整合了NLP领域常用数据集，方便快速检索 |
| 特定任务资源 | 如GLUE基准（https://gluebenchmark.com/tasks）、斯坦福情感分析数据集（https://nlp.stanford.edu/sentiment/）、Facebook bAbI（https://research.fb.com/downloads/babi/） | 聚焦细分任务，提供权威基准            |
| 课程支持     | 可在Ed平台提问或咨询课程 staff                               | 针对课程项目提供个性化数据建议        |

这些渠道体现了NLP领域数据资源的丰富性和可获取性，从竞赛平台到学术资源再到课程支持，为研究者提供了从“通用数据”到“细分任务数据”的全维度选择，帮助不同需求的研究者快速定位合适的数据集开展工作。

![image-20251029232701937](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029232701937.png)



#### 一、研究示例：简单课程项目——将神经网络应用于任务
| 步骤       | 具体说明                                                     | 价值与建议                                                   |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 定义任务   | 示例：文本摘要                                               | 选择明确的NLP任务，聚焦问题边界                              |
| 定义数据集 | 1. 搜索学术数据集<br>（如Newsroom摘要数据集http://lil.nlp.cornell.edu/newsroom/，已有基准模型可对比）<br>2. 自主定义数据（难度更高，需建立新基准）<br>（示例：从新闻故事生成广告推文，鼓励创造性地连接研究与现实需求） | 学术数据集可快速开展实验，自主数据能探索新问题，两者结合可兼顾效率与创新性 |

这种流程体现了课程项目从“任务定义”到“数据选择”的清晰路径，既支持基于既有资源的快速验证，也鼓励探索新颖的研究方向，帮助学生在实践中理解科研的“问题-数据-方法”闭环。

![image-20251029232755960](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029232755960.png)



#### 一、课程项目流程：神经网络应用于任务（续）
| 步骤         | 具体说明                                                     | 价值与建议                                                   |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据集清洗   | 项目初期需划分开发测试集（devtest）和测试集                  | 确保模型评估的客观性，避免数据泄露                           |
| 定义评估指标 | 1. 搜索该任务的成熟指标<br>2. **文本摘要任务示例**：Rouge（基于n-gram与人工摘要的重叠度评估）<br>3. **人工评估仍是摘要任务的更优方式**（可小规模邀请朋友参与） | 选择合适的指标能准确衡量模型性能，人工评估可补充自动指标的不足，提升结果可信度 |

这些步骤体现了科研项目中“数据管理”和“结果评估”的规范性，确保实验从数据划分到性能衡量都具备科学性，帮助学生建立严谨的研究思维。



![image-20251029232919370](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029232919370.png)



##### 5. 建立基准模型
- 先实现最简单的模型（例如，基于单字、双字的逻辑回归，或词向量平均模型）
  - 文本摘要任务示例：可参考LEAD-3基准（取文本前3段作为摘要）
- 仅在训练集和开发集上计算指标，不要用测试集
- 分析错误
- 若指标极佳且无错误：
  - 完成！问题太简单了，需要重启项目。😃/😞

##### 6. 实现现有神经网络模型
- 在训练集和开发集上计算指标
- 分析输出和错误
- 本课程的最低要求

**解释**：
这部分内容是科研项目中“模型迭代”的核心流程。建立基准模型的目的是先明确任务的“ baseline （基线）”——通过简单模型判断问题难度，若简单模型就表现极好，说明任务缺乏研究价值，需更换方向。实现现有神经网络模型则是科研的“入门要求”，通过复现已有模型，确保项目在技术路径上可行，同时为后续的模型改进（如创新结构、优化策略）提供对比基准。这种“从简单到复杂”的步骤设计，能帮助研究者科学地验证思路、规避风险，是科研项目管理的实用方法论。



![image-20251029233138112](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233138112.png)



#### 7. 始终贴近数据（最终测试集除外）
- 可视化数据集
- 收集统计摘要
- 分析错误
- 研究不同超参数对性能的影响

#### 8. 尝试不同模型及模型变体
通过良好的实验设置实现快速迭代，可尝试的模型包括：
- 固定窗口神经网络模型
- 循环神经网络
- 递归神经网络
- 卷积神经网络
- 基于注意力的模型/Transformer

**解读**：
步骤7强调了“数据感知”在科研中的重要性——通过可视化、统计分析和错误研究，研究者能深入理解数据分布和模型短板，为后续优化提供方向。步骤8则体现了“模型迭代”的科研逻辑，通过尝试不同类型的神经网络（从传统的RNN、CNN到前沿的Transformer），在对比中找到最适配任务的模型架构。这种“数据驱动+多模型对比”的思路，是确保科研项目既有理论深度又有实践价值的关键方法。

![image-20251029233157348](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233157348.png)



#### 数据的 “容器”（数据集划分与使用）

- 许多公开可用的数据集都采用 train/dev/test（训练 / 开发 / 测试）的结构发布。
- **我们都遵循诚信原则，仅在开发完成后才进行测试集的运行。**
- 这种划分假设数据集规模相当大。
- 如果没有开发集，或者你想要一个单独的调优集，可以通过拆分训练数据来创建。
  - 我们必须权衡调优集达到一定规模的实用性与训练集规模减少的影响。
  - 交叉验证（详见相关内容）是在数据不多时最大化数据利用率的技术。
- 固定的测试集确保所有系统都在同一套 “黄金数据” 上评估。这通常是有益的，但如果测试集具有异常属性，可能会扭曲任务的进展，这是有问题的。



![image-20251029233402248](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233402248.png)



#### 模型训练与数据“容器”
- **过拟合现象**：训练时，模型会对训练数据过拟合。
  - 模型能准确描述训练数据中的特定模式，但这些模式不够通用，难以应用于新数据。
- **过拟合的监控与避免**：通过独立的验证集和测试集来监控并避免有问题的过拟合……
  - （下方图表展示了训练过程中，训练集错误随时间持续下降，而验证集错误在达到最低点后上升，直观体现了过拟合的发生时机。）

**解读**：
这部分内容核心阐述了**“过拟合”的概念与应对策略**。过拟合是模型训练中的常见问题，表现为模型在训练数据上表现极佳，但在新数据上泛化能力差。而独立的验证集和测试集是检测与避免过拟合的关键工具——通过观察验证集性能的变化（如图表中验证集错误的上升），研究者可以及时停止训练，防止模型过度拟合训练数据。这种“数据划分+性能监控”的方法，是保障模型泛化能力的核心实践，贯穿于从科研到工程的所有机器学习项目中。



![image-20251029233414539](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233414539.png)



- 你在**训练集**上构建（估计/训练）模型。
- 通常，你会在另一组独立的数据（**调优集**）上设置进一步的超参数
  - 调优集是超参数的“训练集”！
- 你在**开发集**（开发测试集或验证集）上衡量项目进展
  - 如果你频繁这样做，会对开发集过拟合，因此增设第二个开发集（**dev2集**）会很有帮助
- 仅在最后阶段，你在**测试集**上评估并呈现最终结果
  - **最终测试集的使用次数应极少……理想情况下仅使用一次**

**解释**：
这部分内容系统定义了机器学习项目中**数据集的分层作用与使用规范**。训练集是模型学习的基础；调优集聚焦超参数优化，避免超参数对训练集的过拟合；开发集用于过程性评估，增设dev2集可进一步降低过拟合风险；测试集则是“最终裁判”，严格限制使用次数以保证结果的客观性。这种分层设计是科研和工程中保障模型泛化能力、确保实验科学性的核心实践，体现了“数据划分-模型优化-结果验证”的闭环逻辑。

![image-20251029233535327](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233535327.png)

- **训练集、调优集、开发集和测试集必须完全区分开。**
- 用训练过的材料进行测试是无效的。
  - 你会得到虚假的良好性能。
  - 我们几乎总会在训练集上发生过拟合。
- 你需要一个独立的调优集。
  - 如果调优集和训练集相同，超参数就无法正确设置。
- 如果你持续在同一个评估集上运行实验，你会开始对该评估集过拟合。
  - 实际上你是在“训练”评估集……你在学习该特定评估集上有效和无效的东西，并利用这些信息。
- **为了得到系统性能的有效衡量，你需要另一个未训练过的、独立的测试集……因此有了dev2集和最终测试集。**

**解释**：
这部分内容进一步强调了**数据集独立性和过拟合避免**的关键原则。训练、调优、开发、测试集的完全区分是为了确保每一步评估的客观性——用训练过的数据集测试会导致结果虚高，超参数调优集与训练集重叠会让超参数失去泛化价值，反复在同一开发集上迭代则会让模型“适配”该开发集而非真实场景。引入dev2集和严格限制使用次数的最终测试集，是为了在层层验证中排除过拟合的干扰，最终得到模型在真实场景下的可靠性能。这些规则共同构成了机器学习实验“科学性”的底线，确保研究结论的可信度。

![image-20251029233740531](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233740531.png)

- **以积极心态开始！**
  - **神经网络具有学习欲！**
    - 若网络不学习，是你在某些方面阻碍了它的成功学习。
- **认清残酷现实：**
  - **存在大量因素会导致神经网络完全不学习或学习效果很差**
    - 排查并修复这些问题（“调试与调优”）往往比实现模型耗时更久。
- **很难弄清楚这些问题具体是什么**
  - 但经验、严谨的实验和经验法则会有所帮助！

**解释**：
这部分内容从**心态建设和实践认知**两个层面，为神经网络训练的研究者提供了指引。积极心态帮助研究者建立对模型学习能力的信心，明确问题源于人为因素；对“调试耗时”的认知则让研究者做好投入时间解决问题的准备。而“经验、实验严谨性和经验法则”的强调，点明了神经网络训练是“实践驱动”的过程，需要在不断试错中积累解决问题的能力，这些认知是成功开展神经网络研究的基础。

![image-20251029233853716](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233853716.png)



- **模型对学习率敏感**
  - 出自Andrej Karpathy的CS231n课程笔记
  - （下方图表展示了不同学习率下损失函数随训练轮次（epoch）的变化：
    - 极高学习率（very high learning rate）：损失先降后升，模型无法收敛；
    - 低学习率（low learning rate）：损失下降缓慢，收敛效率低；
    - 高学习率（high learning rate）：损失下降较快，但最终收敛精度不足；
    - **合适的学习率**（good learning rate）：损失快速且稳定下降，收敛效果最佳。）

**解释**：
这部分内容直观阐述了**学习率对神经网络训练的关键影响**。学习率是控制模型参数更新幅度的超参数，其取值直接决定了模型的收敛速度和最终性能——过高会导致模型震荡不收敛，过低则训练效率低下，只有合适的学习率才能让模型快速且稳定地学习到有效模式。这种“学习率调优”是神经网络训练中的核心环节，研究者需通过实验找到适配任务的最佳学习率，才能保障模型的训练效果。

![image-20251029233913803](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233913803.png)



![image-20251029233938714](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029233938714.png)

#### 训练（门控）循环神经网络（RNN）
1. 使用LSTM（或GRU）：这会让你的工作简单得多！
2. 将循环矩阵初始化为正交矩阵
3. 以合理（**小！**）的尺度初始化其他矩阵
4. 将遗忘门偏置初始化为1：默认保持记忆
5. 使用自适应学习率算法：Adam、AdaDelta等
6. 对梯度范数进行裁剪：当与Adam或AdaDelta结合使用时，1–5是合理的阈值
7. 要么仅垂直使用Dropout，要么考虑使用贝叶斯Dropout（Gal和Gahramani提出——PyTorch中不原生支持）
8. 保持耐心！优化需要时间

**引用**：[Saxe et al., ICLR2014; Ba, Kingma, ICLR2015; Zeiler, arXiv2012; Pascanu et al., ICML2013]

**解释**：
这部分内容是训练门控RNN（如LSTM、GRU）的**实战指南**。从模型选择（优先LSTM/GRU以简化训练）、参数初始化（正交矩阵、小尺度、遗忘门偏置设1）到优化策略（自适应学习率、梯度裁剪），再到正则化（Dropout的合理使用），每一步都针对门控RNN的训练痛点（如梯度消失、训练不稳定）提供了经实践验证的解决方案。这些技巧是科研和工程中快速实现门控RNN并保障其训练效果的关键，体现了“问题导向+经验驱动”的实用方法论。



![image-20251029234050540](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234050540.png)



#### 实验策略
- **逐步推进！**
- **从非常简单的模型开始，让它先运行起来！**
  - 修复复杂但有问题的模型很困难
- **逐个添加复杂组件，确保模型在添加每个组件后都能运行（或放弃该组件）**
- **初始阶段在极少量数据上运行**
  - 在小规模数据集上更容易发现bug
  - 4-8个样本这样的规模就很合适
  - 合成数据通常在这一步很有用
  - 确保在这些数据上能达到100%的性能
    - 否则，你的模型要么不够强大，要么存在故障

**解释**：
这部分内容是科研实验的**“最小可行验证”策略**。从简单模型和小规模数据入手，是为了先验证核心逻辑的可行性——简单模型易调试，小规模数据（甚至合成数据）能快速暴露bug，确保模型在“极简场景”下能正常工作后，再逐步添加复杂组件和扩大数据规模。这种“从简到繁、从少到多”的实验流程，能有效规避因模型或数据过于复杂导致的调试困境，是保障科研效率和实验科学性的实用方法论。

![image-20251029234106103](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234106103.png)

- **在大规模数据集上训练并运行模型**
  - 优化后，模型在训练数据上的得分应仍接近100%
    - 否则，你可能需要考虑更强大的模型！
    - 在深度学习中，不必害怕对训练数据过拟合
      - 这些模型通常具有良好的泛化能力，因为分布式表示通过共享统计强度来实现泛化，即使对训练数据过拟合也是如此
- **但，你仍然需要良好的泛化性能：**
  - 对模型进行正则化，直到它在开发数据上不再过拟合
    - L2正则化等策略可能有用
    - 但通常**大量使用Dropout**是成功的关键

**解释**：
这部分内容阐述了**深度学习中“拟合-泛化”的辩证关系**。在大规模数据上，模型先实现对训练数据的高拟合（甚至过拟合），是验证模型表达能力的基础——若无法拟合训练数据，说明模型能力不足。而后续通过正则化（如L2、Dropout）来控制过拟合，是为了让模型在新数据上具备泛化能力。这种“先确保拟合能力，再针对性正则化”的思路，是深度学习模型训练的典型策略，体现了对模型“能力”和“泛化”的平衡把控。

![image-20251029234257530](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234257530.png)



- **细节至关重要！**
  - 观察你的数据，收集统计摘要
  - 观察模型的输出，进行错误分析
  - 超参数调优往往对神经网络的成功至关重要

**解释**：
这部分内容强调了科研中**“细节把控”的核心地位**。观察数据并收集统计信息，能帮助研究者理解数据分布，为模型设计和预处理提供依据；分析模型输出的错误案例，是定位问题、迭代改进的关键；而超参数调优则直接影响模型的性能上限。这些细节工作共同构成了科研“从数据到模型再到结果”的闭环，是保障实验科学性和模型性能的基础。

<img src="https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234317462.png" alt="image-20251029234317462" style="zoom:67%;" />

- **6. 默认期末项目**
  - 阅读理解
    - 又名：基于文档的问答

![image-20251029234410434](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234410434.png)



![image-20251029234421630](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234421630.png)

澳大利亚的第三任总理是**约翰·克里斯蒂安·沃森（John Christian Watson）**，他通常被称为克里斯·沃森（Chris Watson），于1904年担任澳大利亚总理。

这张图片展示的是谷歌搜索结果页面，其中的“特色摘要”直接从网页中提取信息回答了“澳大利亚第三任总理是谁”的问题，而非通过谷歌知识图谱（原Freebase）这种结构化数据来回答。

![image-20251029234512881](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234512881.png)



- **动机：问答任务**
  - 面对海量的全文文档集合（例如网络），仅返回相关文档的作用是有限的
  - 相反，我们通常希望得到问题的**答案**
  - 尤其是在移动设备上
  - 或者在使用数字助手设备时，如Alexa、谷歌助手等
  - 我们可以将其拆分为两个部分：
    1. 找到（可能）包含答案的文档
       - 这可以通过传统的信息检索/网络搜索来处理
    2. 在段落或文档中找到答案
       - 这个问题通常被称为**阅读理解**

**解释**：
这部分内容阐述了**问答任务的现实需求和技术拆解**。在网络等海量文档场景下，用户需要的是直接的答案而非相关文档，这种需求在移动设备和智能助手上更为突出。为了实现问答，技术上可拆分为“文档检索”和“阅读理解”两个环节——前者依靠传统信息检索技术筛选可能包含答案的文档，后者则聚焦于从文档中提取具体答案，这也是自然语言处理中“阅读理解”任务的核心目标。这种拆解清晰地界定了问答系统的技术边界，为后续的模型设计和任务实现提供了方向。

![image-20251029234532349](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234532349.png)

- **斯坦福问答数据集（SQuAD）**
  （Rajpurkar等，2016）
  - **问题**：哪支球队赢得了第50届超级碗？
  - **段落**
    第50届超级碗是一场美式橄榄球比赛，用于确定2015赛季美国国家橄榄球联盟（NFL）的冠军。美国橄榄球联合会（AFC）冠军丹佛野马队以24-10击败国家橄榄球联合会（NFC）冠军卡罗莱纳黑豹队，赢得了他们的第三个超级碗冠军。比赛于2016年2月7日在加利福尼亚州圣克拉拉旧金山湾区的 Levi's 体育场举行。
  - 10万条示例
  - **答案必须是段落中的一个片段**
  - 又名：**抽取式(extractive)问答**

**解释**：
SQuAD是自然语言处理领域中**抽取式问答任务的标杆数据集**。它要求模型从给定段落中抽取连续的文本片段作为问题的答案，这种“抽取式”设定明确了任务边界，便于模型训练和性能评估。该数据集的大规模（10万条示例）使其成为衡量问答模型能力的重要基准，推动了BERT等众多预训练模型在问答任务上的发展。

![image-20251029234547897](/Users/zjj/Library/Application Support/typora-user-images/image-20251029234547897.png)

- **斯坦福问答数据集（SQuAD）**
  - 段落
    私立学校，也被称为独立学校、非政府学校或非州立学校，不由地方、州或国家政府管理；因此，它们保留选择学生的权利，其资金全部或部分来自向学生收取学费，而非依赖公共（政府）资金的强制税收；在一些私立学校，学生可能能够获得奖学金，这会降低成本，具体取决于学生可能拥有的才能（例如体育奖学金、艺术奖学金、学术奖学金）、经济需求或可能提供的税收抵免奖学金。
  - 问题1：除了非政府学校和非州立学校，私立学校的另一个名称是什么？
    标准答案：① independent ② independent schools ③ independent schools
  - 问题2：除了体育和艺术，哪类是基于才能的奖学金？
    标准答案：① academic ② academic ③ academic
  - 问题3：私立学校主要依靠什么 funding，而非税收？
    标准答案：① tuition(学费) ② charging their students tuition ③ tuition

**解释**：
这部分内容进一步展示了**SQuAD数据集的问答形式**。每个问题都要求从给定段落中抽取连续的文本片段作为答案，且同一问题可能有多个等效的标准答案（如“independent”和“independent schools”都可作为私立学校的别名）。这种设计既贴近真实的语言表达多样性，又为模型评估提供了灵活且严谨的标准，是SQuAD成为抽取式问答领域标杆数据集的重要原因。

![image-20251029234825055](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234825055.png)



- **SQuAD评估，v1.1**
  - 作者收集了3个标准答案
  - 系统通过两个指标评分：
    - 精确匹配：是否匹配3个答案中的一个，以0/1计准确率
    - F1：将系统答案和每个标准答案视为词袋，评估精确率（$ \text{Precision} = \frac{TP}{TP+FP} $）、召回率（$ \text{Recall} = \frac{TP}{TP+FN} $），并计算调和均值F1（$ \text{F1} = \frac{2PR}{P+R} $）。得分是每题F1的（宏观）平均值
  - F1指标被视为更可靠，作为主要评估指标
    - 它较少依赖于完全匹配人类选择的片段，而这种完全匹配易受换行等因素影响
  - 两个指标均忽略标点和冠词（仅a、an、the）

**解释**：
这部分内容定义了**SQuAD数据集的评估标准**。精确匹配是“非对即错”的严格评估，而F1通过词袋模型衡量答案与标准答案的语义重叠度，更具鲁棒性，因此成为主要指标。这种评估设计既保证了对答案准确性的衡量，又兼容了自然语言表达的多样性，是SQuAD成为问答领域权威基准的重要支撑。

![image-20251029234838119](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234838119.png)

- **SQuAD 2.0示例：新增不可回答问题**
  - 段落
    成吉思汗于1206年统一了草原上的蒙古和突厥部落，成为大汗。他和他的继承者将蒙古帝国扩张至整个亚洲。在成吉思汗第三子窝阔台汗统治时期，蒙古人于1234年摧毁了衰落的金朝，征服了中国北方大部分地区。窝阔台向侄子忽必烈提供了河北邢州的职位。忽必烈不会读中文，但他早年就由母亲唆鲁禾帖尼为他配备了几位汉族教师。他寻求中国佛教和儒家顾问的建议。蒙哥汗于1251年接替窝阔台的儿子贵由成为大汗。
  - 问题：成吉思汗何时杀死了大汗？
  - 标准答案：<无答案>
  - 预测答案：1234 [来自微软nlnet]

**解释**：
SQuAD 2.0在SQuAD 1.1的基础上**新增了不可回答问题的类别**，即段落中不存在问题的答案。这种设计更贴近真实场景（用户的问题可能无法从给定文档中得到解答），要求模型不仅能抽取答案，还要能判断“无答案”的情况。示例中模型错误地预测了“1234”作为答案，说明在处理不可回答问题时，模型需要具备识别“无答案”的能力，这也是SQuAD 2.0对模型的核心挑战之一。

![image-20251029234942997](https://gitee.com/zhang-junjie123/picture/raw/master/image/image-20251029234942997.png)