# 《从Scratch构建语言模型》（CS336，2025春季）详细笔记

## 一、课程介绍

### 1. 课程背景
这是斯坦福CS336课程的第二次开设，规模较上一次扩大50%。所有讲座将在YouTube上发布，面向全球开放。

### 2. 课程开设原因
- **研究人员与底层技术脱节**：
  - 8年前：研究人员自己实现和训练模型
  - 6年前：下载模型（如BERT）并微调
  - 现在：直接提示专有模型（如GPT-4、Claude、Gemini）
- 虽然抽象层级提升提高了生产力，但这些抽象是“有漏洞的”，对于需要重构技术栈的基础研究，全面理解技术至关重要。本课程通过“构建”来实现理解。

### 3. 工业化挑战
- 前沿模型（如GPT-4）参数规模巨大（据称1.8万亿）、训练成本极高（据称1亿美元），且没有公开的构建细节，对课程参与者来说遥不可及。
- 但课程聚焦**可迁移的知识**：
  - 机制（Mechanics）：如Transformer原理、模型并行如何利用GPU
  - 思维模式（Mindset）：如何榨取硬件效能、重视缩放规律
  - 直觉（Intuitions）：部分可迁移的，关于数据和建模决策如何影响精度的认知

### 4. 核心思想：效率驱动设计
-  bitter lesson的正确理解：**可缩放的算法才重要**
- 公式：accuracy = efficiency × resources
- 效率在大规模时更重要，因为无法承受浪费。

## 二、语言模型发展历程

### 1. 前神经时代（2010年代前）
- 香农1950年：用语言模型测量英语的熵
- n-gram语言模型：用于机器翻译、语音识别（如Brants等人2007年的工作）

### 2. 神经组件时代（2010年代）
- 首个神经语言模型（Bengio等人2003）
- 序列到序列建模（Sutskever等人2014）
- Adam优化器（Kingma等人2014）
- 注意力机制（Bahdanau等人2014）
- Transformer架构（Vaswani等人2017）
- 混合专家（Shazeer等人2017）
- 模型并行技术（Huang等人2018；Rajbhandari等人2019；Shoeybi等人2019）

### 3. 早期基础模型（2010年代末）
- ELMo：基于LSTM的预训练+微调（Peters等人2018）
- BERT：基于Transformer的预训练+微调（Devlin等人2018）
- Google T5（11B）：文本到文本统一框架（Raffel等人2019）

### 4. 规模化与封闭化阶段
- GPT-2（1.5B）：流畅文本生成，零样本能力初现（Radford等人2019）
- 缩放定律（Kaplan等人2020）：为规模化提供预测性
- GPT-3（175B）：上下文学习，封闭模型（Brown等人2020）
- PaLM（540B）：超大模型，训练不足（Chowdhery等人2022）
- Chinchilla（70B）：计算最优缩放（Hoffmann等人2022）

### 5. 开放模型阶段
- EleutherAI：开放数据集The Pile和模型GPT-J（Gao等人2020；Wang等人2021）
- Meta OPT（175B）：GPT-3复现，硬件问题多（Zhang等人2022）
- BLOOM：聚焦数据来源（BigScience Workshop等人2022）
- Meta Llama系列（Touvron等人2023；Grattafiori等人2024）
- 阿里Qwen系列（Qwen等人2024）
- DeepSeek系列（DeepSeek-AI等人2024）
- AI2 OLMo 2（Groeneveld等人2024；OLMo等人2024）

### 6. 开放层级
- 封闭模型（如GPT-4o）：仅API访问
- 开放权重模型（如DeepSeek）：权重可用，有架构细节和部分训练细节，无数据细节
- 开源模型（如OLMo）：权重和数据都可用，论文细节丰富（但不一定包含设计 rationale和失败实验）

### 7. 当前前沿模型
- OpenAI o3
- Anthropic Claude Sonnet 3.7
- xAI Grok 3
- Google Gemini 2.5
- Meta Llama 3.3
- DeepSeek r1
- 阿里Qwen 2.5 Max
- 腾讯Hunyuan-T1

## 三、课程形式与后勤

### 1. 课程形式
- 可执行讲座：所有内容以代码形式呈现，可查看、运行代码，清晰呈现讲座层级结构，支持概念跳转（如跳转到“监督微调”定义）。

### 2. 课程属性
- 5学分课程，作业量较大（有评价称首次作业工作量相当于CS224n全部5次作业加期末项目）。

### 3. 选课建议
- 建议选的情况：有强烈的“刨根问底”需求、想锻炼研究工程能力
- 建议不选的情况：本季度需完成研究、对AI最新技术（如多模态、RAG）感兴趣、仅想在自身应用领域出成果

### 4. 跟随方式
- 所有讲义和作业在线发布，讲座通过CGOE录制后在YouTube发布（有延迟），课程计划下一年再开。

### 5. 作业
- 5次作业，涵盖基础、系统、缩放定律、数据、对齐方向
- 无脚手架代码，但提供单元测试和适配器接口
- 需本地实现验证正确性，再在集群上运行基准测试
- 部分作业设排行榜（如给定训练预算最小化困惑度）
- 需谨慎使用AI工具（如CoPilot）

### 6. 计算集群
- 由Together AI提供，需提前阅读使用指南，且作业需尽早启动

## 四、课程核心模块

### 1. 基础（basics）
- 目标：让完整pipeline跑通，包含分词、模型架构、训练三大组件。

#### （1）分词（Tokenization）
- 分词器实现字符串与整数序列（token）的转换，直觉是将字符串拆为“流行片段”。
- 课程聚焦字节对编码（BPE）分词器（Sennrich等人2015），同时提及无分词器方法（直接处理字节，有前景但尚未规模化到前沿）。

#### （2）模型架构
- 以原始Transformer（Vaswani等人2017）为起点，涉及多种变体：
  - 激活函数：ReLU、SwiGLU（Shazeer 2020）
  - 位置编码：正弦、RoPE（Su等人2021）
  - 归一化：LayerNorm（Ba等人2016）、RMSNorm（Zhang等人2019）
  - 归一化位置：预归一化 vs 后归一化（Xiong等人2020）
  - MLP：dense、混合专家（Shazeer等人2017）
  - 注意力：全注意力、滑动窗口、线性注意力（Jiang等人2023；Katharopoulos等人2020）
  - 低维注意力：组查询注意力（GQA）、多头潜在注意力（MLA）（Ainslie等人2023；DeepSeek-AI等人2024）
  - 状态空间模型：如Hyena（Poli等人2023）

#### （3）训练
- 优化器：如AdamW（Loshchilov等人2017）、Muon（Keller 2024）、SOAP（Vyas等人2024）
- 学习率调度：如余弦（Loshchilov等人2016）、WSD（Hu等人2024）
- 批大小：如临界批大小（McCandlish等人2018）
- 正则化：如dropout、权重衰减
- 超参数：头数、隐藏维度等的网格搜索

#### （4）作业1
- 实现BPE分词器、Transformer、交叉熵损失、AdamW优化器、训练循环
- 在TinyStories和OpenWebText上训练
- 排行榜目标：在H100上用90分钟最小化OpenWebText困惑度

### 2. 系统（systems）
- 目标：榨取硬件最大效能，包含内核、并行、推理三大组件。

#### （1）内核
- 需理解GPU（如A100）架构，通过组织计算来最大化GPU利用率（最小化数据移动）
- 需用CUDA/Triton/CUTLASS/ThunderKittens编写内核

#### （2）并行
- 多GPU（如8块A100）场景下，数据移动更慢，需遵循“最小化数据移动”原则
- 使用集合操作（如gather、reduce、all-reduce）
- 在GPU间分片（参数、激活、梯度、优化器状态）
- 采用数据并行、张量并行、流水线并行、序列并行等拆分方式

#### （3）推理
- 目标：根据提示生成token，推理计算（每次使用）在全局上超过训练计算（一次性成本）
- 分为预填充（类似训练，可一次性处理所有token，计算受限）和解码（需逐一生成token，内存受限）阶段
- 加速解码的方法：
  - 使用更廉价模型（剪枝、量化、蒸馏）
  - 推测解码（用廉价“草稿”模型并行生成多个token后用全模型评分）
  - 系统优化（KV缓存、批处理）

#### （4）作业2
- 用Triton实现融合RMSNorm内核
- 实现分布式数据并行训练
- 实现优化器状态分片
- 对实现进行基准测试和性能分析

### 3. 缩放定律（scaling_laws）
- 目标：小规模实验，预测大规模下的超参数和损失
- 核心问题：给定计算预算（$C$），是用更大模型（$N$）还是更多token（$D$）训练？
- 计算最优缩放定律（Kaplan等人2020、Hoffmann等人2022）：$D^* = 20 N^*$（如14亿参数模型应在280亿token上训练），但未考虑推理成本

#### 作业3
- 定义基于过往运行的训练API（超参数→损失）
- 提交“训练作业”（在计算预算下）并收集数据点
- 拟合缩放定律
- 提交缩放后超参数的预测
- 排行榜目标：在计算预算下最小化损失

### 4. 数据（data）
- 核心问题：模型应具备什么能力（多语言、代码、数学等）

#### （1）评估
- 困惑度：语言模型的经典评估
- 标准化测试：如MMLU、HellaSwag、GSM8K
- 指令遵循：如AlpacaEval、IFEval、WildBench
- 测试时计算缩放：如思维链、集成
- LM作为评判者：评估生成任务
- 全系统：如RAG、智能体

#### （2）数据策展
- 数据来源：网络爬取页面、书籍、arXiv论文、GitHub代码等
- 版权问题：涉及“合理使用”争议，或需授权（如谷歌与Reddit数据）
- 格式：HTML、PDF、目录（非文本）

#### （3）数据处理
- 转换：将HTML/PDF转为文本（保留内容和部分结构）
- 过滤：保留高质量数据，移除有害内容（通过分类器）
- 去重：节省计算，避免记忆；使用布隆过滤器或MinHash

#### （4）作业4
- 将Common Crawl HTML转为文本
- 训练分类器过滤质量和有害内容
- 用MinHash去重
- 排行榜目标：在token预算下最小化困惑度

### 5. 对齐（alignment）
- 基础模型是“原始潜力”，擅长下一个token预测，对齐使其真正有用
- 对齐目标：让模型遵循指令、调整风格（格式、长度、语气等）、融入安全性（如拒绝回答有害问题）
- 分为**监督微调（supervised_finetuning）**和**从反馈中学习（learning_from_feedback）**两个阶段

#### （1）监督微调（SFT）
- 基于（提示，响应）对的指令数据
- 直觉：基础模型已有技能，只需少量示例即可激发
- 监督学习：微调模型以最大化p(response | prompt)

#### （2）从反馈中学习
- 基于偏好数据（对给定提示生成多个响应，用户给出偏好）
- 验证器：形式化验证器（如代码/数学验证）或学习到的验证器（如LM作为评判者）
- 算法：如近端策略优化（PPO）、直接偏好优化（DPO）、组相对偏好优化（GRPO）

#### （3）作业5
- 实现监督微调
- 实现直接偏好优化（DPO）
- 实现组相对偏好优化（GRPO）

## 五、深入探讨：分词 (Tokenization)

- **目的:** 分词器是连接 **字符串 (strings)** 和 **整数序列 (tokens)** 的桥梁。
- **核心权衡:** 在 **词汇表大小 (Vocabulary Size)** 和 **序列长度 (Sequence Length)** 之间找到平衡点。



#### 1. 字符分词 (Character Tokenizer)

- **方法:** 使用 `ord()` 将每个 Unicode 字符转为整数。
- **优点:** 简单。
- **缺点:**
  1. **词汇表太大:** 约 15 万 Unicode 字符。
  2. **效率低下:** 很多字符非常罕见（如 "🌍"）。



#### 2. 字节分词 (Byte Tokenizer)

- **方法:** 将字符串用 `string.encode("utf-8")` 编码为字节序列（整数 0-255）。
- **优点:**
  1. **词汇表很小:** 固定为 256。
  2. **无歧义:** 能处理所有文本。
- **缺点:**
  1. **压缩率极差 (1.0):** 序列会变得**非常长**（例如 "🌍" 变为 4 个 token）。
  2. **计算灾难:** 对于上下文长度有限且注意力计算为 $O(N^2)$ 的 Transformer 来说，这是不可接受的。

> byte-based tokenizer（基于字节的分词器）是大语言模型中常用的分词方案，核心结论是：**它以 “字节（byte）” 为最小单位构建词汇表，避免未登录词（OOV）问题，同时平衡分词效率与语义完整性**。
>
> #### 核心定义
>
> 分词器（tokenizer）的作用是把文本拆成模型能处理的基本单元（token），而 byte-based tokenizer 的核心特点是：
>
> - 词汇表的 “基础元素” 是字符对应的字节（而非完整字符或子词），比如英文单词 “apple” 先转成 ASCII 字节，中文汉字转成 UTF-8 字节；
> - 在此基础上，通过**算法（如 BPE、WordPiece）合并高频字节组合**，形成更大的 token，兼顾 “无 OOV” 和 “分词效率”。
>
> #### 典型工作流程（以 GPT-2 的 BytePairEncoding 为例）
>
> 1. **文本转字节**：将输入文本按编码格式（如 UTF-8）转换为字节序列。例如，中文 “你好” 转 UTF-8 字节是 `0xE4 0xBD 0xA0 0xE5 0xA5 0xBD`。
> 2. **初始化词汇表**：词汇表初始包含所有 256 个 UTF-8 字节，每个字节作为独立 token。
> 3. **合并高频字节对**：统计语料中所有相邻字节对的出现频率，反复合并频率最高的字节对，直到词汇表达到预设大小（如 GPT-2 的词汇表规模是 50257）。
> 4. **文本分词**：用最终的词汇表对新文本进行拆分，优先匹配最长的 token 组合。
>
> 

#### 3. 词分词 (Word Tokenizer)

- **方法:** 使用正则表达式（如 `r"\w+|."`）将文本分割成单词。
- **优点:** 序列短，压缩率高。
- **缺点:**
  1. **词汇表巨大:** 语言中所有单词。
  2. **无法处理未登录词 (OOV):** 必须使用 `<UNK>` (Unknown) 标记，这会丢失信息。



#### 4. 字节对编码 (Byte Pair Encoding, BPE)

- **核心思想:** BPE 是一个数据压缩算法，在字节和词之间找到了最佳平衡点。它是一种**启发式算法 (heuristic)**。

- **直觉:** 常见的字符序列（如 " the"）应合并为单个 token；罕见的序列（如 "CS336"）应分解为多个 token。

- **BPE 训练算法 (`train_bpe`):**

  1. **初始化:** 词汇表 `vocab` 仅包含 256 个基本字节（`{0: b'\x00', ..., 255: b'\xff'}`）。

  2. **准备数据:** 将所有训练文本编码为字节整数序列 `indices`。

  3. 迭代合并: 循环 num_merges 次（num_merges 决定了最终词汇表大小）：

     a.  在 indices 中统计所有相邻 token 对的出现频率（如 (116, 104) 即 b'th'）。

     b.  找到频率最高的 token 对 pair。

     c.  创建一个新索引 new_index（如 256）。

     d.  记录这次合并：merges[pair] = new_index。

     e.  更新词汇表：vocab[new_index] = vocab[pair[0]] + vocab[pair[1]] (例如 vocab[256] = b'th')。

     f.  替换: 遍历 indices，将所有 pair 替换为 new_index。

- **BPE 编码 (`encode`):**

  1. 将输入字符串转为字节序列 `indices`。
  2. **按顺序**遍历所有学到的 `merges` 规则，替换 `indices` 中的字节对。

- **BPE 解码 (`decode`):**

  1. 使用 `vocab` 将 `indices` 列表中的每个整数映射回其对应的 `bytes`。
  2. `b"".join()` 拼接所有字节，然后 `.decode("utf-8")`。

- **总结:** 分词是目前的一个**“必要的恶” (necessary evil)**。BPE 是一个有效的启发式方法，也许未来可以直接在字节上操作。