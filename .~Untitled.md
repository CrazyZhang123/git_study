# 译文

## 3.2 表示对齐（Representation Alignment）

我们的目标是对与“负向特征”（如“不真实性”）相关的激活向量进行干预，使其在多属性类型下更接近与“正向特征”（如“真实性”）相关的激活向量。然而，并非所有属性或场景都存在“包含同一提示及对应正负响应”的配对数据（\(x, y_p, y_n\)），因此需要从“反事实响应”中学习——即对于已标注的负向响应\(y_n\)，推测其对应的正向响应\(y_p\)应当是什么。

为此，我们采用**最大均值差异损失（Maximum Mean Discrepancy, MMD；Gretton等人，2012）**。该损失无需显式的样本配对，即可对整个分布进行比较。此外，现有ITI研究（Li等人，2024；Zou等人，2023）中使用的传统损失通常仅关注“匹配低阶统计量”（如均值），这可能会遗漏方差等关键的高阶差异；与之相反，MMD通过将数据映射到“再生核希尔伯特空间（Reproducing Kernel Hilbert Space, RKHS）”，能够捕捉高阶矩信息，为分布提供更丰富、更完整的表征。这使得我们的模型能够识别并修正“正负样本激活分布”之间的差异，从而实现更有效的干预。通过最小化MMD损失，我们促使“编辑后的激活向量\(f(a_i \mid \theta_1, \dots, \theta_T)\)的分布”与“正向激活向量的分布”高度匹配，进而将负向激活向量推向期望的属性区域（见图2（A））。


## 3.3 冲突避免（Avoiding Conflicts）

通过门控机制组合多个“属性专属引导向量”时，可能会出现属性间冲突。例如，用于“抑制偏见”的引导向量与用于“提升有用性”的引导向量，若同时作用于同一个token，可能会相互抵消效果（见图2）。为应对这一挑战，我们引入了多个互补的正则化目标：

### 正向样本保留（Preservation of Positive Samples）

对于已与正向属性对齐的激活向量，我们需避免施加不必要的干预。因此，我们引入一个惩罚项，迫使门控函数对“正向激活向量”的输出接近0：  
\[ \mathcal{L}_{\text{pos}} = \sum_{t=1}^T \sum_{a_i \in \mathcal{A}_p^t} \left[ G_t(a_i) \right]^2 \tag{6} \]  
该惩罚项可保留激活向量原有的语义信息，防止“过度修正”（见图2（A））。

### 负向样本稀疏性（Sparsity for Negative Samples）

并非每个引导向量都与所有激活向量相关，因此我们仅需对“与负向行为相关的激活向量”进行选择性干预。门控输出的“稀疏性”可确保仅应用“与当前激活最相关的属性专属引导向量”。我们通过施加\(l_1\)惩罚项实现稀疏性（与\(l_2\)惩罚项“仅降低数值大小”不同，\(l_1\)惩罚项会自然促使大量数值变为0，见图2（B））：  
\[ \mathcal{L}_{\text{sparse}} = \sum_{t=1}^T \sum_{a_i \in \mathcal{A}_n^t} \left| G_t(a_i) \right| \tag{7} \]  
该正则化项限制了“活跃引导向量”的数量，从而降低属性间冲突的概率。

### 引导向量正交性（Orthogonality of Steering Vectors）

作用于同一token的两个“属性专属引导向量”可能会产生破坏性干扰（例如，在相反方向上抵消彼此的分量）。为避免这种情况，我们对引导向量施加“正交性约束”：  
\[ \mathcal{L}_{\text{ortho}} = \sum_{t=1}^T \sum_{\substack{t'=1 \\ t' \neq t}}^T \left( \frac{\theta_t^\top \theta_{t'}}{\|\theta_t\|_2 \|\theta_{t'}\|_2} \right)^2 \tag{8} \]  
通过促使引导向量相互正交，我们确保每个向量在“独特且互补的方向”上发挥作用（见图2（C）），从而最大限度减少干扰——避免针对某一属性的干预“溢出”并对其他属性产生不利影响。

需要强调的是，LLM的激活空间维度极高（通常为\(d\)维），因此只要属性数量\(T \ll d\)（例如\(d=4096\)），所有引导向量完全正交在数学上是可行的。此外，我们的公式通过“可微正则化”将正交性实现为“柔性惩罚”——它鼓励（而非强制）严格正交：训练过程中，引导向量可根据需求调整，同时随着属性数量增加，该惩罚会抑制向量间的方向重叠。这种“柔性设计”在属性存在语义关联时尤为有益：当“允许引导向量存在少量方向重叠”能提升整体性能时，模型可灵活适配[注4]。


## 3.4 归一化与总损失函数（Normalization and Overall Loss Function）

干预过程中需避免扭曲原始激活向量的“幅度”（即向量长度）。因此，在应用引导函数后，我们对“编辑后的激活向量”进行归一化处理。设\(a_i\)为“样本\(i\)中token\(j\)的原始激活向量”，\(\tilde{a}_i = f(a_i \mid \theta_1, \dots, \theta_T)\)为编辑后的激活向量，归一化公式如下：  
\[ \tilde{a}_i \leftarrow \tilde{a}_i \cdot \frac{\|a_i\|_2}{\|\tilde{a}_i\|_2} \tag{9} \]  
该步骤可保留激活向量原有的\(l_2\)范数（即向量长度），确保干预仅改变激活向量的“方向”，而非“尺度”。

总损失函数是公式（5）、（6）、（7）、（8）中各损失项的加权和：  
\[ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MMD}} + \lambda_{\text{pos}} \mathcal{L}_{\text{pos}} + \lambda_{\text{sparse}} \mathcal{L}_{\text{sparse}} + \lambda_{\text{ortho}} \mathcal{L}_{\text{ortho}} \]  
其中\(\lambda_{\text{pos}}\)、\(\lambda_{\text{sparse}}\)、\(\lambda_{\text{ortho}}\)为超参数，用于平衡各损失项的贡献权重。

构建迷你批次（mini-batch）时，我们对所有属性的样本进行打乱，确保每个批次中“各属性的正负样本数量相等”。这一操作可稳定“表示损失”的计算，并提升“稀疏性损失”与“正交性损失”计算结果的鲁棒性[注5]。


### 关键补充说明

1. **公式（6）\(\mathcal{L}_{\text{pos}}\)的惩罚逻辑**：  
   对正向激活向量的门控输出平方求和——若门控输出\(G_t(a_i)\)偏离0（即对正向激活施加了干预），惩罚项会增大，迫使模型“不触碰”已符合要求的激活向量，避免过度修正导致语义丢失。

2. **公式（7）\(l_1\)惩罚的稀疏性原理**：  
   \(l_1\)惩罚对“非零值”的惩罚力度更强（例如，\(|0.5| + |0.5| = 1\)，而\(0.5^2 + 0.5^2 = 0.5\)），因此会促使大多数门控输出变为0——仅保留“与负向激活强相关的引导向量”，减少无关向量的干扰。

3. **公式（8）正交性约束的数学意义**：  
   分子\(\theta_t^\top \theta_{t'}\)是两向量的点积（点积为0表示完全正交），分母是两向量\(l_2\)范数的乘积（用于归一化），整体表示“两向量的余弦相似度”。对余弦相似度的平方求和并最小化，可确保任意两个引导向量的方向尽可能独立（余弦相似度接近0）。

4. **公式（9）归一化的必要性**：  
   激活向量的“尺度”（长度）与模型对token的“关注度”相关——若干预改变尺度，可能导致模型过度关注或忽略某token。归一化可维持原始尺度，确保干预仅调整“激活方向”（即token的语义倾向）。