---
created: 2025-01-28T18:45
updated: 2025-01-29T13:16
---
# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
DeepSeek-R1：通过强化学习激发大语言模型的推理能力

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary(初步的) step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing(引人入胜的) reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues(问题) and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates (使并入；包含；) multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense(稠密的) models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled(蒸馏)  from DeepSeek-R1 based on Qwen and Llama.

我们推出了第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero是通过大规模强化学习（RL）训练的模型，**无监督微调（SFT）** 作为初始步骤，它展现出了卓越的推理能力。通过强化学习，DeepSeek-R1-Zero自然涌现出许多强大且引人注目的推理行为。然而，**它也面临着诸如可读性差和语言混杂等挑战。** 为了解决这些问题并进一步提升推理性能，我们推出了DeepSeek-R1，该模型**在强化学习之前融入了多阶段训练和冷启动数据**。**DeepSeek-R1在推理任务上的表现与OpenAI-o1-1217相当**。为了支持研究社区，我们开源了DeepSeek-R1-Zero、DeepSeek-R1，以及基于Qwen和Llama从DeepSeek-R1中提炼出的6个稠密模型（15亿、70亿、80亿、140亿、320亿、700亿参数规模）。

```ad-abstract
- 第一代推理模型 DeepSeek-R1-Zero 和 DeepSeek-R1。
- DeepSeek-R1-Zero 经大规模强化学习训练，无需监督微调即可展现强大推理能力，但存在可读性差、语言混杂的问题。
- DeepSeek-R1 融入多阶段训练和冷启动数据，解决了 DeepSeek-R1-Zero 存在的问题，在推理任务上性能与 OpenAI-o1-1217 相当。
- 开源了这两个模型及 6 个基于 Qwen 和 Llama 提炼的稠密模型。
```

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20250128200022.png)
					图1 :  DeepSeek-R1的基准测试性能。
## 1. Introduction

In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively(逐步地；愈益) diminishing(减少；降低；) the gap towards Artificial General Intelligence (AGI).

Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including **process-based reward models** (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), **reinforcement learning** (Kumar et al., 2024), and search algorithms such as **Monte Carlo Tree Search and Beam Search** (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI’s o1 series models.

近年来，大语言模型（LLMs）经历了快速的迭代和演进（Anthropic，2024；谷歌，2024；OpenAI，2024a），与通用人工智能（AGI）之间的差距逐渐缩小。
最近，**后训练已成为完整训练流程的重要组成部分**。**研究表明，后训练能够提高推理任务的准确性，使其符合社会价值观，并适应用户偏好，而且与预训练相比，所需的计算资源相对较少**。在推理能力方面，OpenAI的o1系列模型（OpenAI，2024b）率先通过**延长思维链推理过程的长度来引入推理时缩放机制**。这种方法在数学、编程和科学推理等各种推理任务中都取得了显著改进。然而，**如何在测试时进行有效缩放**，这一挑战对于研究界来说仍是一个悬而未决的问题。此前有多项研究探索了各种方法，包括**基于过程的奖励模型**（Lightman等人，2023；Uesato等人，2022；Wang等人，2023）、**强化学习**（Kumar等人，2024），以及**蒙特卡罗树搜索(MCTS)和束搜索(BS)** 等搜索算法（Feng等人，2024；Trinh等人，2024；Xin等人，2024）。然而，这些方法中没有一种能在通用推理性能上与OpenAI的o1系列模型相媲美。

In this paper, we **take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL)**. Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.

在本文中，我们迈出了**使用纯强化学习（RL）提升语言模型推理能力的第一步**。我们的**目标是探索大语言模型（LLMs）在没有任何监督数据的情况下发展推理能力的潜力，重点关注它们通过纯强化学习过程实现的自我进化。** 具体而言，我们使用DeepSeek-V3-Base作为基础模型，并采用**分组相对策略优化（GRPO）**（Shao等人，2024）作为强化学习框架，以提高模型的推理性能。在训练过程中，DeepSeek-R1-Zero自然地展现出许多强大且有趣的推理行为。经过数千步的强化学习，DeepSeek-R1-Zero在推理基准测试中表现卓越。例如，在2024年美国数学邀请赛（AIME）中，其单次通过率（pass@1）从15.6% 提升至71.0%，通过多数投票法，这一分数进一步提高到86.7%，与OpenAI-o1-0912的性能相当。

However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.

然而，DeepSeek-R1-Zero面临着诸如可读性差和语言混杂等挑战。为了解决这些问题并进一步提升推理性能，我们推出了**DeepSeek-R1，它融入了少量冷启动数据和多阶段训练流程**。具体来说，我们首先收集数千条冷启动数据来微调DeepSeek-V3-Base模型。之后，我们像训练DeepSeek-R1-Zero那样进行以推理为导向的强化学习。在强化学习过程接近收敛时，我们通过对强化学习的检查点进行拒绝采样，结合来自DeepSeek-V3在写作、事实性问答和自我认知等领域的监督数据，创建新的监督微调（SFT）数据，然后重新训练DeepSeek-V3-Base模型。用新数据微调后，该检查点会再经历一次强化学习过程，将所有场景的提示都考虑在内。经过这些步骤，我们得到了一个名为DeepSeek-R1的检查点，其在性能上与OpenAI-o1-1217相当。