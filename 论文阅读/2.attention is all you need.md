---
created: 2024-11-10T23:11
updated: 2025-02-22T12:31
---

 [https://blog.csdn.net/weixin_42046845/article/details/134949511](https://blog.csdn.net/weixin_42046845/article/details/134949511)
论文：Attention Is All You Need
下载： https://arxiv.org/abs/1706.03762
纯中文解读参考：[中文解读](https://blog.csdn.net/weixin_42046845/article/details/134955064?csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22134955064%22,%22source%22:%22weixin_42046845%22%7D)

> 谷歌于2017年发布论文《Attention Is All YouNeed》，提出了一个只基于attention的结构来处理序列模型相关的问题，比如机器翻译。相比传统的CNN与RNN来作为encoder-decoder的模型，谷歌这个模型摒弃了固有的方式，并没有使用任何的CNN或者RNN的结构，该模型可以高度并行的工作，相比以前串行并且无法叠加多层、效率低的问题。那么Transorformer可以高度并行的工作，所以在提升翻译性能的同时训练速度也特别快。

### Abstract
**The dominant sequence transduction models** are based on **complex recurrent or convolutional neural networks** that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

**主流的序列转导模型基于复杂的循环或卷积神经网络**，包括一个编码器和一个解码器。最高性能的模型还通过注意机制连接编码器和解码器。**我们提出了一种新的简单网络架构：Transformer**，仅基于注意机制，完全摒弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优秀，同时更可并行化，并且需要更少的训练时间。我们的模型在WMT 2014的英德翻译任务上达到28.4 BLEU，在包括集合的现有最佳结果基础上提升了2 BLEU以上。在WMT 2014的英法翻译任务上，在8个GPU上训练3.5天后，我们的模型在单一模型中建立了新的BLEU得分纪录，达到了41.8，训练成本仅为文献中最佳模型的一小部分。我们展示了Transformer在其他任务上的泛化能力，成功地应用于英语分区，无论是使用大规模还是有限的训练数据。

单词
- solely adv.仅仅，只
- mechanisms n.机制，构造，方法
- dispense with 无需，免除  
	- dispense n.分配，分发
- parallelizable 可并行的，可平行的
- significantly  显著地；明显地；意味深长地；
- ensembles  全部，乐团
- fraction 小部分；分数；小数；
- constituency (选举议会议员的)选区；(统称)支持者；
	- english constituency parsing   英语句法分析
```ad-note
#### 精读
**以前的方法**

主流序列转导模型基于复杂的CNN或RNN，包括编码器和解码器。

有的模型使用注意力机制连接编码器和解码器，达到了最优性能。

**缺点：**

   ①难以并行

   ②时序中过早的信息容易被丢弃

   ③内存开销大

**本文方法**

本文提出的Transformer完全摒弃了之前的循环和卷积操作，完全基于注意力机制，拥有更强的并行能力，训练效率也得到较高提升。

**SOTA解释**
SOTA是State-Of-The-Art的首字母缩写，翻译为体现最高水平的。

  SOTA 模型：State-Of-The-Art 模型，是指在该项研究任务中，对比该领域的其他模型，这个是目前最好/最先进的模型。

  SOTA 结果：State-Of-The-Art 结果，一般是说在该领域的研究任务中，此论文的结果对比已经存在的模型及实现结果，此论文的模型具有最好的性能/结果。

**效果：**

在两个翻译任务上表明，我们的模型在质量上更好，同时具有更高的并行性，且训练所需要的时间更少。
```

## 1 Introduction
**Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular,** have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].

**循环神经网络（RNN），尤其是长短期记忆（LSTM）和门控循环神经网络（GRU）**，已经被广泛应用于序列建模和转换问题，如语言建模和机器翻译。多个研究努力不断推动循环语言模型和编码器-解码器架构的发展。

单词
- state of the art    最先进的应用，最先进技术的，目前最高水平
- Numerous    很多的；众多的；许多的

**Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time,** they generate a sequence of hidden states h t , as a function of the previous hidden state h t−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.


**循环模型一般会按照输入和输出序列中各个符号的位置来逐步进行计算。将位置与计算时间[[z之前学习/JAVA/NEW/production/JAVA/production/JAVA/java-韩顺平/openEuler_tomcat/步骤|步骤]]对齐，** 它们会根据前一个隐状态 $h_{t−1}$和位置 $t$的输入生成隐状态序列 $h_{t}$​。这种隐含的顺序性使得在训练示例中无法实现并行化，而在较长的序列长度下这一点变得至关重要，**因为内存限制限制了跨示例的批处理(batching across examples)**。最近的研究通过**因子化技巧(factorization tricks)[21]和条件计算(conditional computation )**[32]在计算效率方面取得了显著进展，同时在后者的情况下也改进了模型性能。然而，**顺序计算的基本限制仍然存在。**

Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.

注意机制已经成为引人注目的序列建模和转换模型的重要组成部分，适用于各种任务，**可以模拟输入或输出序列中的依赖关系，而不考虑它们的距离**[2, 19]。然而，在除了一些特殊情况[27]之外，**这种注意机制通常与递归网络一起使用**。

In this work we propose the **Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.** The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

在这项工作中，我们提出了**Transformer，这是一种模型架构，完全避免使用循环神经网络，并且完全依赖于注意机制来在输入和输出之间建立全局依赖关系。** **Transformer可以实现更高程度的并行化，并且在仅在8个P100 GPU上训练12小时后**，能够达到最新的翻译质量水平。

单词
- typically factor（通常将...分解）
- precludes  排除；妨碍；阻止
- without regard to 不考虑，不顾不行的
	- regard 将…认为；看待；(尤指以某种方式)注视，
- in conjunction with 连同…;与…一起
- eschew  vt. 避免；(有意地)避开；回避

**Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time**, they generate a sequence of hidden states h t , as a function of the previous hidden state h t−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit **batching across examples**. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.

**递归模型通常会将计算过程因子化为输入和输出序列的符号位置。将位置与计算时间步骤对齐**，它们会根据**前一个隐状态 h_{t−1} 和位置 t的输入生成隐状态序列h_{t}。这种隐含的顺序性使得在训练示例中**无法实现并行化**，而在较长的序列长度下这一点变得至关重要，因为内存限制限制了跨示例的批处理(batching across examples)。最近的研究通过因子化技巧(factorization tricks)[21]和条件计算(conditional computation )[32]在计算效率方面取得了显著进展，同时在后者的情况下也改进了模型性能。**然而，顺序计算的基本限制仍然存在。

单词：

- in case of万一；如果发生……；若在……情况下
- preclude 排除；妨碍；阻止；
  

**Attention mechanisms** have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such **attention mechanisms** are used in conjunction with **a recurrent network.**

**注意机制**已经成为引人注目的序列建模和转换模型的重要组成部分，适用于各种任务，可以模拟输入或输出序列中的依赖关系，而不考虑它们的距离[2, 19]。然而，在除了一些特殊情况[27]之外，这种**注意机制**通常与**递归网络**一起使用。

- integral 完整的；不可或缺的；必需的；
- compelling 引人注目的；扣人心弦的；不可抗拒的；非常强烈的
- regard v.将…认为；看待；(尤指以某种方式)注视，凝视
    - n. 注意；关注；尊重；关心；
- In all but a few cases 在除了一些特殊情况之外
- in conjunction with 连同…;与…一起
    - conjunction 连词 (如and、but、or)；(引起某种结果的事物等的)结合；
      

In this work we propose the **Transformer,a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.** The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

在这项工作中，我们提出了**Transformer**，**这是一种模型架构，完全避免使用循环，并且完全依赖于注意机制来在输入和输出之间建立全局依赖关系。** Transformer可以**实现更高程度的并行化**，并且在仅在8个P100 GPU上训练12小时后，能够达到最新的翻译质量水平。

- architecture 结构；体系结构；建筑设计；
- eschew 避免；(有意地)避开；回避
- draw 提取；牵引；抽出;获取
- significantly 显著地；明显地；
- state of the art 最先进的

```ad-note
### 精读
**之前语言模型和机器翻译的方法和不足**

**方法**： RNN、LSTM、GRU、Encoder-Decoder

**不足**：

   ①从左到右一步步计算，因此难以并行计算

   ②过早的历史信息可能被丢弃，时序信息一步一步向后传递

   ③内存开销大，训练时间慢

**近期工作和问题**

- 近期一些工作通过**因子化技巧(factorization tricks)[21]和条件计算(conditional computation )**提高了计算效率，但是顺序计算的本质问题依然存在
- 递归模型和注意力机制搭配使用


**本文改进**

（1）**引入注意力机制**： 注意力机制可以在RNN上使用，通过注意力机制把encoder的信息传给decoder，可以允许不考虑输入输出序列的距离建模。

（2）**提出Transformer** ： 本文的 Transformer **完全不用RNN，这是一种避免使用循环的模型架构，完全依赖于注意机制来绘制输入和输出之间的全局依赖关系，并行度高，计算时间短。**
```

## **2 Background**

The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing **hidden representations** in parallel for all input and output positions. In these models, **the number of operations** required to relate signals from two arbitrary input or output positions **grows** in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced **effective resolution** due to **averaging attention-weighted positions**, an effect we counteract with **Multi-Head Attention** as described in section 3.2.

减少顺序计算的目标也是Extended Neural GPU [16]、ByteNet [18]和ConvS2S [9]的基础，它们都使用卷积神经网络作为基本构建模块，对所有输入和输出位置并行计算**隐状态表示（hidden representations）**。_在这些模型中，将两个任意输入或输出位置之间的信号相关联所需的**操作次数**，随着位置之间的距离增加而**增加**_，对于ConvS2S是线性增加，对于ByteNet是对数增加。这使得学习远距离位置之间的依赖关系更加困难[12]。在Transformer中，这种增长被减少为一定数量的操作，尽管通过**平均注意力加权位置**来降低了**有效分辨率**的代价，我们通过3.2节中描述的**多头注意力**来抵消这种影响。

- arbitrary 任意的；武断的；随心所欲的
- distant 遥远的；远处的；
- albeit 尽管；虽然
- effective resolution 有效分辨率
- counteract 抵消；抵抗；
  

> **the number of operations** required to relate signals from two arbitrary input or output positions **grows** in the distance between positions, 在这些模型中，将两个任意输入或输出位置之间的信号相关联所需的**操作次数**，随着位置之间的距离增加而**增加**

> we counteract with **Multi-Head Attention** as described in section 3.2. 我们通过3.2节中描述的**多头注意力**来抵消这种影响。

**Self-attention**, sometimes called **intra-attention** is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, **abstractive summarization, textual entailment** and learning task-independent sentence representations [4, 27, 28, 22].

**自注意力**，有时也称为**内部注意力**，是一种注意机制，用于关联单个序列的不同位置，以便计算序列的表示。自注意力已成功应用于各种任务，包括阅读理解、摘要生成、文本推理和学习与任务无关的句子表示[4, 27, 28, 22]。

- abstractive summarization, textual entailment 摘要总结，文本推理

End-to-end memory networks are based on a **recurrent attention mechanism** instead of **sequence aligned recurrence** and have been shown to perform well on simple-language question answering and language modeling tasks [34].

端到端记忆网络是基于**循环注意机制（recurrent attention mechanism）而不是序列对齐的循环（sequence aligned recurrence）**，并且已经显示出在简单语言问答和语言建模任务上表现良好。[34]

To the best of our knowledge, however, the Transformer is the first transduction model **relying entirely on self-attention** to compute representations of its input and output without using sequence aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].

然而，据我们所知，Transformer 是**第一个完全依赖自注意力**来**计算其输入和输出表示（compute representations）**的转换模型，而不使用序列对齐的循环神经网络或卷积。在接下来的几节中，我们将描述Transformer，解释自注意力的动机，并讨论它相对于诸如[17，18]和[9]模型的优势。

```ad-note
### 精读

**CNN代替RNN**

**优点**：

    ①减小时序计算

    ②可以输出多通道

**问题**：
_在这些模型中，将两个任意输入或输出位置之间的信号相关联所需的**操作次数**，随着位置之间的距离增加而**增加**_

卷积的感受视野是一定的，距离间隔较远的话就需要多次卷积才能将两个远距离的像素结合起来，所以对长时序来讲比较难。

**自注意力**

有时也称为内部注意力，<mark style="background: #FF0000;">是一种将单个序列的不同位置关联起来以计算序列表示的注意机制</mark>。 自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习与任务无关的句子表示。

**端到端记忆网络**

基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务中表现良好。

**Transformer优点**

- 用注意力机制可以直接看一层的数据，就规避了CNN的那个缺点。
- Transformer 是**第一个完全依赖自注意力**来**计算其输入和输出表示（compute representations）** 的转换模型，而不使用序列对齐的循环神经网络或卷积
```

## **3 Model Architecture**

Most competitive **neural sequence transduction models** have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x 1 , …, x n ) to a sequence of continuous representations z = ( z1 , …, z n ). Given z, the decoder then generates an output sequence (y 1 , …, y m ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.

大多数具有竞争力的**神经序列转导模型（neural sequence transduction models）都采用编码器-解码器结构。在这个结构中，编码器将输入的表示序列 (x_1，...，x_n)映射到一系列连续的表示z = (z_1，...，z_n)。给定z，解码器则逐步生成一个输出符号序列 (y1，...，ym)。** 在每一步中，模型是自回归的，即在生成下一个符号(symbols)时，它使用先前生成的符号作为额外的输入。

- continuous representations 连续的表示
  

The Transformer follows this **overall architecture** using **stacked self-attention** and **point-wise fully connected layers** for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.

Transformer模型的总体架构如下，它在编码器和解码器中都采用了**堆叠(overall architecture)的自注意力机制(stacked self-attention)和逐点全连接层(point-wise fully connected layers)**，如图1的左右两半所示。

- overall 总体
  

![img](https://gitee.com/zhang-junjie123/picture/raw/master/image/697264da844754bbda6a71f8ef182bad.png)

Figure 1: The Transformer - model architecture.

```ad-note
### 精读

**神经序列转导模型**
（neural sequence transduction models）都采用编码器-解码器结构，输入到编码器映射，再到解码器生成输出，采用自回归(auto-regressive)生成一个，加到之前产生的里面一起送入解码器和编码器的输出，循环往复。

**Transformer模型的总体架构**
它在编码器和解码器中都采用了**堆叠(overall architecture)的自注意力机制(stacked self-attention)和逐点全连接层(point-wise fully connected layers)**
有关架构的可以看李宏毅老师的

```

[[1.注意力机制_李宏毅]]
### **3.1 Encoder and Decoder Stacks**

**Encoder:** The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, **position-wise** fully connected **feed-forward network**. We employ **a residual connection** [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function **implemented** by the sub-layer itself. To **facilitate** these residual connections, all sub-layers in the model, as well as **the embedding layers**, produce outputs of dimension dmodel = 512.

**编码器:** 编码器由N ( N = 6 ) 个相同的层组成。每个层包含两个子层。第一个子层是一个多头自注意机制，第二个子层是一个简单的**逐位置**全连接**前馈网络**。我们采用**残差连接**[11]将这两个子层围绕起来，然后进行层归一化[1]。也就是说，每个子层的输出是 LayerNorm(x + Sublayer(x))，其中 Sublayer(x) 是子层本身**实现**的函数。**为了方便这些残差连接，模型中的所有子层以及嵌入层产生的输出都有一个维度为dmodel=512。**

**LayerNorm(x + Sublayer(x)) 残差连接+样本归一化layernorm**

- identical layers 完全相同的；相同的
  

**Decoder:** The decoder is also composed of a stack of N = 6 identical layers. **In addition to** the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. **Similar to** the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to **prevent positions from attending to subsequent positions.** This masking, **combined with fact that the output embeddings are offset by one position**, ensures that the predictions for position i can depend only on the known outputs at positions less than i.

**解码器：** 解码器也由N(N=6)个相同层次的堆叠组成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，它在编码器堆叠的输出上执行多头注意力机制。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，**以防止位置关注后续的位置**。**这种掩码机制与输出嵌入偏移一个位置相结合**，确保位置i 的预测仅依赖于小于i 位置已知的输出。

**masked multi-head self-attention mechanism**: 只关注自己位置及以前的输入

```ad-note
### 精读

**编码器 encoder**

将一个长为n的输入（如句子），序列(x1, x2, … xn)映射为(z1, z2, …, zn)（机器学习可以理解的向量）

encoder由n个相同层组成，**重复6个layers**，每个layers会有两个sub-layers，每个sub-layers里第一个layer是multi-head attention，第二个layer是 simple，position-wise fully connected feed-forward network，简称 MLP。

每个sub-layer的输出都做一个残差连接和layerNorm。计算公式：**LayerNorm( x + Sublayer(x) )，Sublayer(x) 指 self-attention 或者 MLP。**

残差连接需要输入和输出的维度一致，所以每一层的输出维度在transformer里都是固定的，都是512维。

与CNN不一样的是，**MLP的空间维度是逐层下降，CNN是空间维度下降，channel维度上升。**

![image.png|348](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117173302.png)


**解码器 decoder**

decoder 拿到 encoder 的输出，会生成一个长为 m 的序列(y1, y2, … , ym)。n 和 m 可以一样长、也可以不一样长，编码时可以一次性生成，解码时只能一个个生成（auto-regressive 自回归模型）

decoder同样由n个相同层组成。

除了encoder中的两个子层外，decoder还增加了一个子层：对encoder层的输出执行多头注意力。

另外对自注意力子层进行修改(Mask)，防止某个position受后续的position的影响。确保位置i的预测只依赖于小于i的位置的已知输出。

输出就是标准的 Linear+softmax。
![image.png|358](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117173408.png)



关于encoder和decoder更多详细了解请看：[[【Transformer系列（1）】encoder（编码器）和decoder（解码器）]] 

**LayerNorm模块**

LayerNorm是层标准化，和 BatchNorm 在很多时候几乎一样，除了实现方法不同。

<mark style="background: #FF0000;">BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征。在BN和LN都能使用的场景中，BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。</mark>

但是有些场景是不能使用BN的，**例如batchsize较小或者在RNN中，这时候可以选择使用LN，LN得到的模型更稳定且起到正则化的作用**。RNN能应用到小批量和RNN中是因为LN的归一化统计量的计算是和batchsize没有关系的。

```
### **3.2 Attention**

An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight **assigned** to each value is computed by a **compatibility function** of the query with the **corresponding key.**

**注意力函数**可以被描述为**将一个查询和一组键值对映射到一个输出的函数**，其中**查询、键、值( query, keys, values)和输出都是向量。输出是通过对值的加权求和来计算的，其中对每个值赋予的权重是通过将查询与相应的键(corresponding key.)** 进行 **兼容性函数(compatibility function)** 计算得到的。
![在这里插入图片描述](https://gitee.com/zhang-junjie123/picture/raw/master/image/09c51c777076bbedf7e6be9b9315d8c0.png)Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.
图2：（左侧）缩放点积注意力。 （右侧)多头注意力由多个并行运行的注意力层组成。

```ad-note
### 精读

**概念**

注意力机制是对每个 Q 和 K做内积，将它作为相似度。

当两个向量做内积时，如果他俩的 d 相同，向量内积越大，余弦值越大，相似度越高。

如果内积值为0，他们是正交的，相似度也为0。

**相关参数**

    Q： query(查询)

    K： key(键)

    V： value(值)

交叉注意力：
Q就在目标 target区域，就是decoder那块，K和V都在源头 ，就是encoder区域。

自注意力则是QKV都在一个区域，要么都在decoder要么都在encoder。 目的就是为了能够发现一句话内部或者一个时序内部的关联信息。
```

#### 3.2.1 Scaled Dot-Product Attention

We call our particular attention “Scaled Dot-Product Attention” (Figure 2). The input consists of **queries and keys** of dimension d k , and **values** of dimension d v . We compute the dot products of the query with all keys, divide each by and√ d k , and apply a softmax function to obtain the weights on the values.

> Scaled （有刻度的；依比例缩放的；降低的）

我们将我们的特殊注意力机制称为**缩放点积注意力(Scaled Dot-Product Attention)**（图2）。输入包括维度为 $d_k$ 的查询和键，以及维度为$d_v$的值。我们计算查询与所有键的点积，将每个点积除以$\sqrt{d_k}$，并应用 softmax函数来获取值的权重。


In practice, we compute the attention function on a set of queries **simultaneously**, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:

在实践中，我们**同时**对一组查询计算注意力函数，将它们打包成矩阵Q。键和值也被打包成矩阵K和V。我们通过以下方式计算输出矩阵：

$$
Attention(Q, K, V )= softmax(\frac{QK^T}{\sqrt{d_k}})V 
$$

The two most commonly used attention functions are **additive attention** [2], and **dot-product (multiplicative) attention.** 
Dot-product attention is identical to our algorithm, except for the **scaling factor** 1 of 1 √1 √dk . 
Additive attention computes **the compatibility function** using a feed-forward network with a single hidden layer. 

While the two are similar **in theoretical complexity**, dot-product attention is much faster and more space-efficient in practice, since it can be **implemented** using highly optimized **matrix multiplication** code.

最常用的两种注意力函数是**加性注意力**和**点积（乘法）注意力**。

- 点积注意力与我们的算法完全相同，唯一的区别是**缩放因子** $\frac{1}{\sqrt{d_k}}$。
- 加性注意力使用具有单个隐藏层的前馈神经网络来计算**相似度函数**。
  

虽然这两种函数在**理论复杂度**上相似，**但点积注意力在实践中更快速和更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现**。

- compatibility (尤指计算机及程序的)兼容性，相容性
- matrix multiplication 矩阵乘法

While for small values of dk the two mechanisms perform similarly, 
additive attention **outperforms** dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk.

- 当 $d_k$的值较小时，这两种机制的表现相似，
- **但对于较大的$d_k$​值，加性注意力机制优于没有缩放的点积注意力机制**[3]。
**我们猜测对于较大的 $d_k$​ 值，点积的结果变得非常大，将 softmax 函数推向梯度极小的区域**[4]。为了抵消这种影响，我们通过 $\frac{1}{\sqrt{d_k}}$ ​来缩放点积的结果。

```ad-note
### 精读 

**Scaled DotProduct Attention简介**

Scaled Dot-Product Attention是特殊attention，输入包括查询Q和键K的维度dk 以及值V的维度dv​ 。计算查询和键的点积，将每个结果除 ，然后用 softmax() 函数来获得值的权重。 

在实际使用中，我们同时计算一组查询的注意力函数，并一起打包成矩阵 Q。键和值也一起打包成矩阵 K 和 V。 

**加性attention和点积(乘性)attention区别**

   ①点积attention与我们的算法一致，除了缩放因子

   ②加性attention使用带一个单隐层的前馈网络计算兼容函数

尽管这两种attention在原理复杂度上相似，但**点积attention在实践中更快、空间效率更高，因为它可以使用高度优化的矩阵乘法代码。**

为什么用softmax？

对于一个Q会给 n 个 K-V 对，Q会和每个K-V对做内积，产生 n 个相似度。传入softmax后会得到 n 个非负并且和为 1 的权重值，把权重值与 V 矩阵相乘后得到注意力的输出。

为什么除以$\sqrt{ d_{k} }$——scaled缩放的原因

虽然对于较小的 dk 两者的表现相似，但在较大的 dk 时，加法注意力要优于没有缩放机制的点乘注意力。我们认为在较大的 dk 时，点乘以数量级增长，将 softmax 函数推入梯度极小的区域，值就会更加向两端靠拢，算梯度的时候，梯度比较小。为了抵抗这种影响，我们使用 缩放点乘结果。

**我们猜测对于较大的 $d_k$​ 值，点积的结果变得非常大，将 softmax 函数推向梯度极小的区域,我们通过 $\frac{1}{\sqrt{d_k}}$ ​来缩放点积的结果,减少这个的影响。**
```
- outperform  (效益上)超过，胜过
- dot products  点积
- grow large in magnitude 增长幅度增大，
	- magnitude  巨大；震级；星等；重大；重要性；


#### 3.2.2 Multi-Head Attention 多头注意力

Instead of performing a single attention function with d-dimensional keys, values and queries, we found it beneficial to linearly **project** the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2

我们发现，与使用  $d_{model}$​ 维度的单个注意力函数相比，使用不同的**学习线性投影**将**查询、键和值**线性投影到  $d_k$​ ​、 $d_k$​ ​和  $d_v$​ 维度上h 次，对于性能是有益的。在每个投影版本（projected versions）的查询、键和值上，我们并行执行注意力函数，得到了 $d_v$​ 维度的输出值。这些值被串联起来，然后再次进行投影，得到最终的值，如图 2 所示。

Multi-head attention allows the model to jointly attend to information from different **representation subspaces** at different positions. With a single attention head, averaging inhibits this.

多头注意力允许模型同时关注来自**不同表示子空间 (representation subspaces)** 在不同位置的信息。使用单个注意力头会抑制这种效果。

- jointly  连带地
- yielding v.产生（收益、效益等）；屈服；提供；
- depicted  描绘；描述；

$$MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O} 
$$

$$
where  headi=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) 
$$

$$
\text{其中,}W_{i}^{Q}\in R^{d_{model}\times d_{k}},W_{i}^{K}\in R^{d_{model}\times d_{k}},W_{i}^{V}\in R^{d_{model}\times d_{v}}\mathrm{~and~}W^{O}\in R^{hd_{v}\times d_{model}}
$$
In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.

在这项工作中，我们使用了$h = 8$个并行的注意力层，也就是头部(heads)。对于每个头部，我们使用了 $d_k = d_v = d_{model} /h ​=64$。由于每个头部的维度减小，总的**计算成本与具有完整维度**的单头注意力相似。

关于多头注意力机制：


多头注意力机制（Multi-Head Attention）是Transformer模型中的一个关键组件，它允许模型同时关注输入序列的不同部分，从而捕获更丰富的上下文信息。在代码实现中，多头注意力机制通过将输入向量映射到多个查询（Query）、键（Key）和值（Value）向量，并在多个头（Head）上并行计算注意力权重来实现。

以下是对代码中多头注意力机制实现的详细解释：

1. **输入表示**：输入 `x` 是一个形状为 `(N, L, embed_dim)` 的张量，其中 `N` 是批量大小，`L` 是序列长度，`embed_dim` 是嵌入维度。
    
2. **线性变换**：通过三个线性层 `W_q`、`W_k` 和 `W_v`，将输入 `x` 分别映射到查询（Query）、键（Key）和值（Value）空间。这些线性层的输出形状分别为 `(N, L, key_size)`、`(N, L, key_size)` 和 `(N, L, value_size)`。
    
3. **多头分割**：将查询、键和值向量分割成多个头。每个头的维度为 `head_dim`，其中 `head_dim = key_size // num_heads`。这样，每个头都可以专注于输入序列的不同部分。
    
4. **注意力计算**：对于每个头，计算查询向量 `q` 和键向量 `k` 的点积，并除以键向量维度的平方根以进行缩放。然后，应用Softmax函数获得注意力权重 `att`。注意力权重表示每个位置对当前位置的重要性。
    
5. **加权求和**：将注意力权重 `att` 与值向量 `v` 相乘，得到每个头的加权和。这一步骤捕获了输入序列中不同位置之间的依赖关系。
    
6. **头合并**：将所有头的输出进行合并，得到一个形状为 `(N, L, value_size)` 的张量。
    
7. **线性投影**：通过线性层 `out_proj`，将合并后的多头注意力输出映射回原始的嵌入维度 `embed_dim`。
    

在代码中，多头注意力机制的实现主要涉及以下步骤：

```python
def forward(self, x):
        """
        Args:
            X: shape: (N, L, embed_dim), input sequence, 
            是经过input embedding后的输入序列，L个embed_dim维度的嵌入向量

        Returns:
            output: (N, L, embed_dim)
        """
        query = self.W_q(x)  # (N, L, key_size)
        key = self.W_k(x)  # (N, L, key_size)
        value = self.W_v(x)  # (N, L, value_size)
        # 将查询、键和值分别投影到多个头中，并将其重新排列为 (N, num_heads, L, head_dim) 的形状。
        q, k, v = self.q_proj(query), self.k_proj(key), self.v_proj(value)
        # 计算注意力权重，并将其重新排列为 (N, num_heads, L, L) 的形状。
        N, L, value_size = v.size()
        
        # 将查询向量 q 重塑为多头格式，并进行维度转置。
        q = q.reshape(N, L, self.num_heads, self.q_head_dim).transpose(1, 2)
        k = k.reshape(N, L, self.num_heads, self.k_head_dim).transpose(1, 2)
        v = v.reshape(N, L, self.num_heads, self.v_head_dim).transpose(1, 2)

        # 缩放点积注意力，计算注意力相似度。
		att = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(k.size(-1))
		att = F.softmax(att, dim=-1)
        # 计算注意力输出。
		output = torch.matmul(att, v)
        # 将多头注意力输出重塑为原始形状，并进行维度转置。
		output = output.transpose(1, 2).reshape(N, L, value_size)
        
		output = self.out_proj(output)
		
        return output

```

通过这种方式，多头注意力机制能够并行地处理多个注意力头，从而更好地捕捉输入序列中的复杂依赖关系。
```ad-note
### 精读

**方法**

不再使用一个attention函数，而是使用不同的学习到的线性映射将queries，keys和values分别线性投影到 dq、dk 和 dv 维度 h 次。

然后在queries，keys和values的这些投影版本中的每一个上并行执行注意力功能，产生h个注意力函数。

最后将这些注意力函数拼接并再次投影，产生最终输出值。

**目的**

一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。

**本文的点积注意力先进行了投影，而投影的权重w是可学习的。** 多头注意力给h次机会学习不一样的投影方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个头拼接起来，最后再做一次投影。这种做法有一些像卷积网络 CNN的多输出通道。

多头注意力的输入还是 Q 、K 、V，但是输出是将不同的注意力头的输出合并，在投影到W0 里，每个头hi把 Q,K,V 通过 可以学习的 Wq , Wk , Wv投影到 dv上，再通过注意力函数，得到head。
```

#### 3.2.3 Applications of Attention in our Model

The Transformer uses multi-head attention in three different ways:

Transformer模型在三个不同的方式中使用多头注意力：

- In “encoder-decoder attention” layers, the queries come from the
previous decoder layer, and the memory keys and values come from the
output of the encoder. This allows every position in the decoder to
attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence
models such as [38, 2, 9].

- mimics 模仿
==交叉注意力机制

在“编码器-解码器注意力”层中，**查询(queries )来自前一个解码器层，而记忆键( memory keys)和值(values)来自编码器的输出。这使得解码器中的每个位置都可以关注输入序列中的所有位置。这模拟了序列到序列模型中典型的编码器-解码器注意力机制，例如**[38, 2, 9]。

- The encoder contains self-attention layers. In a self-attention layer
all of the keys, values and queries come from the same place, in this
case, the output of the previous layer in the encoder. Each position
in the encoder can attend to all positions in the previous layer of
the encoder.

编码器包含自注意力层。**在自注意力层中，所有的键、值和查询来自同一个位置，即编码器中前一层的输出。** 编码器中的==每个位置都可以关注编码器前一层的所有位置。

- Similarly, self-attention layers in the decoder allow each position
in the decoder to attend to all positions in the decoder up to and
including that position. We need to prevent leftward information
flow in the decoder to preserve the auto-regressive property.
We implement this inside of scaled dot-product attention by masking
out (setting to −∞) all values in the input of the softmax which
correspond to illegal connections. See Figure 2.

- mask out 屏蔽

同样，**解码器中的自注意力层允许解码器中的每个位置关注解码器中的所有位置，直到该位置。** 我们需要防止在解码器中出现左向信息流(leftward information flow)，以保持自回归的性质(the auto-regressive property)。
我们在缩放点积注意力中通过屏蔽（将值设为 -∞）对应于非法连接的所有输入值来实现。参见图2。

问题
- ==左向信息流，这里我理解是自回归是右向的，所以不应该左向

```ad-note
精读
Transformer用了三种不同的注意力头：

（1）Encoder 的注意力层： 输入数据在经过 Embedding+位置encoding 后，复制成了三份一样的东西，分别表示 K Q V。同时这个数据既做 key 也做 query 也做value，其实就是一个东西，所以叫自注意力机制。输入了n个 Q，每个Q会有一个输出，那么总共也有n个输出，输出是 V 加权和（权重是 Q 与 K 的相似度）。
![image.png|280](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117201738.png)



（2）Decoder 的注意力层： 这个注意力层就不是自注意力了，其中 K 和 V 来自 Encoder的输出，Q来自掩码多头注意力输出。
![image.png|280](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117201723.png)


（3）Decoder 的掩码注意力层： 掩码注意力层就是将t时刻后的数据权重设置为 0，该层还是自注意力的。
![image.png|280](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117201743.png)


```


### 3.3 Position-wise Feed-Forward Networks 位置前馈网络

In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.

除了注意力子层之外，我们的编码器和解码器中的每个层都包含一个全连接的前馈网络，它分别对每个位置进行相同的处理。这个前馈网络由两个线性变换组成，它们之间有一个ReLU激活函数。

While the linear transformations are the same across different positions, they use different parameters from layer to layer.==Another way of describing this is as two convolutions with kernel size 1.== 
The dimensionality of input and output is d_model = 512, and the inner-layer has dimensionality dff = 2048.
$$
FFN(x) = \mathrm{ReLu}(xW_{1}+b_{1})W_{2}+b_{2}
$$
- linear transformations 线性变换

在不同位置上，尽管线性变换是相同的，但它们使用的参数会在各个层之间不同。可以将其描述为具有核大小为1的两个卷积。输入和输出的维度为 $d_{model}$ = 512，而内部层的维度为$d_{ f f }= 2048$

==1\*1卷积说明==
引用：[[10、CNN_Basic CNN#1 *1 卷积的作用]]

问题：
- 这里还是不太明白，为什么和卷积有关系，输入输出的维度。

```ad-note
精读
除了attention子层，我们encoder-decoder框架中每一层都包含一个全连接的前馈网络，它分别相同地应用于每个位置。它由两个线性变换和中间的一个ReLU激活函数组成

公式
$$
FFN(x) = \mathrm{ReLu}(xW_{1}+b_{1})W_{2}+b_{2}
$$
两个线性转换作用于相同的位置，但是在他们用的参数不同。 另一种描述方式是比作两个knernel size=1的卷积核。输入输出的维度为512，中间维度为2048。


实现
x = self.w_2(F.relu(self.w_1(x)))
```

### 3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by -so √d model

与其他**序列转换模型(sequence transduction models)类似，我们使用学习到的嵌入(embeddings)将输入标记(input tokens )和输出标记(output tokens)** 转换为维数为$d_{model}$的向量。我们还使用常规的学习到的线性变换和softmax函数将解码器输出转换为预测的下一个标记的概率。在我们的模型中，==我们在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以$\sqrt{d_{model}}$==。

注意：
- 输入输出的embeddings和预softmax线性变换直接共享相同的权重矩阵，权重还要乘以$\sqrt{d_{model}}$
```ad-note
### 精读
**Embedding**： 特征嵌入，==embedding是可以简单理解为通过某种方式将词向量化，即输入一个词输出该词对应的一个向量。==（embedding可以采用训练好的模型如GLOVE等进行处理，也可以直接利用深度学习模型直接学习一个embedding层，Transformer模型的embedding方式是第二种，即自己去学习的一个embedding层。）
首先我们来看Embedding的参数。

 nn.Embedding((num_embeddings,embedding_dim)

其中，num_embeddings代表词典大小尺寸，比如训练时所可能出现的词语一共5000个词，那么就有num_embedding=5000，而embedding_dim表示嵌入向量的维度，即用多少来表示一个符号。提到embedding_dim，就不得先从one_hot向量说起。

最初的时候，人们将word转换位vector是利用one_hot向量来实现的。简单来讲，现在词典里一共5个字，[‘我’,‘是’,‘中’,‘国’,‘人’]，即num_embedding=5，而现在有一句话‘我是人’，one_hot则利用一个长度为5的01向量来代表这句话中的每个字（假设词典按顺序编码），有

- 我：[1 0 0 0 0 ]
    
- 是：[0 1 0 0 0 ]
    
- 人：[0 0 0 0 1 ]
    

显然，这种方法简单快捷，但是当词典的字很多，比如50000个字的时候，这种方法会造成极大的稀疏性，不便于计算。而且one_hot方法无法处理原来标签的序列信息，比如“我是人”这句话中，“我”和“人”的距离与“我”和“是”的距离一样，这显然是不合理的。

因此，为了改进这些缺点，embedding算是它的一个升级版（没有说谁好和谁不好的意思，现在one hot向量也依旧在很多地方运用，选择特征时要选择自己合适的才行。）

embedding翻译word是这样操作的，首先，先准备一本词典，这个词典将原来句子中的每个字映射到更低的维度上去。比如，字典中有50000个字，那按照One-hot方法，我们要为每个字建立一个50000长度的vector,对于embedding来说，我们只需要指定一个embedding_dim，这个embedding_dim<50000即可。

![在这里插入图片描述|520](https://gitee.com/zhang-junjie123/picture/raw/master/image/665b89c8c3af9000ad96eaac8fa3b15b.jpeg) 见上图，也就是说，原来one-hot处理一句话（这句话有length个字），那我们需要一个（length，50000）的矩阵代表这句话，现在只需要（length，embedding_dim）的矩阵就可以代表这句话。 从数学的角度出发就是（length，50000）*（50000,embedding_dim），做了个矩阵运算。

![在这里插入图片描述|520](https://gitee.com/zhang-junjie123/picture/raw/master/image/59cf24f39fd1fe7315333b6e0136354f.jpeg) 上面这张图是计算示意图，为了方便计算，我们将句子的最大长度设置为max_length,也就是说，输入模型的所有语句不可能超过这个长度。原来用one_hot向量表示的话，如果浓缩起来就是上面的那个长条，如果展开则是下方的那个矩阵。 **也就是说，当整个输入数据X只有一句话时** X（1, max_length, num_embeddings<one-hot时，每个词需要num_embeddings维>） 字典为（num_embeddings, embedding_dim） 则经过翻译之后，也就是把one-hot转为embedding，这句话变成（1，max_length，embedding_dim）
也就是说one-hot的时候 X(1,20,50000),词典是词的个数50000*emb编码的维度也就是embedding_dim如3000，经过翻译矩阵相乘后，维度变为了X(1,20,3000)。

**当输入数据X有多句话时，即Batch_size不等于1**,有 X（batch_size, max_length, num_embeddings） 字典为（num_embeddings, embedding_dim） 则经过翻译之后，输入数据X变成（batch_size，max_length，embedding_dim）

![在这里插入图片描述|520](https://gitee.com/zhang-junjie123/picture/raw/master/image/52a19047db1d9b9d9b5ad3058fca831d.jpeg) 因此，nn.embedding（num_embeddings,embedding_dim）的作用就是将输入数据降维到embedding_dim的表示层上，得到了输入数据的另一种表现形式。

**代码应用如下**

 import torch  
 import numpy as np  
 ​  
 batch_size=3  
 seq_length=4  
 ​  
 input_data=np.random.uniform(0,19,size=(batch_size,seq_length))#shape(3,4)  
 ​  
 input_data=torch.from_numpy(input_data).long()   
 ​  
 embedding_layer=torch.nn.
 Embedding(vocab_size,embedding_dim)  
 ​  
 lstm_input=embedding_layer(input_data)#shape(3,4,6)

注意，输入进embedding层的数据并不是经过词典映射的，而是原始数据，因此张量内部有超出embedding层合法范围的数，这会导致embedding层报错，所以一开始input_data要约束在0,19之间。 且在输入数据的时候，不需要自己手动的将vocab_num给设置出来，这个embedding会自己完成映射。

**本文方法**

embeddings将输入和输出tokens转换为向量，线性变换和softmax函数将decoder输出转换为预测的写一个token概率。
```


### 3.5 Positional Encoding
位置编码

Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].

- inject 给…)注射(药物等)；(给…)添加，增加(某品质)；

由于我们的模型不包含循环和卷积，为了让模型利用序列的顺序，我们需要注入一些关于相对或绝对位置的信息。为此，==我们将“位置编码(positional encodings)”添加到编码器和解码器堆栈底部(at the bottoms of the encoder and decoder stacks.)的输入嵌入中。== 位置编码具有与嵌入相同的模型维度$d_{model}$，因此可以将两者相加。关于位置编码，有很多选择，可以是可学习的，也可以是固定的[9]。

注意：
- ==位置编码具有与嵌入相同的模型维度$d_{model}$，因此可以将两者相加。==
- ==可以是学习得到的，也可以是固定的==

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241115223521.png)


表1：不同层类型的最大路径长度、每层复杂性和最小顺序操作次数。n是序列长度，d是表示维数，k是卷积的核大小，r是限制自注意中邻域的大小。

In this work, we use sine and cosine functions of different frequencies:

在这项工作中，我们使用不同频率的正弦和余弦函数：
$$
PE_{pos,2i} = \sin\left( \frac{pos}{10000^{2i/d_{model}}} \right)
$$

$$
PE_{pos,2i+1} = \cos\left( \frac{pos}{10000^{2i/d_{model}}} \right)
$$
where pos  is the position and i  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E Epos+k can be represented as a linear function of P E Epos .

其中 pos是位置(position)，i 是尺寸(dimension)。也就是说，位置编码中的每个维度对应于一个正弦波。波长从2 π到10000·2π形成了一个几何级数(a geometric progression)。我们选择了这个函数，==是因为我们假设它可以让模型更容易学会通过相对位置进行注意力，因为对于任何固定的偏移量k ， P E_{pos+k}​可以表示为P E_{pos} 的线性函数。==

We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.

我们也尝试使用学习的位置嵌入 [9]，并发现这两个版本的结果几乎相同（参见表3行（E））。==我们选择了正弦版本，因为它可能允许模型对训练期间遇到的序列长度之外的序列长度进行外推。==

```ad-note
### 精读
**目的**

因为transformer模型不包含循环或卷积，输出是V的加权和（权重是 Q与K的相似度，与序列信息无关），对于任意的K-V，将其打乱后，经过注意力机制的结果都一样

但是它顺序变化而值不变，在处理时序数据的时候，一个序列如果完全被打乱，那么语义肯定发生改变，而注意力机制却不会处理这种情况。

**方法**

在注意力机制的输入中加入时序信息，位置在encoder端和decoder端的embedding之后，用于补充Attention机制本身不能捕捉位置信息的缺陷。

一次词在嵌入层表示成一个512维的向量，用另一个512维的向量表示位置数字信息的值。用周期不一样的sin和cos函数计算。
```

## 4 Why Self-Attention
为什么使用自注意力机制

In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations$(x_1 , ..., x_n )$to another sequence of equal length $(z_1 , …, z_n )$ with $x_i$ , $z_i$ y ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.

- desiderata 需求

在这一部分中，我们将自注意力层与常用的用于将一个变长符号表示序列$(x_1 , ..., x_n )$映射到另一个等长序列 $(z_1 , …, z_n )$ 的循环和卷积层进行比较，其中 $x_i$ , $z_i$ y ∈ R d ，例如典型的序列转换编码器或解码器中的隐藏层。**我们考虑到以下三个要点来说明我们使用自注意力的原因。**

One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.

**第一个要点是每层的总计算复杂度。第二个要点是可以并行计算的计算量，即所需的最小顺序操作数量。**

The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.

==第三个要点是网络中长程依赖的路径长度。学习长程依赖是许多序列转换任务中的一个关键挑战。==影响学习此类依赖的一个关键因素是前向和后向信号在网络中必须穿越的路径长度。这些路径==在输入和输出序列的任意组合位置之间越短==，学习长程依赖就越容易[12]。因此，我们还比较由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. 
To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.

- word-piece 和 byte-pair representations

如表1所示，自注意力层将所有位置连接到一个常数数量的顺序执行操作，而循环层则需要O(n)个顺序操作。**在计算复杂度方面，当序列长度n小于表示维度d时，自注意力层比循环层更快，而这通常是机器翻译中现有模型使用的句子表示形式，例如词元和字节对表示法。** 为了提高**处理非常长序列的任务**的计算性能，**自注意力可以限制只考虑大小为r的邻域**。输入序列围绕相应输出位置进行了居中处理。这将增加最大路径长度为O(n/r)。我们计划在未来的工作中进一步研究这种方法。

A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.

一个宽度为k < n的单个卷积层无法连接所有的输入和输出位置。这需要一个 O(n/k)的卷积层堆栈，在连续内核的情况下，或者在扩张卷积[18]的情况下，需要O(log_k (n))，从而增加网络中任意两个位置之间最长路径的长度。卷积层的复杂性通常比递归层高，有一个因子k。然而，可分离卷积[6]可以显著降低复杂性，达到O(k · n · d + n · d ^2 )。然而，==即使当 k = n时，可分离卷积的复杂性仍然等于自注意层和逐点前馈层的组合，这是我们模型中采用的方法。==

As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.

- yield 产生(收益、效益等)；屈服；提供
- interpretable  可解释的

作为附加好处，**自注意力可以产生更可解释的模型。** 我们检查模型的注意力分配，并在附录中提供和讨论示例。不仅每个注意头明显学会了执行不同的任务，许多注意头似乎还展现出与句子的句法和语义结构相关的行为。

==自注意力可以产生更可解释的模型。==

```ad-note
### 精读
#### 考虑因素

**一是每层的总计算复杂度**

**二是可以并行化的计算量，以所需的最小序列操作数衡量**

**三是网络中长距离依赖关系之间的路径长度**

**研究关键**

计算远距离依赖一直是序列转换任务中的关键挑战，其中的一个关键因素就是其路径长度。

路径距离越短，学习长距离依赖越容易。

**和CNN、RNN比较**

n表示序列长度，d是隐藏层维度，k表示卷积核尺寸，r表示受限自注意力的窗口大小
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117202723.png)

 

结论

self-attention能够产生解释性更强的模型
```


## 5 Training

This section describes the training regime for our models. 本节描述了我们模型的训练制度。

### 5.1 Training Data and Batching

We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.

我们在标准的WMT 2014英德数据集上进行了训练，该数据集包含约450万个句子对。句子使用字节对编码进行编码[3]，其中共享的源-目标词汇表约为37000个标记。对于英法语言对，我们使用了数量显着更大的WMT 2014英法数据集，其中包含3600万个句子，并将标记分为一个32000个词片词汇表[38]。句子对按照近似的序列长度进行分组。每个训练批次包含一组句子对，其中包含大约25000个源标记和25000个目标标记。

```ad-note
### 精读
- **数据集**： standard WMT 2014 English-German dataset  ==4.5 million==
	-  WMT 2014 English-French dataset  ==36Million==
- **编码方式**：byte-pair encoding
	- **训练方式：** 序列长度相近的句子一起进行批处理
- **共享词汇表**：英德 shared source-target vocabulary  ==37000 tokens==
	- 英法 word-piece vocabulary ==32000 tokens==
- 训练批大小 train batch :  25000 source tokens and 25000 target tokens.
```
### 5.2 Hardware and Schedule

We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).

- Schedule 工作计划；日程安排；


我们使用了一台装有8个NVIDIA P100 GPU的机器来进行模型训练。对于我们的基本模型，使用了本文中描述的超参数，每个训练步骤大约需要0.4秒的时间。我们对基本模型进行了总计100,000步或者12小时的训练。对于大型模型（在表格3最后一行中描述），每个训练步骤需要1.0秒的时间。大型模型进行了总计300,000步或者3.5天的训练。

```ad-note

- 硬件:8个NVIDIA P100 GPU
- 时间：
	- 基础模型：每个训练步骤大约需要0.4秒的时间。我们对基本模型进行了总计100,000步或者12小时的训练。
	- 大模型：每个训练步骤需要1.0秒的时间。大型模型进行了总计300,000步或者3.5天的训练。

```

### 5.3 Optimizer

We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ε =  10 −9 . We varied the learning rate over the course of training, according to the formula:
$$
lrate = d_{model}^{-0.5} .min(step\_num^{-0.5},step\_num* warmup\_steps^{-1.5})
$$
- formula 公式；配方，处方；计划，方案；

我们使用了Adam优化器[20]，其中β_1 = 0.9，β_2 = 0.98，ε = 10^-9。根据以下公式，我们在训练过程中变化学习率：$lrate = d^{ −0.5}. min(step\_num ^{-0.5},step\_num · warmup\_steps^{-1.5})$ 

This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000. 

- corresponds to 相对应；相当于

这指的是在前 warmup_steps 次训练步骤中线性增加学习率，然后随着步骤数量的逆平方根成比例地递减学习率。我们使用了warmup_steps = 4000 

```ad-note
### 精度
- 理解公式
	- 前4000步的两个的计算结果如下：
	![image.png|424](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117103941.png)
	- 学习率显然开始的是右侧的$step\_num · warmup\_steps^{-1.5}$呈线性增加，后面就是左侧的 $step\_num ^{-0.5}$递减了
```


### 5.4 Regularization

We employ three types of regularization during training: 我们在训练过程中采用了三种类型的正则化方法：

Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.

残差Dropout：我们对每个子层的输出应用了Dropout [33]，在将其添加到子层输入并归一化之前。此外，==在编码器和解码器堆栈中，我们也对嵌入和位置编码的总和应用了Dropout==。对于基本模型，我们使用的Dropout率为 P_drop = 0.1。 ![在这里插入图片描述](https://gitee.com/zhang-junjie123/picture/raw/master/image/09fdd607156e478141051649fd2782a6.png) 表2:Transformer在2014年英语到德语和英语到法语新闻测试中获得了比以前最先进的型号更好的BLEU分数，而培训成本只是其中的一小部分。

- BLEU分数 [[BLEU分数]]

**(以下是ChatGpt生成的解释，大家可以看一下。)** 该表格列出了各种神经机器翻译模型在英德和英法数据集上的BLEU分数以及它们的训练成本（以浮点运算次数FLOPs计算）。下面是对表格的分析：

- ByteNet是一种基于卷积神经网络的翻译模型，它在英德数据集上达到了23.75的BLEU分数。该模型的训练成本并未给出。
  
- Deep-Att + PosUnk是一种使用注意力机制和位置未知标记的模型，它在英法数据集上达到了39.2的BLEU分数。该模型的训练成本为1.0·1020 FLOPs。
  
- GNMT + RL是一种使用增强学习训练的Google神经机器翻译系统，它在英德数据集上达到了24.6的BLEU分数，在英法数据集上达到了39.92的BLEU分数。该模型的训练成本分别为2.3·1019 FLOPs和1.4·1020 FLOPs。
  
- ConvS2S是一种基于卷积神经网络的翻译模型，它在英德数据集上达到了25.16的BLEU分数，在英法数据集上达到了40.46的BLEU分数。该模型的训练成本分别为9.6·1018 FLOPs和1.5·1020 FLOPs。
  
- MoE是一种使用多层感知机（MLP）的模型，它在英德数据集上达到了26.03的BLEU分数，在英法数据集上达到了40.56的BLEU分数。该模型的训练成本分别为2.0·1019 FLOPs和1.2·1020 FLOPs。
  
- Deep-Att + PosUnk Ensemble是Deep-Att + PosUnk模型的集成版本，它在英法数据集上达到了40.4的BLEU分数。该模型的训练成本为8.0·1020 FLOPs。
  
- GNMT + RL Ensemble是GNMT + RL模型的集成版本，它在英德数据集上达到了26.30的BLEU分数，在英法数据集上达到了41.16的BLEU分数。该模型的训练成本分别为1.8·1020 FLOPs和1.1·1021 FLOPs。
  
- ConvS2S Ensemble是ConvS2S模型的集成版本，它在英德数据集上达到了26.36的BLEU分数，在英法数据集上达到了41.29的BLEU分数。该模型的训练成本分别为7.7·1019 FLOPs和1.2·1021 FLOPs。
  
- Transformer是一种基于自注意力机制的翻译模型，包括base model和big model两个版本。其中，base model在英德数据集上达到了27.3的BLEU分数，在英法数据集上达到了38.1的BLEU分数。它的训练成本为3.3·1018 FLOPs。big model在英德数据集上达到了28.4的[[BLEU分数]]，在英法数据集上达到了41.8的[[BLEU分数]]。它的训练成本为2.3·1019 FLOPs。
  

综合来看，根据[[BLEU分数]]的表现，Transformer模型的big版本在英法翻译任务上达到了最佳性能。但是，GNMT + RL Ensemble和ConvS2S Ensemble在英法翻译任务中也表现出色。总的来说，这些模型都有不同的训练成本，选择适合任务需求和计算资源的模型将是一个权衡。


Label Smoothing During training, we employed label smoothing of value $\epsilon_{ls}$ = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 


- hurt [hɜː(r)t]：v. 受伤，感到疼痛，使不快，使烦恼 n. 委屈，心灵创伤 adj.（身体上） 受伤的，（感情上） 受伤的
- perplexity [pə(r)'pleksəti]：n. 困惑，迷惘，难以理解的事物，疑团


**标签平滑化：** 在训练过程中，我们采用了标签平滑化，其中平滑值 $\epsilon_{ls}$ [36]。这让模型不易理解，因为模型学会变得更加不确定，但可以提高准确率和BLEU分数。

#### Dropout 层
Dropout 层是深度学习中一种常用的正则化技术，主要用于缓解神经网络过拟合问题，以下是关于它的详细介绍：

##### 基本原理

- 在神经网络训练过程中，Dropout 层会按照一定的概率（这个概率通常是人为设定的，比如 0.5 等）随机地 “丢弃”（也就是让其输出为 0）一些神经元，使得网络在每次训练迭代时结构都有所不同。形象地说，就好比训练一个团队，每次训练时都随机让一部分成员 “休息”，整个团队的构成处于动态变化中。
- 例如，对于一个全连接神经网络的某一层有 100 个神经元，设定 Dropout 概率为 0.3，那么在每次前向传播训练时，大约会有 30 个神经元被随机置为 0，它们不参与此次的计算和后续的梯度更新等操作。

##### 作用机制

- **减少神经元之间的复杂共适应关系**：如果没有 Dropout，神经元之间可能会逐渐形成非常固定、复杂的相互依赖关系来拟合训练数据，这容易导致过拟合。而 Dropout 通过随机丢弃神经元，使得每个神经元不能过度依赖其他特定的神经元，迫使它们学习更具鲁棒性、更普遍的特征，因为任何一个神经元都有可能在某次训练中被 “抛弃”，所以它们需要独立地对各种输入情况做出合理反应。
- **增加模型的泛化能力**：由于训练时网络结构不断变化，模型相当于在训练多个不同结构的 “子网络”，到了测试阶段（此时 Dropout 层通常是关闭的，即所有神经元都参与计算），模型可以综合这些不同 “子网络” 学习到的特征来对新的数据进行更好的预测，从而使得模型对未见过的数据（也就是测试数据或者实际应用中的新数据）有更好的泛化表现，避免只对训练数据拟合得很好，但对新数据效果很差的过拟合情况。
## 6 Results

### 6.1 Machine Translation

On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3:5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.

- surpass  超过；优于；胜过
- at a fraction of 一小部分
- ensembles 整合

在 WMT 2014 英语-德语翻译任务中，大型 transformer 模型 (表 2 中的 Transformer (big)) 比以前报道的最佳模型 (包括整合模型) 高出 2.0 个 BLEU 以上，确立了一个全新的新的最高 BLEU 分数为28.4。该模型的配置列在表 3 的底部。训练在 8 个 P100 GPU 上花费 3.5 天。即使我们的基础模型也超过了以前发布的所有模型和整合模型，且训练成本只是这些模型的一小部分。

Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
在英语到德语和英语到法语的 newstest2014 测试中，Transformer 的 BLEU 分数高于之前的先进模型，而训练成本仅为之前的一小部分。
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123639.png)


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.
Transformer 架构的变体。未列出的值与基本模型的值相同。所有指标都基于英文到德文翻译开发集 newstest2013。
根据我们的字节对编码，列出的困惑是每个单词的，不应该与每个单词的困惑进行比较。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123647.png)


对模型自身的参数执行改变自变量的测试，确认哪些参数对模型的影响比较大。

- workpiece ['wɜ:kˌpi:s]：na. 工作件
- perplexity [pə(r)'pleksəti]：n. 困惑，迷惘，难以理解的事物，疑团

On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_{drop} = 0.1, instead of 0.3.
在 WMT 2014 英语-法语翻译任务中，我们的大型模型的 BLEU 得分为 41.0，超过了之前发布的所有单一模型，训练成本低于先前最先进模型的 1/4。英语-法语的 Transformer (big) 模型使用丢弃率为 dropout rateP_{drop} = 0.1，而不是 0.3。

For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 \alpha = 0.6α=0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].
对于基础模型，我们使用的单个模型来自最后 5 个检查点的平均值，这些检查点每 10 分钟写一次。对于大型模型，我们对最后 20 个检查点进行了平均。==我们使用 beam search，beam 大小为 4，长度惩罚 α=0.6 ==[38]。这些超参数是在开发集上进行实验后选定的。在推断时，我们设置==最大输出长度为输入长度 + 50==，但在可能时尽早终止 [38]。

- beam search：定向搜索
- beam [biːm]：n. 梁，光线，平衡木，(电波的) 波束 v. 照射，发光，笑容满面，眉开眼笑

Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.
表 2 总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、所使用的 GPU 的数量以及每个 GPU 的持续单精度浮点能力的估计值相乘来估计用于训练模型的浮点运算的数量。

脚注：
We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively

### 6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.
为了评估 Transformer 不同组件的重要性，我们以不同的方式改变我们的基础模型，测量开发集 newstest2013 上英文-德文翻译的性能变化。我们使用前一节所述的 beam search，但没有平均检查点。我们在 Table 3 中列出这些结果.

[[Beam Search解码-CSDN博客]]

In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
在 Table 3 rows (A) 中，我们改变 attention head 的数量和 attention key 和 value 的维度，保持计算量不变，如 3.2.2节所述。虽然只有一个 head attention 比最佳设置差 0.9 BLEU，但质量也随着 head 太多而下降。
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117125843.png)

In Table 3 rows (B), we observe that reducing the attention key size d_k​ hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.
- compatibility 兼容性
- sophisticate [sə'fɪstɪkeɪt]：v. 用诡辩欺骗，使迷惑，窜改，掺坏 n. 老于世故的人，见多识广的人
在 Table 3 rows (B) 中，我们观察到减小 key 的大小 d_k会有损模型质量。这表明确定兼容性并不容易，并且比点积更复杂的兼容性函数可能更有用。我们在 rows (C) and (D) 中进一步观察到，如预期的那样，更大的模型更好，并且丢弃对避免过度拟合非常有帮助。在 row (E) 中，我们用学习到的位置嵌入 [9] 来替换我们的正弦位置编码，并观察到与基本模型几乎相同的结果。

- determine [dɪ'tɜː(r)mɪn]：v. 确定，决定，测定，查明


### 6.3 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].

- English constituency parsing 英语选区划分
- is subject to 受 ...支配
- attain [ə'teɪn]：v. 得到，达到 （某年龄、水平、状况）
- regimes  统治制度，政权；组织方法；养生法；（科技）管理体系；

为了评估 Transformer 是否可以推广到其他任务，我们对 English constituency parsing 进行了实验。这项任务提出了具体的挑战：输出受到强大的结构约束，并且比输入要长得多。此外，RNN 序列到序列模型还无法在小数据体制中获得最好的结果 [37]。


Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123715.png)


We trained a 4-layer transformer with  d_{model} = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, ==using the larger high-confidence and BerkleyParser corpora ==from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.

- portion 部分
-  semi-supervised setting 半监督设置
	- supervised 监督；管理；指导；主管；有监督的
- using the larger high-confidence and BerkleyParser corpora  更大的高置信度和 BerkleyParser 语料库
	- corpora 全集

我们用  d_{model} = 1024 在 Penn Treebank [25] 的 Wall Street Journal (WSJ) 部分训练了一个 4 层 transformer，约 40K 个训练句子。我们还使用==更大的高置信度和 BerkleyParser 语料库==，在半监督环境中对其进行了训练，大约 17M 个句子 [37]。我们使用了一个 16K 词符的词汇表作为 WSJ 唯一设置，和一个 32K 词符的词汇表用于半监督设置。

We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.

- corpus ['kɔːpərə]：n. 语料，全集，文集，语料库
- parameters 参数；范围；规范

我们仅进行了少量实验，以选择 Section 22 开发集上的 dropout、注意力和残差 (section 5.4)、学习率和波束大小，所有其他参数在英语到德语基础翻译模型中均保持不变。在推论过程中，我们将最大输出长度增加到输入长度 + 300。对于仅 WSJ 和半监督设置，我们使用 21 的波束大小和 α = 0.3 \alpha = 0.3α=0.3。

- ==不是很懂  dropout 退学；退出==


Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].
- tuning  (给收音机、电视等)调谐，调频道；调整，调节
- yield 产生

表 4 中我们的结果表明，尽管缺少特定任务的调优，我们的模型表现得非常好，得到的结果比之前报告的除循环Recurrent Neural Network Grammar [8] 之外的所有模型都好。

In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.
- In contrast to  和 ... 相比
与 RNN 序列到序列模型 [37] 相比，即使仅在 40K 句子的 WSJ training set 上训练时，Transformer 也胜过 BerkeleyParser [29]。

## 7 Conclusion
In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.
- presented 提出；(以某种方式)展现，显示
在这项工作中，我们介绍了 Transformer，这是完全基于注意力的第一个序列转换模型，用 multi-headed self-attention代替了编码器-解码器体系结构中最常用的循环层。

For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.
- In the former task 在前面的任务中
- ensembles model 集成模型
对于翻译任务，与基于循环层或卷积层的体系结构相比，可以大大加快 Transformer 的训练速度。在 WMT 2014 English-to-German and WMT 2014 English-to-French 翻译任务上，我们都达到了新的最佳水平。在前面的任务中，我们最好的模型甚至胜过以前报道过的所有整合模型。

We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.
- modalities 方式
- other than text   除了...之外
我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。==我们计划将 Transformer 扩展到文本以外的涉及输入和输出方式的问题，并研究局部受限的注意机制，以有效处理大型输入和输出，例如图像、音频和视频。 ==让生成具有更少的顺序性是我们的另一个研究目标。

The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.

**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.

- Acknowledgements 感谢；致谢；鸣谢；致谢词

我们感谢Nal Kalchbrenner和Stephan Gouws富有成效的评论、更正和启发。

fruitful ['fruːtf(ə)l]：adj. 成果丰硕的，富有成效的，富饶的，丰产的
inspiration [.ɪnspə'reɪʃ(ə)n]：n. 灵感，妙计，启发灵感的人 (或事物)，使人产生动机的人 (或事物)
modality [məʊ'dæləti]：n. 形态，情态，形式，方式

## Attention Visualizations
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123736.png)

Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making…more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.

attention 机制的一个示例，5/6 层的编码器 self-attention 中的长距离依赖。很多 attention head 都关注与动词 making 的远距离依赖关系，正好补全 making…more difficult 这个短语。不同的颜色代表不同的 head。==彩色效果最佳。==

spirit ['spɪrɪt]：n. 精神，灵魂，心灵，勇气 v. 偷偷带走，让人不可思议地弄走
registration [.redʒɪ'streɪʃ(ə)n]：n. 登记，注册，挂号，登记文档

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123759.png)

Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.

apparently 显然

两个 attention head，也在 5/6 层，显然有逆向照应 (下文的词返指或代替上文的词)。Top: head 5 的完整的 attention。Bottom: 仅将 attention heads 5 和 6 中单词 its 的 attention 分离出来。请注意，这个词的 attention 非常明确。

anaphora [ə'næfərə]：n. 逆向照应 (下文的词返指或代替上文的词)

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241117123807.png)

Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.
很多 attention head 表现出的行为似乎与句子的结构有关。我们给出了两个这样的例子，来自编码器 5/6 层 self-attention 的两个不同的 head。Heads 清楚地学会了执行不同的任务。

