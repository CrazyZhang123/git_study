---
created: 2024-11-02T10:53
updated: 2024-12-30T22:59
---

## 11.02 A Survey of Resource-efficient LLM and Multimodal Foundation Models

### Abstract

Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations.
The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.

大型基础模型，包括大型语言模型（LLM）、视觉转换器（ViT）、扩散和基于 LLM 的多模态模型，正在彻底改变从训练到部署的整个机器学习生命周期。 然而，这些模型在通用性和性能方面的巨大进步需要付出巨大的硬件资源代价。 为了以可扩展和环境可持续的方式支持这些大型模型的发展，人们一直非常关注开发资源高效型战略。 本调查报告深入探讨了此类研究的关键重要性，从算法和系统两方面进行了研究。 它提供了从现有文献中收集到的全面分析和宝贵见解，涵盖了从前沿模型架构和训练/服务算法到实用系统设计和实现的广泛主题。 本调查报告的目的是让人们全面了解当前的方法是如何应对大型基础模型带来的资源挑战的，并为该领域未来的突破提供潜在的灵感。

单词：
1. substantial 重大的，坚实的
2. deployment 部署
3. versatility 多功能性，通用性
4. scalable 可扩展的
5. delves into 深入研究
6. valuable insights 宝贵见解
7. encompass 拥有，包含
8. a broad array of 各种各样的
9. cutting-edge model architectures 前沿的模型架构
10. overarching 总体的
11. tackling 解决，应对

\_Keywords Foundation Model(基础模型)  ⋅ Large Language Model(大语言模型)  ⋅ Vision Transformer(视觉 transformer)  ⋅ Diffusion Model(扩散模型)  ⋅ Multimodal LLM(多模态LLM)  ⋅ Model Compression(模型压缩)  ⋅ Machine Learning System(机器学习系统)  ⋅ Serving System(服务系统)  ⋅ Pre-training(预训练)  ⋅ Fine-tuning(微调)  ⋅ Edge Intelligence(边缘智能)

### 1 INTRODUCTION

In the rapidly evolving field of artificial intelligence (AI), a paradigm shift is underway. We are witnessing the transition from specialized, fragmented deep learning models to versatile, one-size-fits-all foundation models. These advanced AI systems are capable of operating in an open-world context, interacting with open vocabularies and image pixels for unseen AI tasks, i.e., zero-shot abilities. They are exemplified by (1) Large Language Models (LLMs) such as GPTs [41](https://arxiv.org/html/2401.08092v2#bib.bib41) that can ingest almost every NLP task in the form as a prompt; (2) Vision Transformers Models (ViTs) such as Masked Autoencoder [141](https://arxiv.org/html/2401.08092v2#bib.bib141) that can handle various downstream vision tasks; (3) Latent Diffusion Models (LDMs) such as Stable Diffusion [336](https://arxiv.org/html/2401.08092v2#bib.bib336) that generate high-quality images with arbitrary text-based prompts; (4) Multimodal models such as CLIP [321](https://arxiv.org/html/2401.08092v2#bib.bib321) and ImageBind [123](https://arxiv.org/html/2401.08092v2#bib.bib123) that map different modal data into the same latent space and are widely used as backbone for cross-modality tasks like image retrieval/search and visual-question answering. Such flexibility and generality marks a significant departure from the earlier era of AI, setting a new standard for how AI interfaces with the world.

在快速发展的人工智能（AI）领域，模式正在发生转变。 我们正在见证从专业化、碎片化的深度学习模型向多功能、一刀切的基础模型过渡。 这些先进的人工智能系统能够在开放世界的环境中运行，与开放的词汇表和图像像素进行交互，以完成未曾见过的人工智能任务，即零拍摄能力。 它们的例子有：(1) 大型语言模型（LLMs），如 GPTs [41]，能以提示形式摄取几乎所有 NLP 任务；(2) 视觉转换器模型（ViTs），如 Masked Autoencoder [141]，能处理各种下游视觉任务； (3) 潜在扩散模型（LDM），如Stable Diffusion[336](https://arxiv.org/html/2401.08092v2#bib.bib336)，可生成高质量图像，并带有任意文本提示；(4) 多模态模型，如 CLIP [321] 和 ImageBind [123]，将不同模态数据映射到同一潜在空间，被广泛用作图像检索/搜索和视觉问题解答等跨模态任务的支柱。 这种灵活性和通用性标志着与早期人工智能时代的重大不同，为人工智能与世界的交互方式设定了新标准。

单词
1. a paradigm shift is underway 模式转变正在进行中
2. specialized, fragmented deep learning models to versatile, one-size-fits-all 专业化、碎片化的深度学习模型向多功能、一刀切
3. image pixels 图片像素 
4. They are exemplified by (1) Large Language Models (LLMs)    具体例子有 (1) 大语言模型
5. GPTs that can ingest almost every NLP task in the form as a prompt;   GPT 可以将表单中的几乎所有 NLP 任务进行提取作为提示；
6. latent space 潜空间
7. backbone 主干
8. cross-modality tasks 跨模态任务
9. image retrieval/search and visual-question answering. 图像检索/搜索和视觉问题解答
10. marks a significant departure(离开，出发，离职)  标志着重大转变


The success of these foundation models is deeply rooted in their scalability: unlike their predecessors, these models’ accuracy and generalization ability can continuously expand with more data or parameters, without altering the underlying simple algorithms and architectures. An impressive evidence is the scaling law [[177](https://arxiv.org/html/2401.08092v2#bib.bib177)]: it describes how the performance of transformer-based models can predictably improve with more model size and data volume; until today, the scaling law stands still. This scalability is not just a matter of model size; it extends to their ability to tackle increasingly complex tasks, making them a cornerstone in the journey towards artificial general intelligence (AGI).

这些基础模型的成功深深植根于它们的可扩展性：与它们的前辈不同，这些模型的准确性和泛化能力可以随着数据或参数的增加而不断扩大，而无需改变底层的简单算法和架构。 一个令人印象深刻的证据是==缩放定律[177]：它描述了基于transformer的模型的性能如何随着模型规模和数据量的增加而得到可预测的提高；直到今天，缩放定律仍未改变。== 这种可扩展性不仅仅是模型大小的问题，它还扩展到了处理日益复杂任务的能力，使其成为人工通用智能（AGI）的基石。

单词
1. predecessors 前任，前辈
2. volume 卷、音量、量
3. cornerstone 基石
4. AGI—— artificial general intelligence 人工通用智能

However, the scalability comes at a cost of huge resource demand. Foundation models, by their very nature, are resource-hungry for training and deployment. These resources encompass not only the computing processors like GPUs and TPUs, but also the memory, energy, and network bandwidth. For example, the pre-training of LLaMa-2-70B takes 1.7× millions of GPU hours and consumes 2.5×10^12 Joules of energy. The estimated total emissions were 291 tons of CO2 equivalent. Beyond training, the data processing, experimentation, and inference stages consume comparable or even more electricity according to Meta AI [[416](https://arxiv.org/html/2401.08092v2#bib.bib416)]. A recent analysis [[81](https://arxiv.org/html/2401.08092v2#bib.bib81)] reveals that, to satisfy the continuation of the current trends in AI capacity and adoption, NVIDIA needs to ship 1.5 million AI server units per year by 2027. These servers, running at full capacity, would consume at least 85.4 terawatt-hours of electricity annuall – more than what many countries like New Zealand and Austria use in a whole year, as illustrated in Figure [1](https://arxiv.org/html/2401.08092v2#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models"). Since foundation models proceed growth in size and complexity, their resource requirements escalate, often exponentially, posing a significant challenge in their development and deployment.

然而，可扩展性的代价是巨大的资源需求。 基础模型就其本质而言，在训练和部署时需要大量资源。 这些资源不仅包括 GPU 和 TPU 等计算处理器，还包括内存、能源和网络带宽。 例如，LLaMa-2-70B 的预训练需要 1.7× 百万 GPU 小时，消耗 2.5×10^12 焦耳能量。 估计总排放量为 291 吨二氧化碳当量。 据 Meta AI称，除训练外，数据处理、实验和推理阶段消耗的电力相当甚至更多。 最近的一项分析显示，为了满足当前人工智能容量和应用趋势的持续发展，英伟达公司需要在2027年前每年出货150万台人工智能服务器。 如图[1](https://arxiv.org/html/2401.08092v2#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models")所示，这些满负荷运行的服务器每年将消耗至少 85.4 太瓦时的电力，超过新西兰和奥地利等许多国家全年的用电量。 由于基础模型的规模和复杂性不断增长，其资源需求也随之增加，通常呈指数级增长，这给基础模型的开发和部署带来了巨大挑战。


单词
1. by their very nature 就他们本质而言
2. estimated 估计
3. comparable 相当
4. adoption 采用，应用
5. ship 船运
6. terawatt-hours 兆兆瓦时
7. as illustrated in Figure 就像图所示
8. escalate 升级
9. exponentially 指数地

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241102120850.png)
Figure 1: The electricity consumption comparison between countries and AI. Data source: [[81](https://arxiv.org/html/2401.08092v2#bib.bib81)].


The huge resource footprint of large foundation model also hinders its democratization. Till the end of 2023, there are only a few major players capable of training and deploying the state-of-the-art foundation models, who thereby have powerful control over the public and can potentially manipulate them in a way they prefer. The models are served on clouds instead of devices as many lightweight DNNs do [[434](https://arxiv.org/html/2401.08092v2#bib.bib434), [476](https://arxiv.org/html/2401.08092v2#bib.bib476)]; it makes data privacy preservation almost impossible. Though recently, smartphone vendors have been boasting about running large foundation models locally and some pioneering engines are developed for on-device LLMs [[121](https://arxiv.org/html/2401.08092v2#bib.bib121), [11](https://arxiv.org/html/2401.08092v2#bib.bib11), [10](https://arxiv.org/html/2401.08092v2#bib.bib10)], the models demonstrated are limited to relatively small scale (e.g., <10B) [[266](https://arxiv.org/html/2401.08092v2#bib.bib266)] and have not yet seen real-world deployment.

大型基金会模式的巨大资源占用也阻碍了其民主化。 到 2023 年底，只有少数几个主要公司有能力训练和部署最先进的基础模型，从而对公众拥有强大的控制权，并有可能按照自己喜欢的方式操纵这些模型。 正如许多轻量级 DNN 所做的那样 [434, 476]，这些模型在云端而非设备上提供服务；这使得数据隐私保护几乎成为不可能。 尽管最近智能手机供应商一直在吹嘘可以在本地运行大型基础模型，而且一些开创性的引擎也是为设备上的 LLMs 开发的[121, 11, 10]，但所展示的模型仅限于相对较小的规模（例如，小于 10B）[266]，而且尚未在现实世界中部署。


单词
1. smartphone vendors 智能手机供应商

Thereby, a significant amount of research has been dedicated to enhance the efficiency of these foundation models. These efforts span a wide range of approaches, from optimizing algorithms to system-level innovations, focusing on reducing the resource footprint of these models without compromising their performance. This survey aims to delve into these research efforts, exploring the diverse strategies employed to make foundation models more resource-efficient. We will examine advancements in algorithmic efficiency, system optimizations, data management techniques, and the development of novel architectures that are less resource-intensive. The survey also spans from clouds to edge and devices, where the large foundation models gain dramatic attentions as well. Through this exploration, we aim to provide a comprehensive understanding of the current state and future directions of resource-efficient algorithms and systems in the realm of foundation models.


因此，大量研究致力于提高这些基础模型的效率。 ==这些努力涵盖了从优化算法到系统级创新等多种方法，重点是在不影响性能的前提下减少这些模型的资源占用。== 本调查旨在深入探讨这些研究工作，探索为提高基础模型的资源效率而采用的各种策略。 我们将考察在算法效率、系统优化、数据管理技术以及开发资源密集度更低的新型架构方面取得的进展。 调查范围还包括云、边缘和设备，其中大型基础模型也受到了极大关注。 通过这些探索，我们旨在全面了解基础模型领域资源高效算法和系统的现状和未来发展方向。


单词
1.  be dedicated to 致力于
2. reducing the resource footprint of these models 减少这些模型的资源占用
3. compromise 妥协，折中
4. resource-intensive 资源密集型
5. span 跨越，范围，包括。涵盖
6. comprehensive 综合的；所有的；综合性的(接收各种资质的学生)；全部的；
7.  in the realm(领域) of foundation models. 在基础模型领域 

**Scope and rationales.** The scope of this survey is mainly defined by following aspects. (i) We survey only algorithm and system innovations; we exclude a huge body of work at hardware design, which is equally important but has been already wrapped up well [[192](https://arxiv.org/html/2401.08092v2#bib.bib192), [185](https://arxiv.org/html/2401.08092v2#bib.bib185)]. (ii) The definition of resource in this survey is limited to mainly physical ones, including computing, memory, storage, bandwidth, etc; we exclude training data (labels) and privacy that can also be regarded as resources; (iii) We mainly survey papers published on top-tier CS conferences, i.e., those included in CSRankings.

**范围和理由** 本次调查的范围主要由以下几个方面确定。 
(i) 我们只调查算法和系统创新；我们排除了硬件设计方面的大量工作，这些工作同样重要，但已被很好地总结[[192](https://arxiv.org/html/2401.08092v2#bib.bib192), [185](https://arxiv.org/html/2401.08092v2#bib.bib185)]。
(ii) 本调查中的资源定义主要限于物理资源，包括计算、内存、存储、带宽等；我们不包括也可视为资源的训练数据（标签）和隐私；
(iii) 我们主要调查在顶级 CS 会议上发表的论文，即 CSRankings 中收录的论文。


单词
1. Scope and rationales.  范围和理由。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241102153159.png)

We also manually pick related and potentially high-impact papers from arXiv.
(iv) We mainly survey papers published after the year of 2020, since the innovation of AI is going fast with old knowledge and methods being overturned frequently.

**Organization.** Figure [2](https://arxiv.org/html/2401.08092v2#S1.F2 "Figure 2 ‣ 1 INTRODUCTION ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models") illustrates the organization of this survey.

**Full open-source.** All materials of this survey are freely available at:  
  
[https:github.com/UbiquitousLearning/Efficient_Foundation_Model_Survey](https://arxiv.org/html/2401.08092v2/github.com/UbiquitousLearning/Efficient_Foundation_Model_Survey)

我们还人工从 arXiv 中挑选相关的、潜在的高影响力论文。 (iv) 我们主要调查 2020 年以后发表的论文，因为人工智能的创新速度很快，旧的知识和方法经常被推翻。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241102160017.png)

### 2 FOUNDATION MODEL OVERVIEW

#### 2.1 Language Foundation Models  语言基础模型

This section includes a discussion of both text-based and speech-based language models, highlighting their key architecture and milestone models.

本节将讨论基于文本和基于语音的语言模型，重点介绍其关键架构和里程碑模型。

##### 2.1.1 Model Architectures

**Transformer pipeline**. Vaswani et al. [388](https://arxiv.org/html/2401.08092v2#bib.bib388) introduced the attention-based Transformer architecture, a foundational element in the development of most Large FMs. As depicted in Figure [3](https://arxiv.org/html/2401.08092v2#S2.F3 "Figure 3 ‣ 2 FOUNDATION MODEL OVERVIEW ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models"), the process initiates by converting input words into high-dimensional vectors through an embedding layer. During processing, attention mechanisms assign varying weights to different segments of these input vectors. Following attention, layer normalization is applied to the output, ensuring stabilization and standardization of the activations. Subsequently, each position-wise vector undergoes transformation through a feedforward network, introducing non-linearity and enabling the model to capture complex data patterns. Through multiple layers that incorporate these components, the Transformer learns hierarchical representations of the input data. In the final stage, the output from the last Transformer layer is directed into a linear layer, culminating in the final prediction. We briefly outlines the key components of Large FMs as follows:

transformer管道。 Vaswani 等人[388](https://arxiv.org/html/2401.08092v2#bib.bib388)介绍了基于注意力的 Transformer 架构，这是开发大多数大型基础模型的基础元素。 如图[3](https://arxiv.org/html/2401.08092v2#S2.F3 "Figure 3 ‣ 2 FOUNDATION MODEL OVERVIEW ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models")所示，处理过程的第一步是通过嵌入层将输入词转换为高维向量。 在处理过程中，注意力机制会为这些输入向量的不同片段分配不同的权重。 注意之后，对输出进行层归一化处理，确保激活的稳定和标准化。 随后，每个位置向量通过前馈网络进行转换，引入非线性，使模型能够捕捉复杂的数据模式。 通过包含这些组件的多个层，transformer可以学习输入数据的分层表示。 在最后阶段，最后一个transformer层的输出被导入线性层，最终得出预测结果。 我们简要概述大型基础模型的关键组成部分如下：


单词

1. ==“FMs”指“Foundation Models”，即基础模型。==
2. mechanisms 机制
3.  Subsequently 随后
4. undergo 经历，经受
5. a feedforward network 一个前馈神经网络
6. Through multiple layers that incorporate these components  通过包含这些组件的多个层     incorporate  使并入；包含；合并；注册成立；吸收；
7. the Transformer learns hierarchical representations of the input data  transformer可以学习输入数据的分层表示    hierarchical   等级制的；等级制度的；按等级划分的
8. culminate 达到顶点；(以某种结果)告终；



**Embedding**. Initially, the input word is transformed into a sequence of tokens by a tokenizer. Commonly used tokenizers, such as wordpiece and byte-pair encoding, are frequently employed in this process [[380](https://arxiv.org/html/2401.08092v2#bib.bib380)]. Following tokenization, a learned embedding layer converts these tokens into a sequence of vectors. In such sequences, the order of words is essential for meaning. To address this, position encoding is incorporated into the embeddings, infusing them with positional information. This addition is critical for capturing the sequential nature of the input, ensuring that the model accurately interprets word order and context.

**嵌入**。 首先，输入的单词会被标记化器转换成一串token。 在这一过程中，常用的标记化器，如词片和字节对编码，经常被使用[[380](https://arxiv.org/html/2401.08092v2#bib.bib380)]。 标记化之后，学习嵌入层将这些标记转换成向量序列。 在这些序列中，词的顺序对意义至关重要。 为了解决这个问题，嵌入层中加入了位置编码，为它们注入了位置信息。 这种添加对于捕捉输入的顺序性至关重要，可确保模型准确解释词序和上下文。

单词
1. infuse  注入；输注（药物等）；使具有(某特性)；
2. sequential nature 顺序性

**Attention.** Attention mechanisms play a crucial role in capturing the relationships between words in a sequence. The calculation of attention can be represented as:
$$
A(Q,K,V) = softmax(\frac{QK^T}{\sqrt{ d_{k}}}) V
$$

注意机制在捕捉序列中单词之间的关系方面发挥着至关重要的作用.

==where Q, K, and V represent the query, key, and value==, respectively; each derived by multiplying the input vector with a distinct weight matrix, and $d_{k}$ denotes the dimension of these vectors. Self-attention, a specific form of attention where queries, keys, and values all originate from the same input sequence, enables the model to focus on different segments of the input for each position. In contrast, multi-head attention, a variation of self-attention, permits simultaneous attention to information from diverse representation subspaces at different positions. Other variants, such as sparse attention [[36](https://arxiv.org/html/2401.08092v2#bib.bib36)] and multi-query attention [[346](https://arxiv.org/html/2401.08092v2#bib.bib346)], are tailored for efficiency or various downstream tasks. These variants are further detailed in §[3.1](https://arxiv.org/html/2401.08092v2#S3.SS1 "3.1 Efficient Attention ‣ 3 RESOURCE-EFFICIENT ARCHITECTURES ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models"), §[4.3.3](https://arxiv.org/html/2401.08092v2#S4.SS3.SSS3 "4.3.3 Key-Value Cache ‣ 4.3 Inference Algorithms ‣ 4 RESOURCE-EFFICIENT ALGORITHMS ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models") and §[4.3.4](https://arxiv.org/html/2401.08092v2#S4.SS3.SSS4 "4.3.4 Long Context ‣ 4.3 Inference Algorithms ‣ 4 RESOURCE-EFFICIENT ALGORITHMS ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models").


其中，Q、K 和 V 分别代表查询、键和值；每个向量都是通过将输入向量与一个不同的权重矩阵相乘而得到的，$d_{k}$ 表示这些向量的维度。==自注意力是一种特定形式的注意力，其中查询、键和值都源自相同的输入序列，它能使模型关注每个位置的不同输入片段。==相比之下，==多头注意力是自我注意力的一种变体，它允许在不同位置同时关注来自不同表征子空间的信息。==其他变体，如稀疏注意力和多查询注意力，则是为==提高效率或完成各种下游任务而量身定制的==。这些变体在§[3.1](https://arxiv.org/html/2401.08092v2#S3.SS1 “3.1 高效注意力‣ 3 资源高效架构‣ 资源高效 LLM 和多模态基础模型概览”)、§[4.3.3](https://arxiv.org/html/2401.08092v2#S4.SS3.SSS3 "4.3.3 键值缓存‣ 4. 3 推论算法 ‣ 4 资源高效算法 ‣ 资源高效 LLM 和多模态基础模型概览"）和§[4.3.4](https://arxiv.org/html/2401. 08092v2#S4.SS3.SSS4 “4.3.4 长语境 ‣ 4.3 推论算法 ‣ 4 资源节约型算法 ‣ 资源节约型 LLM 和多模态基础模型概览”）。

单词
1. respectively  分别，各自
2. each derived by multiplying the input vector with a distinct weight matrix   每个都是通过将输入向量与一个不同的权重矩阵相乘而得出的
3. derived 获得;取得;得到
4. d_k denotes the dimension of these vectors.  d_k表示这些向量的维数。denotes  表示，标志，象征
5.  a variation of self-attention  自注意力机制的==变体== 
6. simultaneous 同时

**Encoder-decoder architecture.** The standard Transformer architecture consists of two main components: an encoder and a decoder. Encoder processes the input sequence through self-attention mechanisms, allowing the model to assign varying weights to different segments of the input sequence based on their relative importance. This feature is crucial for discerning complex patterns and dependencies within the input data. In contrast, the decoder is responsible for generating the output sequence. Decoder utilizes self-attention mechanisms to understand the relationships within the generated output so far. Additionally, the decoder incorporates cross-attention mechanisms, focusing on the input sequence to extract relevant information for each token in the output sequence. This part of the architecture is autoregressive, generating tokens sequentially. The production of each token depends on the tokens generated previously, unlike the parallel processing approach of the encoder.


**编码器-解码器架构** 标准转换器架构由两个主要部分组成：编码器和解码器。**编码器通过自我关注机制处理输入序列，允许模型根据输入序列不同片段的相对重要性为其分配不同的权重。** 这一功能对于识别输入数据中的复杂模式和依赖关系至关重要。相反，**解码器负责生成输出序列。解码器利用自我注意机制来理解迄今为止生成的输出中的关系。** 此外，解码器还采用**交叉注意机制，重点关注输入序列，以提取输出序列中每个标记的相关信息**。架构的这一部分是**自回归的，按顺序生成标记。** 每个标记的生成都取决于之前生成的标记，这**与编码器的并行处理方法不同。**

单词
1. discerning adj. 敏锐的；有眼力的；v. 识别；(依稀)看出，分辨出，听出；
2. within the input data 在输入数据中。
3. incorporate  v.使并入；包含；注册成立；吸收；将…包括在内


**Auto-regressive decoding and KV cache.** In auto-regressive decoding, the decoder function $F_{Decoder}$ infers a new token $x_{i+1}$ based on the input token sequence $X=\{x_1,x_{2},\dots,x_{i}\}$. Subsequently, $x_{i+1}$ is added to $X$ for the next inference step, constituting auto-regressive decoding. Key-Value (KV) cache [[76](https://arxiv.org/html/2401.08092v2#bib.bib76)] enhances efficiency by storing intermediate states of the attention mechanism at each step. This approach prevents recalculation of tokens processed in earlier steps. While mitigating re-computation, KV cache introduces additional storage or memory overhead, detailed in §[2.1.3](https://arxiv.org/html/2401.08092v2#S2.SS1.SSS3 "2.1.3 Cost Analysis ‣ 2.1 Language Foundation Models ‣ 2 FOUNDATION MODEL OVERVIEW ‣ A Survey of Resource-efficient LLM and Multimodal Foundation Models").

**自回归解码和KV缓存。**在自回归解码中，解码器函数$F_{decoder}$根据输入令牌序列$x=\{x_1，x_{2}，\dots，x_{1}\}$推断出一个新的令牌$x_{i+1}$。随后，在下一个推理步骤中，将$x_{i+1}$添加到$x$中，构成自回归解码。键值（KV）缓存[[76](https://arxiv.org/html/2401.08092v2#bib.bib76)]通过在每个步骤存储注意力机制的中间状态来提高效率。这种方法可以防止重新计算在早期步骤中处理的令牌。在减轻重新计算的同时，KV缓存引入了额外的存储或内存开销，详见§[2.1.3](https://arxiv.org/html/2401.08092v2#S2.SS1.SSS3“2.1.3成本分析？2.1语言基础模型？2基础模型概述？资源节约型LLM和多模式基础模型调查”）。

单词
1. constitute v.组成，构成；制定(法律等)；

```ad-note

1、自回归是根据序列X:$x1->xi$ 推出$x_{i+1}$,然后把$x_{i+1}$继续放入X中，然后推测$x_{i+2}$，这就是自回归。
2、KVcache KV缓存是存储每个步骤注意力机制的中间状态来提高效率，减少重复计算，但是引入了额外的内存等开销。
```

##### 2.1.2 Representative Models and Downstream Tasks

**代表性模型和下游任务。**
 
**Encoder-only FMs.** BERT [88] is an encoder-only Transformer model that utilizes a bidirectional masked language modeling approach during pre-training. In this approach, random words within a sentence are masked, and the model learns to predict these masked words by considering contextual clues. Fine-tuning BERT for various downstream tasks has led to SOTA performance, particularly in discriminative tasks like sentiment analysis and text classification. DistilBERT [340] is a distilled version of BERT, being 40% smaller and 60% faster, yet retaining 97% of BERT’s language comprehension capabilities. RoBERTa [256] enhances BERT’s efficacy through robust optimization tech- niques, including extended training duration, increased batch sizes, and a larger data corpus. Sentence-BERT [332] modifies BERT to generate semantically meaningful sentence embeddings. Employing siamese and triplet network structures, these embeddings can be directly compared using cosine similarity. This model has evolved into the widely utilized sentence-transformer tool1, specializing in sentence embedding.

**Encoder-only FM**。BERT[88]是一种仅编码器Transformer模型，在预训练期间采用双向掩码语言建模方法。在这种方法中，句子中的随机单词被屏蔽，模型通过考虑上下文线索来学习预测这些被屏蔽的单词。对各种下游任务的BERT进行微调可以提高SOTA的性能，特别是在情感分析和文本分类等区分性任务中。DistilBERT[340]是BERT的简化版本，体积小40%，速度快60%，但保留了BERT 97%的语言理解能力。RoBERTa[256]通过稳健的优化技术提高了BERT的效率，包括延长训练时间、增加批量大小和更大的数据语料库。句子BERT[332]修改BERT以生成语义上有意义的句子嵌入。采用连体和三重网络结构，这些嵌入可以直接使用余弦相似性进行比较。该模型已经发展成为广泛使用的句子转换工具1，专门用于句子嵌入。

单词
1. bidirectional 双向的
2. by considering contextual clues 通过考虑上下文线索
3. discriminative(adj.有判别力的 ) tasks 区分性任务
4. distilled version 蒸馏版本（简化版本）
5. efficacy 功效，效力
6. robust optimization techniques 稳健的优化技术
7. duration 持续时间、期间
8. corpus 语料库，(书面或口语的)文集，文献，汇编
9. semantically 语义上，在语义上，语意语义地
10. Employ 雇用，应用；运用
11. siamese and triplet network structures  孪生网络和三元组网络结构 https://zhuanlan.zhihu.com/p/55254100
12. evolve 发展；进化；


```ad-note
- BERT是一种encode-only 的Transformer模型，在预训练期间采用双向掩码语言建模方法。
- 微调bert对下游任务比较好，尤其是区分性任务像情感分析和文本分类。比如 DistilBERT、RoBERT、Sentence-BERT
- 孪生网络和三元组网络结构 https://zhuanlan.zhihu.com/p/55254100
```

**Encoder-decoder FMs.** T5 [325] employs an encoder-decoder architecture and is self-supervised, undergoing pre- training on the C4 dataset. This model introduces a unified framework that converts diverse text-based language problems into a text-to-text format, rendering it applicable to tasks such as summarization and question answering. BART [210] serves as a denoising autoencoder in the pretraining phase, introducing corruption to text through an arbitrary noising function. The primary objective is to learn the reconstruction of the original text. 

**Encoder-decoder FMs.** T5[325]采用编码器-解码器架构，具有自我监督功能，在C4数据集上进行预训练。该模型引入了一个统一的框架，将各种基于文本的语言问题转换为文本到文本的格式，使其适用于摘要和问答等任务。BART[210]在预训练阶段充当去噪自动编码器，通过任意的去噪函数对文本进行破坏。主要目的是学习对原始文本的重建。

单词
1. self-supervised 自我监督
2. a unified framework 一个统一的框架
3. render 提供；提交；给予；翻译；表达 
4. applicable 适用，合适的
5. summarization 摘要
6. serves as 作为
7. a denoising autoencoder 一种去噪自动编码器
8. corruption  n. 腐败；贪污；
9. an arbitrary(任意的；武断的；专横的) noising function 一个任意的噪声函数


```ad-note
- **T5：**统一的框架，将各种基于文本的语言问题转换为文本到文本的格式，使其适用于摘要和问答等任务。
- **BART：**预训练阶段充当去噪自动编码器，通过任意的去噪函数对文本进行破坏。主要目标是学习原始文本的重建。
```

**Decoder-only FMs.** The GPT family [323, 324, 41] utilizes a decoder-only architecture for unsupervised training. GPT-1 [323], the inaugural model in the series, features a transformer architecture with 117 million parameters and demonstrates the efficacy of pre-training on diverse internet text. GPT-2 [324], an enlarged iteration of GPT-1, un- dergoes unsupervised training on WebText, a dataset encompassing millions of webpages. GPT-3 [41] represents a substantial increase in model size with 175 billion parameters, highlighting the advantages of scaling up. It demon- strated exceptional zero-shot performance. Instruct tuning [300] further enhances the model’s capability to accurately follow instructions using human feedback, contributing to the creation of several open-source foundation models, in- cluding LLaMA [383]. GLM [95] improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. PaLM [70] was trained on 6144 TPU v4 chips using Pathways to further understand of the impact of model scale on few-shot learning. Additionally, there are numerous close-sourced generative large FMs, including GPT-42, Claude23, and PaLM 24, among others.

**Decoder-only FMs.** GPT家族[32324,41]利用仅解码器的架构进行无监督训练。GPT-1[323]是该系列中的首款型号，具有1.17亿个参数的变压器架构，并展示了在不同互联网文本上进行预训练的有效性。GPT-2[324]是GPT-1的扩展迭代，在WebText上进行无监督训练，WebText是一个包含数百万个网页的数据集。GPT-3[41]代表了1750亿个参数的模型尺寸的大幅增加，突显了扩大规模的优势。它展示了卓越的零样本性能。指令调优[300]进一步增强了模型使用人类反馈准确遵循指令的能力，有助于创建几个开源基础模型，包括LLaMA[383]。GLM[95]通过添加2D位置编码并允许任意顺序预测跨度来改进填充式预训练，从而在NLU任务上比BERT和T5的性能更高。PaLM[70]使用Pathways在6144 TPU v4芯片上进行了训练，以进一步了解模型规模对少样本学习的影响。此外，还有许多闭源生成型大型功能模块，包括GPT-42、Claude23和PaLM 24等。


单词
1. inaugural 创始的，开幕的
2. feature  v. 以…为特色; 由…主演; 占重要地位; 起重要作用; 以…为主要组成 
3. demonstrate v. 证明，展示，表达; 表露; 显露
4. encompassing 包含，包括，涉及
5. substantial(大量的，重要的，坚实的) increase 大量增加
6. highlight 突出，强调
7. scaling up  按比例放大; 按比例增加
8. exceptional 杰出的，卓越的
9. **NLU 自然语言理解**
10. **few-shot**  Few-shot 学习旨在让模型在仅有少量训练样本（通常远少于传统监督学习所需样本量）的情况下，学习并完成特定任务，如分类、生成等。

```ad-note
**GPT1** 具有1.17亿个参数的transformer架构，在不同互联网文本上进行预训练的有效
**GPT2** 在WebText上进行无监督训练，WebText是一个包含数百万个网页的数据集
**GPT3**  1750亿个参数的模型尺寸的大幅增加，突显了扩大规模的优势。指令微调->LLaMA
**GLM** 通过添加二维位置编码以及允许以任意顺序预测跨度改进了填空式预训练，这使得它在自然语言理解任务上比 BERT 和 T5 取得了更好的性能。
**PaLM** 使用Pathways在6144 TPU v4芯片上进行了训练，以进一步了解模型规模对少样本学习的影响。
**闭源模型** GPT-42、Claude23和PaLM 24

```

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241208191642.png)
table1 milestone language foundation models and their typical tasks
里程碑语言基础模型和他们典型的任务

单词
- oriented(朝向；适应) tasks  定向任务

**Speech FMs.** Speech large FMs [322, 28, 148] have been engineered to derive meaningful representations from raw audio signals. Wav2vec 2.0 [28] showcases, for the first time, that acquiring robust speech representations from unlabeled data could significantly improve performance in subsequent speech-related tasks. These models typically employ convolutional neural network to extract serial features and a transformer to capture contextual information. This approach is effective for various downstream tasks, including speech recognition and spoken language under- standing. For instance, HuBERT [148] leverages a transformer-based architecture for self-supervised learning of speech representations, trained on the 960-hour LibriSpeech audio dataset [305]. Whisper [322] represents a state-of- the-art open-sourced automatic speech recognition system, trained on a vast corpus of 680,000 hours of multilingual and multitask supervised data sourced from the web. 

**Speech FMs.** 语音大FM[322,28148]已被设计为从原始音频信号中导出有意义的表示。Wav2vec 2.0[28]首次表明，从未标记数据中获取鲁棒的语音表示可以显著提高后续语音相关任务的性能。这些模型通常采用卷积神经网络来提取序列特征，并使用变换器来捕获上下文信息。这种方法对各种下游任务都很有效，包括语音识别和口语理解。例如，HuBERT[148]利用基于变换器的架构对语音表示进行自我监督学习，在960小时的LibriSpeech音频数据集上进行训练[305]。Whisper[322]代表了一种最先进的开源自动语音识别系统，该系统在来自网络的68万小时多语言和多任务监督数据的庞大语料库上进行了训练。

单词
1. be engineered to 被设计成
2. derive 获得，得到
3. multilingual 多种语言的

```ad-note
- **Wav2vec 2.0**  首次表明，从未标记数据中获取稳健的语音表征能够显著提升后续语音相关任务的性能。
- **HuBERT**  利用基于transformer的架构对语音表示进行自我监督学习,数据集是960小时的LibriSpeech音频数据集
- **Whisper** 一种最先进的开源自动语音识别系统
```

##### 2.1.3 Cost Analysis 

成本，费用分析

As depicted in Figure 4, we analyze the computational and storage costs associated with the primary components of large FMs5. The embedding component constitutes a significant portion of storage costs, approximately 25% of the total. However, during inference, the embedding functions as a lookup table, incurring minimal computational costs. The FFN layer emerges as the most computation-intensive component, primarily due to the presence of two fully connected layers in each FFN block. lm head, which signifies the output layer of the model, varies depending on the task. For discriminative tasks like BERT, it takes the form of a classification layer with a softmax activation function, whereas for generative tasks like GPT/T5, it manifests as a linear layer. The size of this component is directly proportional to the vocabulary size.

如图4所示，我们分析了与大型FM的主要组件相关的计算和存储成本。嵌入组件占存储成本的很大一部分，约占总成本的25%。然而，在推理过程中，嵌入充当查找表，从而产生最小的计算成本。FFN层是计算最密集的组件，主要是因为每个FFN块中都有两个完全连接的层。lm-head表示模型的输出层，根据任务而变化。对于像BERT这样的判别任务，它采用具有softmax激活函数的分类层的形式，而对于像GPT/T5这样的生成任务，它表现为线性层。这个组件的大小与词汇量成正比。

单词
1. incur(引致，带来(成本、花费等)) minimal costs 带来最小的成本
2. emerge 出现，显露
3. signify  代表，表示，象征；
4. manifests as 表现为


```ad-note
- **embedding 嵌入组件** 占存储成本 25%，但是计算成本很少
- **FFN层-前馈神经网络(Feed-Forward Neural Network)**
	- 计算密集层
	- 每个FFN层包括2个全连接层
- **Im head(language model head) 语言模型头**
	- 代表着模型的输出层
	- 判别任务 如BERT 采用 softmax层
	- 生成任务 像GPT/T5 采用线性层
```
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241208200218.png)

**Cost Analysis Under Different Token Lengths.** The attention mechanism in large FMs faces significant computa- tional bottlenecks primarily due to its quadratic complexity. This complexity stems from calculating attention scores for every pair of positions within the input sequence, posing challenges in managing long sequences and impacting both training and inference efficiency. Additionally, beyond the attention mechanism, the computation complexity of the FFN scales linearly with input length but quadratically with the model’s dimension. As depicted in Figure 5, an increase in the length of the input sequence causes a substantial rise in computational demand, attributable to the quadratic nature of the attention mechanism. In quantitative terms, the computation complexity of attention is O(T^2D), while that of FFN is O(T D^2), where T represents the sequence length and D the hidden state dimension of the model [244]. The decoder’s attention mechanism, similar to that in the encoder, also experiences quadratic scaling with token length. This aspect becomes particularly significant in autoregressive decoding tasks, where each token’s generation depends on the preceding ones, intensifying computational requirements. The implementation of a KV cache in the decoder can substantially mitigate computational costs by reusing key and value vectors across various positions. However, this comes at the expense of additional memory requirements [199]. Assuming that B represents the batch size, S the sequence length, D the size of the hidden dimension, and L the number of layers in the Transformer decoder, the memory required for storing the KV cache in single-precision format can be calculated as (B × S × D × L × 2 × 4) Bytes. This formula considers the dimensions of the batch, sequence, and hidden layer, as well as the number of layers in the decoder, with the factor of 2 representing the key and value in the cache and the factor of 4 accounting for the byte size of single-precision floating-point representation. 

**不同token长度下的成本分析。** 大型FM中的注意力机制主要由于其二次复杂性而面临重大的计算瓶颈。这种**复杂性源于计算输入序列中每对位置的注意力得分**，这给管理长序列带来了挑战，并影响了训练和推理效率。此外，除了注意机制之外，FFN的计算复杂度与输入长度呈线性关系，但与模型的维度呈二次关系。如图5所示，由于注意力机制的二次性，输入序列长度的增加会导致计算需求的大幅增加。在定量方面，注意力的计算复杂度为O（T^2D），而FFN的计算复杂程度为O（T D^2），其中T表示序列长度，D表示模型的隐藏状态维度[244]。解码器的注意力机制与编码器中的机制类似，也经历了令牌长度的二次缩放。这一方面在自回归解码任务中变得尤为重要，其中每个令牌的生成都取决于前一个令牌，从而增加了计算要求。**在解码器中实现KV缓存可以通过在不同位置重用密钥和值向量来大大降低计算成本。** 然而，这是以额外的内存需求为代价的[199]。假设B表示批大小，S表示序列长度，D表示隐藏维度的大小，L表示Transformer解码器中的层数，则以单精度格式存储KV缓存所需的内存可以计算为（B×S×D×L×2×4）字节。此公式考虑了批处理、序列和隐藏层的维度，以及解码器中的层数，系数2表示缓存中的键和值，系数4表示单精度浮点表示的字节大小。

单词
1. computational bottlenecks(瓶颈，阻碍，障碍) 计算瓶颈
2. quadratic complexity 二次复杂性

```ad-note
- **注意力机制的计算成本**会随着输入规模的增大而以**二次方**的速度增加，这种快速增长的计算成本会对模型的性能和效率产生严重的限制 T^2D
	- 复杂性源于**计算输入序列中每对位置的注意力得分**
- FFN的计算复杂度**与输入长度呈线性关系，但与模型的维度呈二次关系** TD^2
- 在解码器中实现KV缓存可以通过**在不同位置重用K和V向量**来大大降低计算成本。
	- **单精度格式存储KV缓存所需的内存可以计算为（B×S×D×L×2×4）字节。**
```

**Speech-specific Considerations.** In speech processing applications, CNN encoder block plays a significant role in computational complexity. The initial layers of the CNN block demand substantially more compute power, often exceeding that of each individual transformer layer by an order of magnitude. The increased requirement is attributed to the convolutional operations intrinsic to the CNN block, where a large number of calculations must be performed for each input token. For instance, despite the transformer having 19× more parameters, wav2vec 2.0 model [28] incurs only 1.8× more computational load compared to the CNN block [117]


**语音特定考虑因素。** 在语音处理应用中，CNN编码器块在计算复杂度方面起着重要作用。CNN块的初始层需要更多的计算能力，通常比每个单独的transformer高出一个数量级。需求的增加归因于CNN块固有的卷积运算，其中必须对每个输入令牌执行大量计算。例如，尽管变压器的参数增加了19倍，但与CNN块相比，wav2vec 2.0模型[28]的计算负载仅增加了1.8倍[117]

单词
1. substantially  adv.基本上; 大体上; 非常; 
2. exceed  v.超过(数量); 超越(法律、命令等)的限制
3. order of magnitude(巨大; 重大; )    数量级
4. intrinsic 固有的，内在的
5. incur 招致；引起，带来（花费等）
6. load 负载，工作量，承载量

```ad-note
**CNN:** 
- 初始层比单独的transformer计算能力高一个数量级
- 原因在于 CNN的卷积运算
```

#### 2.2 Vision Foundation Models 
视觉基础模型
##### 2.2.1 Model Architecture 
模型架构

The advancements in transformers have propelled the emergence of foundation models in the field of computer vision. 
Vision Transformer pipeline. **Vision Transformer** (ViT) is the most classic transformer-empowered vision model. It is inspired by the growing trend of self-supervised pre-trained NLP models like BERT, which is encoder-only. Given an input image, ViT firstly splits image into fixed-size patches (i.e., tokens) by a convolutional embedding layer. For instance, a standard size RGB image input (i.e., 3×224×224) will be splitted to 14×14 patches with 16×16 pixels. This embedding overhead is almost negligible compared to the following compute-intensive transformer encoder (e.g., less than 5%). Besides, an extra learnable classification token ([CLS]) is added to the token sequence in order to perform classification. After that, positional embeddings are added into each token, and tokens are fed to a standard Transformer encoder, which has been depicted in Figure 3 and §2.1. Depending on the specific downstream tasks,the hidden states generated by the Transformer encoder are finally fed into different heads, such as classification, detection, segmentation, etc.

transformers的进步推动了计算机视觉领域基础模型的出现。
Vision Transformer 管道。
视觉transformer（ViT）是最经典的由transformer驱动的视觉模型。它的灵感来自于自监督预训练NLP模型的增长趋势，如BERT，它只是encoder-only.。给定一个输入图像，ViT首先通过卷积嵌入层将图像分割成固定大小的块（即标记）。例如，一个标准大小的RGB图像输入（即3×224×224）将被分割成14×14个16×16像素的补丁。与以下计算密集型变换器编码器相比，这种嵌入开销几乎可以忽略不计（例如，小于5%）。此外，为了执行分类，在标记序列中添加了一个额外的可学习分类标记（[CLS]）。之后，将位置嵌入添加到每个令牌中，并将令牌馈送到标准Transformer编码器，如图3和§2.1所示。根据特定的下游任务，Transformer编码器生成的隐藏状态最终被馈送到不同的头部，如分类、检测、分割等。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241216214058.png)

单词
1. propel  v.推进; 推动; 
2. empowered v.授权; 给(某人)…的权力; 驱动
3. inspired adj. 受到启发的; (与名词、形容词以及副词构成形容词)受…影响的; 
4. patches n.补丁; (与周围不同的)小块，小片; 
5. pixels n.像素(组成屏幕图像的最小独立元素)
6. overhead n.开销; 经常费用; 经常开支; （尤指飞机的）顶舱; 用于高射投影器的幻灯片 adj.高架的; 管理费用的; 经费的; 头上方的; 地面以上的 adv.在空中; 在头上方
7. negligible adj.可以忽略不计的; 微不足道的; 不重要的; 不值一提的
8. compute-intensive adj.计算密集型的
9. depicted v.描绘; 描述; 描写; 刻画; 描画
10. depending on 取决于; 依据，根据
11. segmentation n.分割; 划分; 分割成(或划分成)的部分

```ad-note
- 视觉transformer（ViT）是最经典的由transformer驱动的视觉模型,灵感来源于BERT
- 分割乘patch,然后加入CLS和位置编码->transformer->下游任务
- Transformer编码器生成的隐藏状态最终被馈送到不同的头部，如分类、检测、分割等。
- learnable classification token [CLS] 可学习分类标记
```

##### 2.2.2 Representative Models and Downstream Tasks

**Encoder-only.** Most visual foundation models are encoder-only architecture. ViT [92] is the first work that success- fully trains a Transformer encoder on ImageNet, with alignment to BERT in terms of parameter amount. It performs both supervised and self-supervised pre-training on a large-scale ImageNet-21k dataset. Although it shows competitive accuracy and scalabilty, its demand for training data compared to traditional CNNs is still an obstacle. To this end, DeiT [382] is proposed with much impact. DeiT performs a distillation-empowered pre-training, which enhances the data-efficiency of ViT training

**Encoder-only.** 大多数视觉基础模型都是Encoder-only 架构。ViT[92]是第一个在ImageNet上成功完全训练Transformer编码器的工作，在参数量方面与BERT对齐。它在大规模ImageNet-21k数据集上执行监督和自我监督预训练。尽管它显示出具有竞争力的准确性和可扩展性，但与传统的CNN相比，它对训练数据的需求仍然是一个障碍。为此，DeiT[382]的提出产生了很大的影响。DeiT执行蒸馏授权的预训练，提高了ViT训练的数据效率。

单词
1. with alignment to 和..对齐
2. large-scale adj.大规模的; 大范围的; 
3. obstacle n.障碍; 障碍物;
4. scalabilty n.可扩展性
5. distillation n.蒸馏; 蒸馏法

```ad-note
- **大多数视觉基础模型都是Encoder-only架构**
- **VIT** 
	- 第一个在ImageNet上成功完全训练Transformer编码器的工作，参数量和BERT一样。
	- 优点：准确性和可扩展性
	- 缺点: 对训练数据的需求量很大
- **DeiT**
	- 执行蒸馏驱动的预训练，加速VIT
```

Another storyline is to push the boundary of self-supervised ViT pre-training. BEiT [33] turns the pre-training objective to recover the original visual tokens based on the corrupted image patches. The authors claim that BEiT is “a path to the BERT moment of CV”. MAE [141] introduces a lightweight decoder to the encoder training for reconstructing the masked patches. MAE can effectively mask out most of the patches (by even 80%). Thereby, the training of the encoders can be very cost-effective, which paves the way for large pre-trained vision models. 

另一个故事情节是推动自我监督ViT预训练的边界。BEiT[33]将预训练目标转向基于损坏的图像补丁恢复原始视觉标记。作者声称，BEiT 是 “通向计算机视觉（CV）领域的 BERT 时刻的一条路径”。MAE[141]在编码器训练中引入了一个轻量级解码器，用于重建掩码补丁。MAE可以有效地掩盖大部分补丁（甚至80%）。因此，编码器的训练可以非常经济高效，为大型预训练视觉模型铺平了道路。

单词
1. corrupted v.损坏; 破坏; 使腐化; 使堕落; 引起(计算机文件等的)错误

```ad-note
另一条路**推动自我监督ViT预训练的边界**
- **BEiT**
	- 将预训练目标转向基于损坏的图像补丁恢复原始视觉标记
	- BEiT is “a path to the BERT moment of CV
- **MAE**
	- 在编码器训练中引入了一个轻量级解码器，用于重建掩码补丁，并有效的填充大部分补丁(80%)
- 编码器的训练可以非常经济高效，为大型预训练视觉模型铺平了道路。
```

YOLOS [102] is an object detection model built atop ViT. It demonstrates the transferability of the vanilla ViT pre- trained on mid-sized ImageNet-1k to the more challenging COCO object detection benchmark. ViTDet [228] enables a plain, non-hierarchical ViT to serve as a backbone network for object detection. ViTDet allows the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for MAE-style pre-training. Swin Transformer [260] is a representative work that optimizes the attention mechanism. This model is a hierarchical ViT whose representation is computed with shifted windows. DINOv2 [297] conducts training on a ViT model with 1 billion parameters and then distills it into a set of smaller models that outperform the leading all-purpose features, such as OpenCLIP, on the majority of benchmarks at both image and pixel levels

YOLOS[102]是建立在ViT之上的目标检测模型。它展示了在中型ImageNet-1k上预训练的vanilla ViT可以转移到更具挑战性的COCO对象检测基准。ViTDet[228]使一个简单的、非分层的ViT能够作为对象检测的骨干网络。ViTDet允许对原始的ViT架构进行微调，以用于对象检测，而无需为MAE风格的预训练重新设计分层骨干。Swin Transformer[260]是优化注意力机制的代表性作品。该模型是一个分层ViT，其表示是用移位窗口计算的。DINOv2[297]对具有10亿个参数的ViT模型进行训练，然后将其提炼成一组较小的模型，这些模型在图像和像素级别的大多数基准测试中都优于领先的通用功能，如OpenCLIP

单词
1. atop prep.在...之上
2. benchmark n.基准
3. plain adj. 普通的，简单的，清楚的
4. non-hierarchical adj. 非分层的; 非阶层性
5. backbone n.骨干; 支柱; 基础; 
6. fine-tuned v.对…微调 adj.有调整的；有提高的
7.  distills it into 提取它

```ad-note
- **YOLOS**
	- 是建立在ViT之上的目标检测模型。
	- 在中型ImageNet-1k上预训练的vanilla ViT可以转移到更具挑战性的COCO对象检测基准
- **ViTDet**
	- 使一个简单的、非分层的ViT能够作为目标检测的骨干网络。
- **Swin Transformer**
	- **优化注意力机制**的代表性作品
	- 该模型是一个分层ViT，其表示是用移位窗口计算的
- **DINOv2**
	- 对具有10亿个参数的ViT模型进行训练，然后将其提炼成一组较小的模型，这些模型在图像和像素级别的大多数基准测试中都优于领先的通用功能，如OpenCLIP
```

**Encoder-decoder.** DETR [49] is an early effort to build an end-to-end detection pipeline with transformers. The architecture of DETR is cascaded: it consists of a CNN backbone and an encoder-decoder transformer. DETR sup- ports object detection, instance segmentation and panoptic segmentation through supervised training. The parameter amount of DETR aligns with Faster-RCNN [334], which has about 40M parameters. SegFormer [427] is a semantic segmentation model which unifies Transformers with lightweight multilayer perception (MLP) decoders. LVM [30] has achieved effective learning of visual information using a purely visual approach through image sequence modeling, without the need for any linguistic data.

**编码器-解码器。** DETR[49]是早期使用transformers构建端到端检测流水线的尝试。DETR的架构是级联的：它由CNN骨干网和**Encoder-decoder**transformer组成。DETR通过监督训练支持对象检测、实例分割和全景分割。DETR的参数量与Faster RCNN[334]对齐，后者有大约40M个参数。SegFormer[427]是一种语义分割模型，它将Transformers与轻量级多层感知（MLP）解码器相结合。LVM[30]通过图像序列建模，使用纯视觉方法实现了视觉信息的有效学习，而不需要任何语言数据。

单词
1. cascaded adj.[电]级联的
2. panoptic adj. 全景的；
3. aligns with 与...一致
4. semantic adj.语义的
5. multilayer perception(感知；看法；悟性) 多层感知机

```ad-note
- **DETR**
	- 早期使用transformers构建端到端检测流水线
	- 架构:级联
	- 组成： CNN backbone + transformer (**Encoder-decoder**)
	- 特点：支持目标检测、实例分割和全景分割
	- 数据量：和Faster RCNN相近，40M个参数
- **SegFormer**
	- 语义分割模型
	- 架构： transformer + MLP(多层感知机)解码器
- **LVM**
	- 图像序列建模
	- 纯视觉的方法
```

##### 2.2.3 Cost Analysis 

Due to the alignment of ViT’s architecture with BERT, its resource consumption is also similar. However, unlike language models such as BERT, visual models typically have fixed-length inputs. For standard image inputs, such as 14×14 patches or 16×16 patches, the computational bottleneck lies in the fully connected layers in the FFN and attention. Please refer to §2.1 for more details.

由于ViT的架构与BERT的一致性，其资源消耗也相似。然而，与BERT等语言模型不同，视觉模型通常具有固定长度的输入。对于标准图像输入，如14×14块或16×16块，计算瓶颈在于FFN中的全连接层和注意力。更多详情请参考§2.1。

单词
1. the computational bottleneck 计算瓶颈
2. lies in 在于
3. 前馈神经网络(Feed-Forward Neural Network)
```ad-note
- **VIT和Bert架构相同**
- **区别**：
	- 视觉模型需要固定输入的长度  fixed-length inputs
	- 计算瓶颈在于FFN中的全连接层和注意力
```

#### 2.3 Multimodal Foundation Models

Multimodality is currently a hot research direction in FM research. A large FM often exhibits strong capabilities in cross-modal understanding, translation, and generation.

In general, there are two lines of research on multimodal FMs: one is to encode data in different modalities into the same latent space, mostly adopting transformer encoders; the other one is to generate data in different modalities, often using transformer decoders. Specifically, the multimodal generation mainly centers around text-based image generation, a challenging and realistic ML task that sees great advancements in recent years. The two lines of research have convergence, e.g., multimodal-to-multimodal (or even any-to-any) generation.

多模态是目前基础模型研究的一个热点方向。大型FM通常在跨模态理解、翻译和生成方面表现出很强的能力。

一般来说，关于多模态FM的研究有两条路线：一条是将不同模态的数据编码到同一个潜在空间中，大多采用变换编码器；另一种是以不同的方式生成数据，通常使用变压器解码器。具体来说，多模态生成主要围绕基于文本的图像生成，这是一项具有挑战性和现实性的机器学习任务，近年来取得了巨大的进步。这两条研究路线具有趋同性，例如，多模态到多模态（甚至任何到任何）的生成。

单词
1. exhibit v.展览; 表现，显示，显出(感情、品质或能力); 展出
2. Specifically 具体来说
3. convergence n.汇聚; (不同思想、群体或社会的)趋同，融合



```ad-note
- 大型FM通常在跨模态理解、翻译和生成方面表现出很强的能力。
- **多模态FM的研究有两条路线**
	- 将不同模态的数据编码到同一个潜空间中，大多采用transformer编码器
	- 以不同的模态方式生成数据，通常使用transformer解码器
- 多模态生成主要围绕**基于文本的图像生成**
```

##### 2.3.1 Key Architectures

To ingest and align multimodal input data, existing model architectures typically consist of multiple encoders, with each modality having its own set of transformer encoders. Notably, these encoders are generally trained from scratch, utilizing paired data with the aligned modalities and current modality. Upon receiving input from diverse modalities, they initially encode this data into normalized, fixed-dimensional embeddings. By mapping these embeddings to a high-dimensional space and designing a loss function, researchers aim to minimize the distance between different modalities in the joint semantic space. This approach aligns different modalities and enhances the consistency of their representations. 

为了摄取和对齐多模态输入数据，现有的模型架构通常由多个encoder组成，每个模态都有自己的一组transformer encoder。值得注意的是，这些编码器通常是从头开始训练的，利用具有对齐模态和当前模态的配对数据。在接收到来自不同模态的输入后，他们最初将这些数据编码为归一化的固定维度嵌入。通过将这些嵌入映射到高维空间并设计损失函数，研究人员旨在最小化联合语义空间中不同模态之间的距离。这种方法使不同的模式保持一致，并增强了它们表示的一致性。

单词
1. ingest v.摄取，咽下
2. align v.对其
3. existing model 现有模型
4. notably adv.特别; 尤其是; 尤其; 非常; 极大程度上
5. from scratch 从零开始;从头做起

```ad-note
- 目前大模型架构：
	- 由多个encoder组成，自己模态的transformer encoder，
	- 利用对其的模态和当前模态，从头开始训练
- 将不同模态的输入编码数据为归一化的固定维度的embedding
- 将这些嵌入映射高维空间，并设置损失函数，最小化联合语义空间的不同模态的距离 joint semantic space
- 保持各个模态的一致性
```

With multimodal data aligned, existing research either (i) reuse the LLM that is trained on pure text corpora to generate text; (ii) or diffusion models to generate high-quality image pixels. In the first case, the LLM module is designed to comprehend, reason about, and produce output based on input data aligned with the text modality. This module typically adopts a decoder-only architecture. Due to extensive pretraining on numerous large-scale corpus datasets, LLMs are endowed with rich semantic knowledge. This enables them to effectively comprehend data embedded within the text modality and generate text-based output in an autoregressive fashion when performing specific tasks. In the second case, the diffusion module aims to generate high-quality images by eliminating redundant noise present in the input image. In the training stage, the models introduce noise into an image, transforming it into a random state. In contrast, during the inference stage, this process is reversed, gradually removing the noise. This denoising process is essential for improving image clarity, leading to images with high resolution and detailed sharpness. Stable diffusion models have significantly progressed this technology, showcasing distinctive capabilities in generating high-quality images customized to specific textual and pictorial descriptions. In addition to the multimodal embedding input, the diffusion module primarily consists of two components: an image encoder/decoder and a denoising network. 

在多模态数据对齐的情况下，现有研究要么（i）复用在纯文本语料库上训练的大语言模型（LLM）来生成文本；要么（ii）使用扩散模型来生成高质量的图像像素。在第一种情况下，LLM 模块旨在基于与文本模态对齐的输入数据进行理解、推理并产生输出。该模块通常采用仅解码器架构。由于在众多大规模语料库数据集上进行了广泛的预训练，大语言模型被赋予了丰富的语义知识。这使得它们在执行特定任务时，能够有效地理解嵌入在文本模态中的数据，并以自回归的方式生成基于文本的输出。在第二种情况下，扩散模块旨在通过消除输入图像中存在的冗余噪声来生成高质量图像。在训练阶段，模型会向图像中引入噪声，使其转变为随机状态。相反，在推理阶段，这个过程是相反的，即逐渐去除噪声。这个去噪过程对于提高图像清晰度至关重要，从而生成具有高分辨率和精细锐度的图像。稳定扩散模型显著推进了这项技术，在根据特定文本和图像描述生成高质量定制图像方面展现出独特的能力。除了多模态嵌入输入外，扩散模块主要由两个组件组成：图像编码器 / 解码器和去噪网络。

单词
1. extensive 广阔的，广泛的
2. endowed((向学校等机构)捐钱，捐赠) with 被赋予；赋予具有
3. eliminate redundant noise 消除冗余噪声
4. present adj.存在; 当前的; 出现
5. image clarity 图像清晰度; 影像清晰度
6. resolution n.决议; 分辨率; (问题、分歧等的)解决，消除; 坚决
7. significantly adv.显著地; 明显地; 意味深长地; 别有含义地; 有重大意义地; 
8. image clarity(清楚,清晰) 图像清晰度; 影像清晰度
9. showcasing distinctive(独特的) capabilities 展示了独特的能力
10. specific textual(文本的; 篇章的) and pictorial(图画的; 用图片的) descriptions 特定的文本和图像描述
11. in addition to 除了; 另外

```ad-note
- 多模态数据对其情况下
- **复用在纯文本语料库上训练的大型语言模型（LLM）来生成文本**
	- LLM 模块旨在基于与文本模态对齐的输入数据进行理解、推理并产生输出。该模块通常采用decode only架构
	- 被大规模的语料库数据训练，可以有效地理解嵌入在文本模态中的数据，并以自回归的方式生成基于文本的输出。
-  **利用扩散模型(diffusion models)来生成高质量的图像像素**
	- 扩散模块旨在通过**消除输入图像中存在的冗余噪声**来生成高质量图像。
	- 训练阶段(training stage): 引入噪声，转换为随机状态
	- 推理阶段(inference stage):逐渐去除噪声。这个去噪过程对于提高图像清晰度至关重要,导致图像高分辨率(images with high resolution)精细锐度(detailed sharpness)
	- Stable diffusion models 显著推进了这项技术，在根据特定文本和图像描述生成高质量定制图像方面展现出独特的能力。
	- 除了多模态嵌入输入
	- diffusion module主要组件：图像编码器 / 解码器(image encoder/decoder)和去噪网络(denoising network).
```

**Image Encoder/Decoder.** Diffusion model is the state-of-the-art approach for text-to-image generation. The encoder takes an input image and compresses it into a lower-dimensional latent representation. This compression is vital for reducing the computational load and enhancing the model’s efficiency. The decoder operates in reverse, taking the latent representation and reconstructing it back into a high-resolution image. This process is critical for the model’s capability to generate detailed visual content. Variational autoencoder (VAE) [189] is a generative model that is employed to learn the latent space of the image. VAE consists of an encoder and a decoder: the encoder is responsible for mapping the image to the latent space, while the decoder is responsible for mapping the latent space to the image space. Both encoder and decoder networks are often built with convolutional neural layers. VAE is trained by min- imizing the reconstruction loss and the KL divergence loss. The reconstruction loss is responsible for ensuring that the image generated by the decoder is similar to the original image, while the KL divergence loss is responsible for ensuring that the latent space is similar to the standard normal distribution. VAE is employed in the diffusion mode to learn the latent space of the image. Another variant of VAE model that is often employed in diffusion tasks is VQ-VAE [387], to learn the latent space of the image through a vector quantization layer. The vector quantization layer applies the vector quantization technique, quantizing each pixel of the image to the nearest codebook vector. In this way, the VQ-VAE model can encode and decode the image in a more efficient way.

 **图像编码器/解码器** 
 扩散模型是目前最先进的文本转图像生成方法。编码器将输入图像进行压缩，得到低维的潜在表示。这种压缩对于减少计算负载和提高模型效率至关重要。解码器则进行反向操作，将潜在表示重新构建为高分辨率图像。这一过程对于模型生成详细视觉内容的能力至关重要。变分自编码器（VAE）[189]是一种用于学习图像潜在空间的生成模型。VAE由编码器和解码器组成：编码器负责将图像映射到潜在空间，而解码器负责将潜在空间映射回图像空间。编码器和解码器网络通常都由卷积神经层构建。VAE通过最小化重建损失和KL散度损失来进行训练。重建损失确保解码器生成的图像与原始图像相似，而KL散度损失确保潜在空间与标准正态分布相似。在扩散模型中，VAE被用于学习图像的潜在空间。在扩散任务中经常使用的另一种VAE模型变体是VQ - VAE [387]，它通过向量量化层来学习图像的潜在空间。向量量化层应用向量量化技术，将图像的每个像素量化为最接近的码本向量。通过这种方式，VQ - VAE模型可以更高效地对图像进行编码和解码。

单词
1. compresses v.压缩(文件等); (被)压紧; 
2. latent 潜在的
3. is vital(至关重要的，必不可少的) for 对..至关重要的
4. quantization 量化，分层

```ad-note
- **Image Encoder/Decoder.**
- **扩散模型(Diffusion model)**是目前最先进的文本转图像生成方法。
	- 编码器将输入图像进行压缩，得到低维的潜在表示.
	- decoder反向操作，获取潜在的表示并将其重建回高分辨率图像。
		- 对模型生成详细视觉内容的能力至关重要
- **变分自编码器（VAE）**是一种用于学习图像潜空间的生成模型,在扩散模型中.
- 组成：
	- encoder负责将图像映射到潜空间
	- decoder负责将潜在空间映射回图像空间
		- 编码器和解码器网络通常都由卷积神经层构建。
- 训练方法：最小化重建损失(reconstruction loss)和KL发散损失(KL divergence loss)
	- **重建损失**确保解码器生成的图像与原始图像相似，而**KL散度损失**确保潜在空间与标准正态分布相似
- 变体：**VQ-VAE**
	- 通过向量量化层来学习图像的潜在空间。
	- **向量量化层**:应用向量量化技术，将图像的每个像素量化为最接近的码本向量。
	- **VQ-VAE模型**可以更高效地对图像进行编码和解码
```

**Denoising Network.** Denoising network progressively removes noise from the encoded images by predicting the noise distribution and evicting it through sampling algorithms, such as DDPM [144] and DDIM [356]. Initially, during the training phase, the model adds noise to the images, gradually leading them to a state of pure randomness. The denoising network then learns to reverse this noise addition, step by step, during the inference phase. This gradual denoising is crucial for enhancing the clarity and quality of the final image output. U-Net [337] is often employed as the noise prediction network in diffusion models, which is a convolutional neural network model, consisting of a contracting path and an expansive path. The contracting path is responsible for capturing context information in the image by mapping the image to high-dimensional space, while the expansive path facilitates precise localization by upsampling. To retain the information on the contracting path, the expansive path is connected to the contracting path via skip connections. The U-Net model is a popular choice for image segmentation tasks and is also employed in the diffusion model to predict noise within an image.

 去噪网络 
 去噪网络通过预测噪声分布，并借助如DDPM [144]和DDIM [356]等采样算法去除噪声，逐步从编码图像中去除噪声。在训练阶段初期，模型会向图像中添加噪声，使图像逐渐达到纯随机状态。然后，去噪网络在推理阶段逐步学习逆转这种噪声添加过程。这种渐进式去噪对于提高最终图像输出的清晰度和质量至关重要。U - Net [337]通常在扩散模型中用作噪声预测网络，它是一种卷积神经网络模型，由收缩路径和扩展路径组成。收缩路径负责通过将图像映射到高维空间来捕获图像中的上下文信息，而扩展路径通过上采样有助于精确的定位。为了保留收缩路径上的信息，扩展路径通过跳跃连接与收缩路径相连。U - Net模型是图像分割任务的常用选择，也用于扩散模型中预测图像内的噪声。

单词
1. progressively adv.逐步地；愈益；
2. evict v.驱逐；逐出
3. crucial adj.关键的; 至关重要的; 
4. context information in the image 图像上下文信息
5. facilitates v.促进; 使便利; 促使
6. retain v.保持；保留

```ad-note
- **去噪网络(Denoising Network)**
	- 通过预测噪声分布，并借助如DDPM [144]和DDIM [356]等采样算法去除噪声，逐步从编码图像中去除噪声。
	- 训练阶段初期，模型会向图像中添加噪声，使图像逐渐达到纯随机状态。
	- 推理阶段逐步学习逆转这种噪声添加过程。这种渐进式去噪对于提高最终图像输出的清晰度和质量至关重要。
- **U-Net**
- 通常在扩散模型中用作噪声预测网络，它是一种**卷积神经网络模型**，由收缩路径(contracting path)和扩展路径(expansive path)组成。
	- contracting path 负责通过将图像映射到高维空间来捕获图像中的上下文信息
	- expansive path 通过上采样(upsampling)有助于精确的定位。
		- 上采样 [[图像语义分割中的上采样(Upsampling)和下采样(subsampling)]]
	- 保留收缩路径上的信息，扩展路径通过跳跃连接(skip connections)与收缩路径相连
- U-Net模型是**图像分割任务的流行选择**，也用于**扩散模型来预测图像中的噪声**。
```

**Fusion decoder (FD).** Moreover, FD module aims to enhance the understanding of images based on both the image itself and associated image prompts, and it produces outputs according to task requirements. This module typically involves designing a fusion decoder and is pre-trained on both image and text datasets, enabling it to jointly process image and text representations. This module typically encompasses the design of a fusion decoder and undergoes pre- training on both image and text datasets. This module enables it to collectively process image and text representations.

 融合解码器（FD） 
 此外，融合解码器模块旨在基于图像本身和相关图像提示来增强对图像的理解，并根据任务要求生成输出。该模块通常涉及设计一个融合解码器，并在图像和文本数据集上进行预训练，使其能够联合处理图像和文本表示。该模块通常涵盖融合解码器的设计，并在图像和文本数据集上进行预训练。此模块使其能够共同处理图像和文本表示。


单词
1. fusion n.融合; 结合; 
2. jointly adv. 连带地
3. encompasses v.包含，包括，涉及(大量事物)

```ad-note
- **融合解码器（Fusion decoder——FD）** 
- FD模块(Module)旨在基于图像本身和相关图像提示来增强对图像的理解，并根据任务要求生成输出。
- FD模块(Module)
	- 包含设计融合解码器(Fusion decoder)
	- 并在图像和文本数据集上进行预训练，使其能够联合处理图像和文本表示。
```

#### 2.3.2 Representative Models and Downstream Tasks

**Multi-Encoder FMs**: CLIP [321], ALBEF [216], and ALIGN [164] are some earliest works to propose cross-modal alignment, aiming to learn richer image representations from text by establishing connections between images and text. While these models initially demonstrated the potential of multimodality, their performance was limited by the capabilities of the image and text encoders, as well as the quantity and quality of image-text pair data. Subsequent works like ImageBind [123] and LanguageBind [492] further extended modal alignment. These models employ diverse intermediate modalities as the aligned modal, effectively **mapping** representations from various sources **into** the feature space of the intermediate modal, thereby facilitating cross-modal transformations within a joint vector space. However, significant challenges arise in aligning multimodal representations with those of intermediate modalities, primarily attributed to limitations in encoder capabilities. These limitations, in turn, impact the overall performance of the model.

**多编码器基础模型** CLIP [321]、ALBEF [216]和ALIGN [164]是最早提出跨模态对齐的一些作品，旨在通过建立图像与文本之间的联系，从文本中学习更丰富的图像表示。虽然这些模型最初展示了多模态的潜力，但其性能受到图像和文本编码器能力以及图像 - 文本对数据的数量和质量的限制。后续的作品如ImageBind [123]和LanguageBind [492]进一步扩展了模态对齐。这些模型采用不同的中间模态作为对齐模态，有效地将来自各种来源的表示映射到中间模态的特征空间中，从而促进在联合向量空间内的跨模态转换。然而，在将多模态表示与中间模态的表示进行对齐时出现了重大挑战，这主要归因于编码器能力的限制。这些限制反过来又影响了模型的整体性能。

单词

1. Subsequent adj.随后的; 之后的; Subsequently adv.随后，接着
2. diverse(多种多样的; 不同的;) intermediate(中间的; 中级的; 中等的;) modalities 多种中间模态
3. thereby 从而，因此
4. facilitate v.促进; 使便利; 
5.  attributed to 归因于，由于
6. in turn 依次，轮流，反之，反过来;
```ad-note
- **Multi-Encoder FMs**

- CLIP、ALBEF和ALIGN最早提出跨模态对齐的一些作品,建立图文的联系
  - 优点：展示了多模态的潜力
  - 缺点：性能受限于图像和文本编码(capabilities of the image and text encoders)和文图对(image-text pair data)的数量和质量

- 后续 ImageBind 和LanguageBind 扩展了模态对齐
  - 采用不同的中间模态作为对齐模态，将来自各种来源的表示映射到中间模态的特征空间，促进在联合向量空间内的跨模态转换
  - 对齐限制和挑战：编码器能力(encoder capabilities)
  - 反过来影响模型性能

```

**Encoder-Decoder FMs** utilize the embedding module for modality conversion, allowing the transformed modality to be compatible with the generator.

编码器 - 解码器基础模型利用嵌入模块进行模态转换，使转换后的模态与生成器兼容。

(1) **Encoder-Large FMs**: PandaGPT [361] utilizes multiple single-modal encoders to align inputs to an intermediate modality. PandaGPT feeds the intermediate modality into large FMs for generation, followed by further transformation by the decoder of the target modality. Furthermore, BLIP-2 [215] and MiniGPT-4 [494] focus on cross-modal generation of text and images, by designing image modal encoders and using Q-Former to fuse these image modalities with text modalities before feeding them into multimodal large FMs for cross-modal generation. Meanwhile, mPLUG [453] and LLaVA [248] focus on improving the capacity of modality transformation to enhance the availability and reliability of the generated results. MobileVLM V2 [71], a highly efficient vision-language model designed for resource-constrained devices, utilizes a CLIP-based encoder and MobileLLaMA-based decoder to achieve superior performance while maintaining fast inference speeds. Additionally, Flamingo [19] and LLaMA-Adapter [478] explore how to tune multimodal large FMs with lower costs, leading to the generation of higher-quality multimodal outputs. PaLM-E [93] and HuggingGPT [349] focus on Embodied Intelligence by using large FMs as a central component to incorporate Embodied data into multimodal inputs. These models further design agents to decompose tasks and leverage generative capabilities to accomplish complex tasks.

（1）**编码器 - 大型基础模型(Encoder-Large FMs)**:PandaGPT [361]利用多个单模态编码器将输入对齐到一个中间模态。PandaGPT将中间模态输入到大型基础模型中进行生成，然后由目标模态的解码器进行进一步转换。此外，BLIP - 2 [215]和MiniGPT - 4 [494]专注于文本和图像的跨模态生成，通过设计图像模态编码器，并在将它们输入到多模态大型基础模型进行跨模态生成之前，使用Q - Former将这些图像模态与文本模态融合。同时，mPLUG [453]和LLaVA [248]专注于提高模态转换能力，以增强生成结果的可用性和可靠性。MobileVLM V2 [71]是一款为资源受限设备设计的高效视觉 - 语言模型，它利用基于CLIP的编码器和基于MobileLLaMA的解码器，在保持快速推理速度的同时实现卓越性能。此外，Flamingo [19]和LLaMA - Adapter [478]探索如何以更低的成本调整多模态大型基础模型，从而生成更高质量的多模态输出。PaLM - E [93]和HuggingGPT [349]专注于具身智能，将大型基础模型作为核心组件，将具身数据纳入多模态输入中。这些模型进一步设计智能体来分解任务，并利用生成能力完成复杂任务。

单词
1. conversion n. 转换；转化；
2.  be compatible with 与..一致；适合；
3. fuse v. 融合，熔接，结合；
4. the capacity of modality transformation 模态转换能力
5. superior adj.优越的；更好的
6. tune  v. 调整，调节 n.曲子
7. Embodied(体现的；) Intelligence 具身智能
8. incorporate(使并入；包含,吸收) into 并入；使…成为…的一部分
9. decompose v.分解
10. leverage n.影响力 v.利用

```ad-note
- **编码器 - 大型基础模型(Encoder-Large FMs)**
- **PandaGPT**
	- 将中间模态输入到大型基础模型中,再由目标模态的解码器进行进一步转换。
- **BLIP-2 和MiniGPT** 
	- 专注于文本和图像的跨模态生成
	- 使用Q - Former将这些图像模态与文本模态融合，再丢到多模态大型基础模型(multimodal large FMs)
- **mPLUG 和LLaVA** 
	- 专注于提高模态转换能力
- **MobileVLM V2**
	- 为资源受限设备设计的高效视觉-语言模型
	- 结构：基于CLIP的编码器和基于MobileLLaMA的解码器
	- 保持快速推理速度也有很好的性能
- **Flamingo and LLaMA-Adapter**
	- 探索如何以更低的成本调整多模态大型基础模型，从而生成更高质量的多模态输出
- **PaLM-E and HuggingGPT**
	- 专注于具身智能
	- large FMs 是中心组件(central component)
```


(2) **Encoder-Diffusion FMs:** Stable diffusion [336] is capable of generating high-quality images, by gradually evicting noise in images through a learned process, leading to clear and detailed visual outputs. This model is applied in various downstream tasks such as generating detailed images from text descriptions (text-to-image generation), restoring or completing parts of images (image inpainting), modifying specific aspects of existing images (image editing), and enhancing the resolution of images (image super-resolution). Stable diffusion’s adaptability in these areas makes it a valuable tool in the field of image processing and generation. Consistency models [358] are developed to enhance the efficiency of diffusion models in generating high-quality images, audio, and video. These models facilitate rapid single-step generation, overcoming the slow sampling speed associated with traditional diffusion models. They exhibit the capability to perform zero-shot data editing tasks such as image inpainting, colorization, and super-resolution, without necessitating specific training for these tasks. DALL-E [327] is primarily employed for image generation, demonstrating the capability to create diverse and complex images based on textual descriptions. This model integrates elements of natural language understanding and computer vision, allowing it to produce images that faithfully represent a broad spectrum of textual prompts, ranging from simple descriptions to intricate scenarios.

（2）编码器 - 扩散基础模型(**Encoder-Diffusion FMs**)：稳定扩散模型[336]能够通过学习到的过程逐步去除图像中的噪声，从而生成高质量的图像，产生清晰而详细的视觉输出。该模型应用于各种下游任务，例如根据文本描述生成详细图像（文本到图像生成）、恢复或补全图像的部分内容（图像修复）、修改现有图像的特定方面（图像编辑）以及提高图像分辨率（图像超分辨率）。稳定扩散模型在这些领域的适应性使其成为图像处理和生成领域的一个有价值的工具。一致性模型[358]旨在提高扩散模型生成高质量图像、音频和视频的效率。这些模型有助于实现快速的单步生成，克服了传统扩散模型采样速度慢的问题。它们具备执行零样本数据编辑任务的能力，如图像修复、上色和超分辨率，而无需针对这些任务进行特定训练。DALL - E [327]主要用于图像生成，展示了根据文本描述创建多样化和复杂图像的能力。该模型融合了自然语言理解和计算机视觉的元素，使其能够生成忠实地呈现从简单描述到复杂场景等广泛文本提示的图像。

单词
1. evict v.驱逐；逐出
2. restore v.恢复(某种情况或感受)；修复
3. image inpainting 图像修复
4. aspect n.方面；层面
5. resolution n.决议; 分辨率;
6. exhibit v.展览; 表现，显示，显出(感情、品质或能力); 展出
7. zero-shot data 零样本数据
8. integrate v. (使)合并，成为一体
9. faithfully adv. 忠实地；准确地
10. broad spectrum(谱；光谱；频谱；范围；) 广泛的范围 
11. intricate(错综复杂的) scenarios(方案；设想；) 复杂的场景

In addition to stable diffusion, there is a notable emphasis on “any-to-any” generative models, designed to transform diverse types of inputs into a wide array of outputs. CoDi [372] is designed to produce various output modalities, such as language, image, video, or audio, from different input combinations. Its uniqueness lies in the capability to concurrently generate multiple modalities, without being constrained to specific input types. CoDi aligns modalities in input and output spaces, thereby facilitating the generation of combinations not present in training data. NExTGPT [419] exhibits the capability to perceive inputs and generate outputs across various modalities, including text, images, videos, and audio. NExT-GPT integrates large FMs with multimodal adaptors and diffusion models. The system undergoes fine-tuning with minimal parameter changes, facilitating cost-effective training and straightforward modality expansion. Employing modality-switching instruction tuning and leveraging a specially curated dataset, NExT-GPT enhances cross-modal content generation and understanding, with the objective of modeling universal modalities. Similarly, M4 [462] introduces a scalable mobile AI foundation model that unifies diverse AI tasks by employing multimodal embeddings and a transformer-based backbone for understanding and reasoning across inputs such as text, images, audio, and motion data, enhancing efficiency and scalability for mobile AI.

除了稳定扩散模型外，对“任意到任意”生成模型也有显著的关注，这类模型旨在将不同类型的输入转换为各种各样的输出。CoDi [372]旨在从不同的输入组合中生成各种输出模态，如语言、图像、视频或音频。其独特之处在于能够同时生成多种模态，而不受特定输入类型的限制。CoDi在输入和输出空间中对齐模态，从而便于生成训练数据中未出现的组合。NExTGPT [419]展示了跨多种模态感知输入并生成输出的能力，包括文本、图像、视频和音频。NExT - GPT将大型基础模型与多模态适配器和扩散模型相结合。该系统通过极少的参数变化进行微调，便于进行成本效益高的训练和直接的模态扩展。通过采用模态切换指令调整并利用精心策划的数据集，NExT - GPT增强了跨模态内容的生成和理解，目标是对通用模态进行建模。同样，M4 [462]引入了一种可扩展的移动人工智能基础模型，该模型通过采用多模态嵌入和基于变压器的骨干网络来统一各种人工智能任务，以便对文本、图像、音频和运动数据等输入进行理解和推理，提高了移动人工智能的效率和可扩展性。

单词
1. In addition to 除了；另外，
2. notable adj.值得注意的；显著的；
3. emphasis on 对……的强调；注重
4. uniqueness 唯一性；独特性；
5.  lies in 在于
6. concurrently adv.同时
7. facilitate v.促进; 方便
8. undergo 经历，经受
9. straightforward  adj.直截了当的；简单的；坦率的；
10. curated adj.精心策划的
11. scalable adj.可扩展的
12. reasoning n.v. 推理；理解；思考
13. motion n.运动；动议；移动；

```ad-note
- **Encoder-Diffusion FMs** 编码器 - 扩散基础模型
- **Stable diffusion **
	- 通过学习到的过程逐步(evict)去除图像中的噪声，从而生成高质量的图像
	- 用处：各种各样的下游任务
		- 文生图(text-to-image generation)、图像修复(image inpainting)、图像编辑(image editing)、提高图像分辨率(image super-resolution)
	- 它在上面领域的适应性使其成为**图像处理和生成领域**的一个有价值的工具
- **Consistency models**
	- 旨在提高扩散模型生成**高质量图像、音频和视频的效率**
	- 优点：实现快速的单步生成。克服传统模型采样慢的问题
		- 具备执行零样本数据编辑任务，图像修复，分辨率等，无需单独训练
- **DALL-E **
	- 主要用于图像生成。具有根据文本描述创建多样化和复杂图像的能力
	- 融合(integrate)自然语言理解和计算视觉技术
- 另外的模型 
- **"any-to-any” generative models**
	- 将不同类型的输入转换为各种各样的输出
- **CoDi**
	- 从不同的输入组合中生成各种输出模态，如语言、图像、视频或音频
	- 独特之处：能够同时生成多种模态，不管输入是什么
	- 在输入和输出空间中对齐模态，可以生成没有出现过的结果。
- **NExTGPT **
	- 将large FMs 与多模态适配器(multimodal adaptors) 和扩散模型 diffusion models结合
	- 系统通过微调(fine-tuning)少量的参数，方便成本效益高的训练(cost-effective training)和直接的模态扩展(straightforward modality expansion)
	- 通过采用模态切换指令调整并利用精心策划的数据集,对生成和理解进行增强
- **M4**
	- 可扩展的移动人工智能基础模型(scalable mobile AI foundation model)
	- 架构：采用多模态嵌入(multimodal embeddings) 和  transformer-based backbone骨架
	- 提高了移动AI的效率和可扩展性
```

(3) **Encoder-FD FMs**: UNITER [64] is one of the earliest works to propose a universal fusion of image and text in multimodal settings. It aims to combine image and text features through Transformers to obtain joint features. Building upon this, subsequent works such as FLAVA [354], CoCa [459], and GLIP [219] delve deeper into how to use decoders to better fuse and align image and text representations, thereby enhancing multimodal reasoning. Additionally, SAM [190] and SAM 2 [331] take a step further by leveraging a decoder to fuse prompt embeddings corresponding to images, enabling zero-shot automatic segmentation of images/videos based solely on text prompts. 

（3）**编码器 - 融合解码器基础模型**：UNITER [64]是最早在多模态环境中提出图像和文本通用融合的作品之一。其目标是通过变换器将图像和文本特征相结合，以获得联合特征。在此基础上，后续的作品如FLAVA [354]、CoCa [459]和GLIP [219]更深入地研究了如何使用解码器更好地融合和对齐图像与文本表示，从而增强多模态推理能力。 此外，SAM [190]和SAM 2 [331]更进一步，利用解码器融合与图像相对应的提示嵌入，从而仅基于文本提示就能实现图像/视频的零样本自动分割。

单词
1. fusion n.融合; 结合; 
2. settings n.环境，背景
3. joint adj.联合的; 共同的
4. delve v.翻找
	- delve .. into / into 钻研；探索
5. additionally adv 另外；此外；进一步；
6. prompt n. 提示符 v.鼓励，提示 adj. 迅速的；及时的；
7.  corresponding(相应的；相关的) to 相应的；相应于；
8. solely adv. 仅；只；

```ad-note
- **编码器 - 融合解码器(Encoder-FD) FMs**
- **UNITER**
	- 最早在多模态环境中提出**图像和文本通用融合**的作品之一
	- 目标是: 通过transformer将text和image特征结合得到联合特征(joint feature)
- 基于UNITER的**FLAVA, CoCa, and GLIP** 
	- 研究：如何使用**解码器更好地融合和对齐图像与文本表示**，提高multimodal reasoning
-  **SAM and SAM 2**
	- 利用解码器融合与图像相对应的提示嵌入,从而仅基于文本提示就能实现图像/视频的零样本自动分割。
```

![image.png|471](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241226214854.png)
#### 2.3.3 Cost Analysis

**Multi-Encoder module**: Multi-encoder module is a specialized encoder module designed for modality alignment. This module attuned to various modal inputs, effectively aligns these inputs into a unified semantic space using distinct encoder architectures. Specifically, the primary encoder modules consist of the Image-encoder, Text-encoder, Audioencoder, and IMU-encoder. As shown in Figure 6(a), the encoder module has 0.27B parameters (on average), occupies 1.1G memory (on average), with total GFLOPs of 65.9 for processing a sample (on average). Notably, the Imageencoder emerges as the most resource-intensive component, with 0.63B parameters, occupying 2.4G memory, and executing 167.59 GFLOPs for a single sample.

 多编码器模块**Multi-Encoder module**
  多编码器模块是一种专门用于模态对齐的编码器模块。该模块能够适应各种模态输入，通过不同的编码器架构有效地将这些输入对齐到一个统一的语义空间中。具体而言，主要的编码器模块包括图像编码器、文本编码器、音频编码器和惯性测量单元（IMU）编码器。如图6（a）所示，编码器模块平均有2.7亿个参数，平均占用1.1GB内存，处理一个样本的总浮点运算次数（GFLOPs）平均为65.9。值得注意的是，图像编码器是资源消耗最大的组件，有6.3亿个参数，占用2.4GB内存，处理单个样本时执行167.59 GFLOPs。

单词
1. specialized adj.专门的; 专用的; 
2. attuned v.使协调；适应，熟悉
3.  Specifically 具体来说

```ad-note
- **Multi-Encoder module**
	- 一种专门用于模态对齐的编码器模块
	- **通过不同的编码器架构有效地将这些输入对齐到一个统一的语义空间**
	- 主要结构：图像encoder、文本encoder、音频encoder和惯性测量单元（IMU）encoder
	- 编码器模块平均有2.7亿个参数，平均占用1.1GB内存
	- 图像编码器是资源消耗最大的组件
```

**Decoder module**: In multimodal models, in addition to the multi-encoder module, there is also a decoder module composed of large FMs, diffusion module, and FD module.

(1) **Large FMs module:** This module receives inputs aligned from diverse modalities, enabling autoregressive generation. The cost of the module mainly depends on the size of the large FM parameters. As shown in Figure 6(b), taking an example of the integrated Vicuna-7B, this model consists of 7B parameters, occupies 14G memory, and with total GFLOPs of 312 for processing a sample, significantly exceeding the resource requirements of the encoder module.

 **解码器模块** 
 在多模态模型中，除了多编码器模块外，还有一个由大型基础模型、扩散模块和融合解码器（FD）模块组成的解码器模块。 
 （1）**大型基础模型模块**：该模块接收来自不同模态对齐后的输入，实现自回归生成。模块的成本主要取决于大型基础模型参数的规模。如图6（b）所示，以集成的Vicuna - 7B为例，该模型包含70亿个参数，占用14GB内存，处理一个样本的总浮点运算次数为312，远远超过编码器模块的资源需求。 

单词
1. exceed  v.超过(数量); 超越(法律、命令等)的限制
2. significantly 显著地; 明显地


**FLOPS**:  
注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个**衡量硬件性能的指标**。

**GFLOPs**
这其实是一个单位，1GLOPs=10亿次浮点运算。  
是Paper里比较流行的单位。

**浮点运算量和参数量的区别**
[浮点运算量](https://zhida.zhihu.com/search?content_id=120340464&content_type=Article&match_order=2&q=%E6%B5%AE%E7%82%B9%E8%BF%90%E7%AE%97%E9%87%8F&zhida_source=entity)是实际运算过程中的加减乘除计算过程中的计算次数，描述计算力；  
参数量只是指的模型大小，和输入的图片大小无关，描述需要内存

```ad-note
- 除了多编码器模块外,还有**Decoder module**—— large FMs, diffusion module, and FD module
	- 接收来自不同模态对齐后的输入，实现自回归生成
	- 模块成本：large FM的参数量
```

(2) **Diffusion module:** This module receives optional conditioning inputs and generates high-quality images. Given the variable sizes of these modules in diffusion models, we focus on Stable Diffusion 2.1 as a representative example for this discussion. Stable Diffusion 2.1 incorporates a U-Net for denoising, a VAE for image encoding and decoding, and a CLIP model as the text encoder. Figure 7(a) and (e) show the FLOPs and parameter number percentage in the stable diffusion 2.1 model. We use the textual prompt consisting of 10 tokens for illustration. Figure 7(b) and (f) show the FLOPs and parameter number percentage in the VAE model. Figure 7(c) and (g) show the FLOPs and parameter number percentage in the U-Net model. Figure 7(d) and (h) show the FLOPs and parameter number percentage in the CLIP model. U-Net of Stable Diffusion 2.1 takes image latents of shape 4×96×96 as input and predicts noise in the latent space. The model is trained on the LAION-5B dataset [344]. The U-Net has 865M parameters, with total FLOPs of 759G for processing a sample. Stable Diffusion 2.1 incorporates a VAE that encodes images to latent space and decodes them. VAE takes images with resolution 3×768×768 as input and encodes them to 4×96×96. VAE is co-trained with the U-Net on the same dataset. VAE has 83M parameters, with total FLOPs of 4T for processing a sample. Stable Diffusion 2.1 uses CLIP [321] model for text encoding. CLIP is trained on various (image, text) pair datasets, such as the LAION dataset and DataComp dataset [114]. CLIP takes sentences as the input and encodes each token to a hidden vector with a size of 1024. The model has 289M parameters, with a total FLOPs of 289M for processing a single token.

(3) **FD module**: Due to the similar structures between FD and ViT, their resource consumption and computational bottleneck are similar. For more details, please refer to §2.2.

 （2）扩散模块：该模块接收可选的条件输入并生成高质量图像。鉴于扩散模型中这些模块的大小各不相同，我们以Stable Diffusion 2.1作为本次讨论的代表性示例。Stable Diffusion 2.1包含一个用于去噪的U - Net、一个用于图像编码和解码的VAE以及一个作为文本编码器的CLIP模型。**图7（a）和（e）** 展示了Stable Diffusion 2.1模型中的**浮点运算次数和参数数量**百分比。我们以包含10个词元的文本提示为例进行说明。**图7（b）和（f）** 展示了**VAE模型**中的浮点运算次数和参数数量百分比。**图7（c）和（g）** 展示了**U - Net模型**中的浮点运算次数和参数数量百分比。**图7（d）和（h）** 展示了**CLIP模型**中的浮点运算次数和参数数量百分比。Stable Diffusion 2.1的U - Net以形状为4×96×96的图像潜在表示作为输入，并预测潜在空间中的噪声。该模型在LAION - 5B数据集[344]上进行训练。U - Net有8.65亿个参数，处理一个样本的总浮点运算次数为7590亿次。Stable Diffusion 2.1中的VAE将图像编码到潜在空间并进行解码。VAE以分辨率为3×768×768的图像作为输入，并将其编码为4×96×96。VAE与U - Net在同一数据集上共同训练。VAE有8300万个参数，处理一个样本的总浮点运算次数为4万亿次。Stable Diffusion 2.1使用CLIP [321]模型进行文本编码。CLIP在各种（图像，文本）对数据集上进行训练，如LAION数据集和DataComp数据集[114]。CLIP以句子作为输入，并将每个词元编码为大小为1024的隐藏向量。该模型有2.89亿个参数，处理单个词元的总浮点运算次数为2.89亿次。 
 
 （3）FD模块：由于FD和ViT结构相似，它们的资源消耗和计算瓶颈也相似。更多详细信息，请参考§2.2。

单词
1.  incorporate(使并入；包含,吸收) into 并入；使…成为…的一部分

```ad-note
- (2) **Diffusion module**
	- 接收**可选的条件输入并生成高质量图像**
	- Stable Diffusion 2.1为例
	- 结构：一个用于去噪的U-Net、一个用于图像编码和解码的VAE以及一个作为文本编码器的CLIP模型
```


## 3 RESOURCE-EFFICIENT ARCHITECTURES
 资源高效的架构
![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241226225754.png)
		Figure 8: An overview of resource-efficient architectures.

Model architecture is the core to resource-efficient large FMs, including attention mechanisms, decoders, and their alternatives. The primary objective is to reduce computational and memory expenses. Figure 8 visually illustrates this classification of resource-efficient architecture, considering the standard core blocks and the conventional taxonomy of large FMs. Resource-efficient architecture consists of efficient attention, dynamic neural network, diffusion-specific optimization, and ViT-specific optimization.

模型架构是实现资源高效的大型基础模型的核心，包括注意力机制、解码器及其替代方案。其主要目标是减少计算和内存开销。图8直观地展示了资源高效架构的分类，考虑了标准核心模块和大型基础模型的常规分类法。资源高效架构包括高效注意力机制、动态神经网络、扩散模型特定优化和视觉Transformer特定优化。

### 3.1 Efficient Attention
高效注意力机制

The quadratic time complexity associated with attention architectures, particularly concerning sequence length, presents significant challenges during training and inference. Previous efforts [107, 244, 185, 90, 390] has explored methods to reduce this complexity to linear or identify viable alternatives. The diverse approaches for achieving efficient attention are visually summarized in Figure 9.

与注意力架构相关的二次时间复杂度，特别是在序列长度方面，在训练和推理过程中带来了重大挑战。先前的研究[107, 244, 185, 90, 390]已经探索了将这种复杂度降低到线性或寻找可行替代方案的方法。实现高效注意力的各种方法在图9中进行了直观总结。

#### 3.1.1 Sparse Attention

Motivated by graph sparsification, sparse attention [36, 18, 464, 153, 96, 206, 217] aims to build a sparse attention matrix. This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products. For instance, Longformer [107], ETC [244], and BIGBIRD [464] decompose conventional attention into local windowed attention and task-specific global attention, effectively reducing self-attention complexity to linear. HEPOS [153] introduces head-wise positional strides. This innovation allows each attention head to concentrate on a specific subset of the input sequence, facilitating efficient encoder-decoder handling. MATE [96] transforms attention into a multi-view format, efficiently addressing either rows or columns in a table. TDANet [217] emulates the human brain’s top-down attention mechanism to selectively focus on the most relevant information, thereby enhancing speech separation efficiency. ALBERT [202] implements parameter sharing across layers, resulting in an 89% reduction in parameter count while guaranteeing accuracy, compared to traditional BERT. 

受图稀疏化的启发，稀疏注意力机制[36, 18, 464, 153, 96, 206, 217]旨在构建一个稀疏的注意力矩阵。这种方法旨在保留完全二次自注意力机制在经验方面的优势，同时减少内积运算的数量。例如，Longformer [107]、ETC [244]和BIGBIRD [464]将传统注意力分解为局部窗口注意力和特定任务的全局注意力，有效地将自注意力复杂度降低至线性。HEPOS [153]引入了按头的位置步长。这一创新使得每个注意力头能够专注于输入序列的特定子集，有助于高效地进行编码器 - 解码器处理。MATE [96]将注意力转换为多视图格式，能高效地处理表格中的行或列。TDANet [217]模拟人脑自上而下的注意力机制，有选择性地聚焦于最相关的信息，从而提高语音分离效率。ALBERT [202]实现了跨层的参数共享，与传统的BERT相比，在保证准确率的同时，参数数量减少了89%。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241230204315.png)
表4：变换器及其变体之间复杂度的时间和空间对比分析，其中T代表序列长度，d代表隐藏维度。

#### 3.1.2 Approximate Attention
近似注意力机制

Approximate attention, as explored in numerous works [191, 179, 397, 69, 271, 272, 174, 55, 423], involves low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention mechanism. These approaches avoid the direct computation of the $N ×N$ matrix, aiming to reduce computational complexity and enhance efficiency, particularly in scenarios with long sequence lengths. Linformer [397] demonstrates that the effective decomposition of the attention matrix into a low-rank matrix. This technique involves projecting the length dimensions of keys and values into a lower-dimensional space, resulting in a significant reduction in memory complexity. Reformer [191] utilizes locality-sensitive hashing to replace the conventional dot-product attention. Katharopoulos et al. [179] introduced a kernel-based approach to self-attention, leveraging the associative property of matrix multiplication for computing self-attention weights. Polysketchforme [174] employs polynomial functions and sketching techniques to approximate softmax attention outputs, providing a novel perspective on attention mechanism approximation. Mega [272], featuring a single-head gated attention mechanism, incorporates exponential moving average. This addition effectively integrates inductive bias related to position-aware local dependencies into the inherently positionagnostic attention mechanism. Deformable Attention [423] proposes a data-aware, deformable attention mechanism. , contributing to improved performance within the ViT architecture, This method proves especially beneficial when contrasted with traditional dense attention methods. CrossViT [55] introduces linear cross-attention, empowering the ViT architecture to to efficiently handle variably-sized input tokens while mitigating computational costs.

众多研究[191, 179, 397, 69, 271, 272, 174, 55, 423]所探索的近似注意力机制涉及对自注意力矩阵进行低秩近似以及对自注意力机制进行创新性的重新构建。这些方法避免直接计算N×N矩阵，旨在降低计算复杂度并提高效率，尤其是在处理长序列长度的场景中。 Linformer[397]展示了将注意力矩阵有效分解为低秩矩阵的方法。该技术涉及将键和值的长度维度投影到一个更低维度的空间中，从而使内存复杂度显著降低。 Reformer[191]利用局部敏感哈希来取代传统的点积注意力机制。Katharopoulos等人[179]引入了一种基于核的自注意力方法，利用矩阵乘法的结合律来计算自注意力权重。 Polysketchforme[174]运用多项式函数和草图绘制技术来近似softmax注意力输出，为注意力机制的近似提供了一种新视角。 Mega[272]具备单头门控注意力机制，并融入了指数移动平均。这一添加操作有效地将与位置感知局部依赖相关的归纳偏置整合到了原本与位置无关的注意力机制中。 可变形注意力机制（Deformable Attention）[423]提出了一种数据感知的可变形注意力机制，有助于提升视觉Transformer（ViT）架构内的性能。与传统的密集注意力方法相比，该方法被证明尤其有益。 CrossViT[55]引入了线性交叉注意力，使视觉Transformer架构能够高效地处理大小各异的输入词元，同时降低计算成本。

单词
1. Approximate adj.近似的; 大约的; 接近的
2. Approximation n. 近似；逼近
3. scenarios n.方案；设想；情形
4. effective adj.有效的；生效的；实际的
5. significant adj. 重要的, 有重大意义的；显著的, 值得注意的
6.  associative property  结合律；结合性
7. employs v.应用，使用
8. polynomial adj. 多项式的；
9. feature v. 以…为特色；以…为主要组成
10.  incorporates exponential moving average
		- incorporates v.使并入；包含
		- exponential adj. 指数的；
11. integrate v. (使)合并，成为一体；
12. inductive adj. 归纳的；归纳法的；
13. position-aware 位置感知
14. data-aware 数据感知
15. inherently adv.固有的
16. positionagnostic 位置无关
17. deformable adj.可形变的，应变的
18. contributing to 贡献，有助于
19. contrasted v.对比；对照
20. dense adj. 密集的；稠密的；
21. empower v.使,授权；给(某人)…的权力
22. variably adv.可变地
23. mitigate vt.减轻；缓和

```ad-note
- 近似注意力机制**Approximate Attention**
- 对自注意力矩阵进行低秩近似以及对自注意力机制进行创新性的重新构建
- **Linformer**
	- 将注意力矩阵有效**分解为低秩矩阵**的方法
	- 将K,V维度映射到低维
- **Reformer**
	- 局部敏感哈希(locality-sensitive hashing)来取代传统的点积注意力机制
- **Katharopoulos等人**
	- 基于核的自注意力方法，利用矩阵乘法的结合律来计算自注意力权重
- **Polysketchforme**
	- 运用多项式函数和草图绘制(sketching techniques)技术来近似softmax注意力输出
- **Mega**
	- 引入单头门控(single-head gated)注意力机制，并融入了指数移动平均(exponential moving average)
	- 有效地将与位置感知局部依赖相关的归纳偏置整合到了原本与位置无关的注意力机制中
- **Deformable Attention**
	- 提出了一种数据感知的可变形注意力机制，有助于提升视觉Transformer（ViT）架构内的性能
- **CrossViT**
	- 引入了线性交叉注意力，使视觉Transformer架构能够高效地处理大小各异的输入词元
```

#### 3.1.3 Attention-Free Approaches
无注意力方法

Despite the dominance of attention-based transformer architectures in large FMs, several works [467, 161, 313, 43, 42, 128, 77, 298, 309, 366] have put forth innovative architectures that hold the potential to replace the traditional transformer model. For instance, Hyena [313] introduces an architecture that interleaves implicitly parametrized long convolutions with data-controlled gating. This design provides a subquadratic alternative to attention in large-scale language models, thereby enhancing efficiency in processing long sequences. Another notable trend is the substitution of the attention mechanism with state space models (SSMs), as explored in [128, 77, 298]. Mamba [127] seamlessly integrates selective SSMs into a streamlined neural network architecture, eliminating attention and MLP blocks. This model achieves a notable 5× speed increase over traditional transformers and exhibits linear scaling with sequence length. Gu et al. [119] also offered a comprehensive restructuring of the SSM literature into an informative and cohesive format. Recurrent-Style Transformers (RMT) [43, 42] adopts an recurrent neural network-based architecture, replacing attention with a RNN to achieve linear complexity. RWKV [309] combines the efficient parallelizable training of Transformers with the effective inference capabilities of RNNs. RetNet [366] troduces an architecture that replaces multi-head attention with a multi-scale retention mechanism. This architecture captures and retains information from prior sequence steps, utilizing varying gamma values across heads to regulate retention strength. RetNet not only maintains a constant inference cost regardless of sequence length but also outperforms Transformer models with key-value caches in efficiency. Furthermore, during training, RetNet demonstrates a 25-50% memory saving and a 7× acceleration compared to the standard Transformer.

尽管基于注意力的Transformer架构在大型基础模型中占据主导地位，但多项研究[467, 161, 313, 43, 42, 128, 77, 298, 309, 366]已经提出了一些具有潜力取代传统Transformer模型的创新架构。 例如，Hyena [313]引入了一种架构，该架构将隐式参数化的长卷积与数据控制的门控机制相互交织。这种设计为大规模语言模型中的注意力机制提供了一种次二次（subquadratic）替代方案，从而提高了处理长序列的效率。 另一个值得注意的趋势是用状态空间模型（SSMs）来替代注意力机制，正如[128, 77, 298]中所探索的那样。Mamba [127]将选择性状态空间模型无缝集成到一个精简的神经网络架构中，摒弃了注意力机制和多层感知机（MLP）模块。该模型相较于传统的Transformer实现了显著的5倍速度提升，并且其计算复杂度随序列长度呈线性扩展。顾等人（Gu et al.）[119]还对状态空间模型相关文献进行了全面梳理，使其呈现出内容丰富且条理清晰的形式。 循环式Transformer（Recurrent - Style Transformers，RMT）[43, 42]采用了一种基于循环神经网络（RNN）的架构，用RNN替代注意力机制以实现线性复杂度。RWKV [309]将Transformer的高效并行可训练性与RNN的有效推理能力相结合。 RetNet [366]引入了一种架构，它用多尺度保留机制取代了多头注意力机制。这种架构能够捕捉并保留先前序列步骤中的信息，通过在不同头之间使用不同的伽马（gamma）值来调节信息保留强度。RetNet不仅无论序列长度如何都能保持恒定的推理成本，而且在效率方面优于带有键值缓存的Transformer模型。此外，在训练过程中，与标准Transformer相比，RetNet能节省25% - 50%的内存，并实现7倍的加速。

单词
1. the dominance(支配地位; 支配;) of 占据主导地位
2. put forth(提出；发表；公布) innovative architectures  提出创新的架构
3. hold the potential to  有…… 的潜力；具备…… 的可能性
4. interleave vt. (尤指将片状物)插入，夹进
5. implicitly adv.含蓄地; 无保留地; 暗中地
6. parametrized 参数化
7. subquadratic 描述一种计算复杂度，相较于二次复杂度更低，这里结合语境翻译为 “次二次”，表示在复杂度层面是比**常规基于二次关系计算的注意力机制**更优的一种可替代选择。
8. notable adj.值得注意的; 显著的; 重要的
9. substitution n.取代; 置换;
10. seamlessly 无缝的; 无缝
11. streamlined adj.流线型的
12. eliminate v. 消除；排除；消灭
13. informative adj.提供有用信息的; 给予知识的
14. cohesive adj.有结合力的; 使结合的; 使凝结的;
15. comprehensive adj.综合的; 所有的; 综合性的(接收各种资质的学生); 全部的; 
16. comprehensive restructuring 全面重组
17. literature n.文学; (某学科的)文献，
18. scale n.大小，规模; （实物与图表等之间的）比例，比率; (尤指与其他事物相比较时的)范围; 衡量标准，尺度; 
19. retention n.保留; 保持; 
20. varying adj.不同的，易变的
21. regulate v.规定; 调节，控制
22. strength n.力量; 强度; 实力; 
23. outperforms v.(效益上)超过，胜过

```ad-note
- 1）取代传统Transformer模型的创新架构
- **Hyena提出**
	- 该架构将**隐式参数化的长卷积与数据控制的门控机制**相互交织
	- 提供了一种**次二次（subquadratic）**替代方案，提高了效率
- 2）用状态空间模型（SSMs）来替代注意力机制
- **Mamba等**
	- 将选择性状态空间模型无缝集成到一个精简的神经网络架构中，摒弃了注意力机制和多层感知机（MLP）模块
	- 速度加快
- **Gu等**
	- 对状态空间模型SSMs相关文献进行了全面梳理
- 循环式Transformer**Recurrent-Style Transformers (RMT)**
	- 基于循环神经网络（RNN）的架构,用RNN代替注意力机制，实现线性复杂度
- **RWKV**
	- 将Transformer的高效并行可训练性与RNN的有效推理能力相结合
- **RetNet**
	- 用多尺度保留机制(multi-scale retention mechanism)取代了多头注意力机制
	- 能够捕捉并保留先前序列步骤中的信息，通过在不同头之间使用不同的伽马（gamma）值来调节信息保留强度。
	- 保持恒定的推理成本，而且在效率方面优于带有KV缓存的Transformer模型
	- 和 Transformer相比 能节省25% - 50%的内存，并实现7倍的加速
```

### 3.2 Dynamic Neural Network
**动态神经网络**

#### 3.2.1 Mixture-of-Experts
专家混合（模型 / 方法）
“Mixture-of-Experts” 常缩写为 “MoE”，在深度学习等领域是一种比较常用的架构或方法

Mixture-of-Experts (MoE), illustrated in Figure 10(b), represents an efficient and sparse approach for training and deploying large FMs with extensive parameter sets. This model utilizes routed sparse parameters during inference. Switch Transformer [104] introduces a switch routing algorithm, leading to models with improved efficiency and reduced computational and communication costs. Switch Transformer demonstrates the scalability and effectiveness of MoE framework by managing up to one trillion parameters, even with as many as 2,048 experts. GLaM [94], a family of decoder-only language models, leverages a sparsely activated MoE design. This innovative approach substantially reduces training costs while simultaneously increasing model capacity compared to dense models. V-MoE [335] presents a sparse adaptation of the ViT, scaling to 15 billion parameters, and achieves performance matching dense models while requiring less training time. LIMoE [286] represents the first multimodal model to incorporate sparse MoE, significantly outperforming CLIP in various tasks. Mistral AI introduces Mistral6, an MoE model comprising 8 experts, each with 7 billion parameters. This model outperforms the performance of LLaMA2-70B model [383]. MoEfication [481] converts a model into its MoE variant with equivalent parameters. This technique conditionally utilizes portions of feedforward network parameters, maintaining performance levels comparable to the original dense model. Sparse upcycling [193] initializes sparsely activated MoE from dense checkpoints. Sparse upcycling enables models, such as T5 Base, Large, XL, and Vision Transformer Base and Large, to significantly outperform their dense counterparts on benchmarks like SuperGLUE and ImageNet, while using only about 50% of the original dense pretraining costs. FFF [35] divides the feed-forward layer into separate leaves instead of copying the entire feed-forward layer as an expert. FFF is a log-time alternative to feedforward networks, being up to 220× faster than the original feed-forward layer and up to 64× faster than MoE with about 5% accuracy loss. §5.1 will detail systematic optimizations applied to MoE models.

专家混合模型（MoE），如图10（b）所示，是一种针对具有大量参数集的大型基础模型进行训练和部署的高效且稀疏的方法。该模型在推理过程中使用经过路由的稀疏参数。Switch Transformer（开关变换器）[104]引入了一种开关路由算法，使得模型效率得以提升，同时降低了计算和通信成本。Switch Transformer通过管理多达一万亿个参数（甚至配备多达2048个专家），展示了专家混合模型框架的可扩展性和有效性。 GLaM（通用语言模型）[94]是一系列仅含解码器的语言模型，它采用了稀疏激活的专家混合模型设计。与密集模型相比，这种创新方法大幅降低了训练成本，同时还增加了模型的容量。V - MoE（视觉专家混合模型）[335]呈现了视觉Transformer（ViT）的稀疏适配版本，其参数规模可扩展至150亿，并且在所需训练时间更少的情况下，能实现与密集模型相当的性能表现。LIMoE（语言 - 图像专家混合模型）[286]是首个引入稀疏专家混合模型的多模态模型，在多项任务中其性能显著优于CLIP模型。 Mistral AI推出了Mistral6，这是一个包含8个专家且每个专家有70亿个参数的专家混合模型。该模型的性能优于LLaMA2 - 70B模型[383]。MoEfication（专家混合化）[481]能将一个模型转换为与其参数相当的专家混合模型变体。这种技术有条件地利用前馈网络的部分参数，可维持与原始密集模型相当的性能水平。稀疏升级（Sparse upcycling）[193]从密集检查点初始化稀疏激活的专家混合模型。稀疏升级使得诸如T5基础版、大型版、超大版以及视觉Transformer基础版和大型版等模型，在SuperGLUE（超级胶水基准）和ImageNet（图像网）等基准测试中，性能显著优于对应的密集模型，同时仅使用约50%的原始密集模型预训练成本。 FFF（快速前馈层）[35]将前馈层划分为独立的分支，而非将整个前馈层复制作为专家。FFF是前馈网络的一种对数时间替代方案，它比原始前馈层快达220倍，相较于专家混合模型快达64倍，且准确率损失约为5%。第5.1节将详细介绍应用于专家混合模型的系统性优化。

![image.png](https://gitee.com/zhang-junjie123/picture/raw/master/image/20241230221715.png)

单词
1. mixture n.混合物; 混合;
2. illustrated v.(用示例、图画等)说明，解释; 显示
3. extensive adj.广泛的; 大量的; 
4. up to 达到，接近于; 多达
5. substantially adv.基本上; 大体上; 非常;大大地;
6. simultaneously adv.同时; 联立;
7. adaptation n.适应; 改编本;
8. comprise v.包括; 包含; 组成; 
9. conditionally adv.有条件地
10. checkpoints n.(边防)检查站; 边防关卡；检查点
11. counterpart n.对应方，相当的人
12. separate v.分离; (使)分开; 隔开; 分割; 区分; 
            adj.单独的; 分开的; 独立的; 

```ad-note
- **专家混合模型（MoE）**
	- 是一种针对具有大量参数集的大型基础模型进行训练和部署的高效且稀疏的方法
	- 该模型在推理过程中利用路由稀疏参数
- **Switch Transformer**
	- 引入了一种**开关路由算法**，提升性能降低成本
	- 管理多达一万亿个参数（甚至配备多达2048个专家），展示了专家混合模型框架的可扩展性和有效性
- **GLaM(通用语言模型）**
	- decoder-only language models,采用了稀疏激活的专家混合模型设计
- **V-MoE**
	- 呈现了视觉Transformer（ViT）的稀疏适配版本,参数更多，训练时间更少，性能更好
- **LIMoE**（语言 - 图像）MoE
	- 首个引入稀疏moe的多模态模型，在多项任务中其性能显著优于CLIP模型
- **Mistral6**
	- 一个包含8个专家且每个专家有70亿个参数的moe
	- 性能优于LLaMA2 - 70B
- **MoEfication**（专家混合化）
	- 将一个模型转换为与其参数相当的专家混合模型变体
	- 有条件地利用前馈网络的部分参数，性能与原始密集模型相当
- **稀疏升级（Sparse upcycling）**
	- 从密集检查点(checkpoints)初始化稀疏激活的专家混合模型
	- 使得诸如T5基础版、大型版等在SuperGLUE（超级胶水基准）和ImageNet（图像网）等基准测试，优于对应密集型模型
- **FFF（快速前馈层）**
	- 将前馈层划分为独立的分支(leaves)
	- FFF是前馈网络的一种对数时间替代方案,比原始前馈层快达220倍，相较于专家混合模型快达64倍，且准确率损失约为5%。
```
#### 3.2.2 Early Exiting
提前退出（机制）

在深度学习等相关领域，“Early Exiting” 常指一种让模型在训练或推理过程中提前结束某个阶段等的机制


As illustrated in Figure 10(c), early exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints. He et al. [138] investigates modifications to the standard transformer block, aiming for simpler yet efficient architectures without sacrificing performance. This model involves removing elements such as residual connections, layer normalization, and specific parameters in projection and value, along with serializing MLP sub-blocks. M4 [461] introduces a multi-path task execution framework, enabling elastic fine-tuning and execution of foundational model blocks for different training and inference tasks. FREE [27] proposes a shallow-deep module that synchronizes the decoding of the current token with previously processed early-exit tokens. FREE utilizes an adaptive threshold estimator for determining appropriate early-exit confidence levels. SkipDecode [83] is designed for batch inferencing and KV caching, overcoming previous limitations by establishing a unique exit point for each token in a batch at every sequence position. PABEE [491] enhances the efficiency of pre-trained language models by integrating internal classifiers at each layer. The inference process halts when predictions stabilize for a set number of steps, facilitating quicker predictions with reduced layer usage. DeeBERT [428] augments BERT’s inference efficiency by incorporating early exit points. DeeBERT allows instances to terminate at intermediate layers based on confidence levels, effectively reducing computational demands and accelerating inference. Bakhtiarnia et al. [31] proposed 7 distinct architectural designs for early-exit branches suitable for dynamic inference in ViTs backbones. LGViT [431] presents an early-exiting framework tailored for general ViTs, featuring diverse exiting heads, such as local perception and global aggregation heads, to balance efficiency and accuracy. This approach achieves competitive performance with an approximate 1.8× increase in speed.

如图10（c）所示，提前退出优化是一种策略，当模型对预测结果有较高置信度或遇到资源限制时，允许其提前终止计算过程。 何等人（He et al.）[138]研究了对标准Transformer模块的修改，旨在打造更简单且高效的架构，同时又不牺牲性能。该模型涉及移除诸如残差连接、层归一化以及投影和值中的特定参数等元素，并对多层感知机子模块进行序列化处理。 M4 [461]引入了一个多路径任务执行框架，能够针对不同的训练和推理任务实现基础模型模块的弹性微调与执行。 FREE [27]提出了一个浅深模块，该模块能将当前词元的解码与先前已处理的提前退出词元进行同步。FREE利用自适应阈值估计器来确定合适的提前退出置信水平。 SkipDecode [83]专为批量推理和键值（KV）缓存而设计，通过在每个序列位置为批量中的每个词元建立唯一的退出点，克服了以往的局限。 PABEE [491]通过在每层集成内部分类器来提高预训练语言模型的效率。当预测结果在一定步数内保持稳定时，推理过程就会停止，这有助于在减少层数使用的情况下实现更快速的预测。 DeeBERT [428]通过纳入提前退出点来提高BERT的推理效率。DeeBERT允许实例基于置信水平在中间层就终止运行，有效地降低了计算需求并加快了推理速度。 巴赫蒂亚尼亚等人（Bakhtiarnia et al.）[31]针对适用于视觉Transformer（ViT）主干网络动态推理的提前退出分支提出了7种不同的架构设计。 LGViT [431]呈现了一个专为通用视觉Transformer定制的提前退出框架，它具备多种退出头，比如局部感知头和全局聚合头，以此来平衡效率和准确性。这种方法取得了颇具竞争力的性能，速度大约提升了1.8倍。



