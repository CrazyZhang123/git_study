
## 🔑 SAE 训练的四个核心步骤

SAE 训练的目的是将 LLM 内部的一个稠密激活向量 $h \in \mathbb{R}^d$（维度 $d$ 通常很大）分解成一个高维、稀疏的特征向量 $z \in \mathbb{R}^n$（维度 $n \gg d$），然后用 $z$ 尽可能精确地重建回 $h$。

### 步骤 1：数据提取（构建训练集）

SAE 并不是对 LLM 的输入文本进行训练，而是对 LLM **内部的激活**进行训练。

* **操作：** ==选定 LLM 的某个特定的内部激活层（例如，某个 Transformer 块的 MLP 输出或 Attention 机制的输出）。==
* **数据：** 让 LLM 运行大量的文本数据（来自预训练数据集或特定任务数据集）。在每次运行中，捕获目标层的**稠密激活向量** $h$。
* **结果：** 收集一个庞大的激活数据集 $\mathcal{H} = \{h^{(1)}, h^{(2)}, h^{(3)}, \dots\}$，这成为了 SAE 的**训练输入**。

### 步骤 2：定义 SAE 架构

SAE 本身是一个简单的两层网络：

1.  **编码器（Encoder）：**
    * 作用：将稠密激活 $h$ 投影到稀疏特征空间 $z$。
    * 数学：$z = \text{ReLU}(W_{\text{enc}} h + b_{\text{enc}})$
        * $W_{\text{enc}} \in \mathbb{R}^{n \times d}$，其中 $n \gg d$（**过完备性**是关键）。
        * **ReLU**（整流线性单元）：**强制非负性**，确保特征 $z_i \ge 0$。这是实现**非负**稀疏性的核心。
2.  **解码器（Decoder）：**
    * 作用：将稀疏特征 $z$ 重建回原始激活 $\hat{h}$。
    * 数学：$\hat{h} = W_{\text{dec}} z + b_{\text{dec}}$
        * $W_{\text{dec}} \in \mathbb{R}^{d \times n}$。

### 步骤 3：定义损失函数（Loss Function）

SAE 的训练目标是**最小化**一个包含两项主要成分的损失函数 $L$：

$$
L = \underbrace{L_{\text{重建}}}_{\text{损失 1：精度}} + \underbrace{\lambda \cdot L_{\text{稀疏}}}_{\text{损失 2：稀疏性}}
$$

#### 损失 1：重建损失 ($L_{\text{重建}}$)

* **目标：** 要求解码器的输出 $\hat{h}$ 尽可能接近原始输入 $h$。
* **通常使用：** 均方误差 (MSE) 或 L2 损失。
    $$L_{\text{重建}} = \|h - \hat{h}\|_2^2$$

#### 损失 2：稀疏性损失 ($L_{\text{稀疏}}$)

* **目标：** 惩罚特征向量 $z$ 中**非零元素过多**的情况，强制模型在每个样本上**只激活少数**特征。
* **通常使用：** L1 范数（L1-Norm），它测量所有特征激活强度的总和。
    $$L_{\text{稀疏}} = \|z\|_1 = \sum_{i=1}^n |z_i|$$
* **稀疏惩罚系数 $\lambda$：** 这是一个超参数，控制模型对稀疏性的重视程度。$\lambda$ 越大，模型越稀疏，但重建精度可能越低。

### 步骤 4：优化和单义性产生

* **优化：** 使用梯度下降法（如 AdamW 优化器）最小化总损失 $L$，不断调整 $W_{\text{enc}}, b_{\text{enc}}, W_{\text{dec}}, b_{\text{dec}}$ 的权重。
* **单义性（Monosemanticity）的产生：** 由于 SAE 空间是**过完备** ($n \gg d$) 且被**稀疏惩罚**的，模型被迫为每个输入特征找到一个**专门的、不重叠的**编码方式。例如，当看到“猫”时，它倾向于只激活 $Z_{\text{猫科动物}}$ 而不是激活 $Z_{\text{猫科动物}}$ 和 $Z_{\text{宠物}}$ 的混合。这种分解和专业化，最终形成了可解释的单义性特征。