## 1. 为什么需要聚合？什么步骤？



### A. 为什么需要聚合？



在问答任务（如 TruthfulQA 或 FACTOR）中，模型输出的不是单个 token，而是**一串 token**（即答案）。为了判断一个**完整的答案选项**（比如 A, B, C, D 选项，每个选项都是一个句子）哪个最好，我们需要一个**单一的分数**来衡量模型对这个**完整句子**的信心。

**例如：** 假设选项 A 是 "The capital of France is Paris." (5个 token)。

LLM 会按顺序生成每个 token，并给出它的概率：

1. $P(\text{The})$
2. $P(\text{capital})$
3. $P(\text{of})$
4. $P(\text{France})$
5. $P(\text{is})$
6. $P(\text{Paris})$

为了得到整个选项 A 的分数，我们需要将所有这些**单 token 概率**组合起来，这个组合过程就是**聚合**。



### B. 聚合发生在哪个步骤？



聚合发生在**模型生成完整答案后，计算最终指标之前**。

对于一个完整答案序列 $x_1, x_2, \dots, x_k$，LLM 通常使用**序列的对数概率和**来表示信心：

$$\text{Score}_{\text{序列}} = \sum_{t=1}^k \log P(x_t | x_{<t})$$

- **注：** 对数概率之和等于原始概率的连乘，这是一种常用的序列评分方法。

在 DoLa 的评估中，他们将这个步骤应用在**经过引导校正**后的概率 $\hat{p}$ 上：

$$\text{DoLa Score} = \sum_{t=1}^k \log \hat{p}(x_t | x_{<t})$$

------



## 2. 聚合为什么就错了？（Chuang et al. 的原研究的错误）



问题不在于聚合这个步骤本身，而在于**聚合时使用的数值类型是错误的**。



### A. 错误的聚合（原研究的做法）



原研究（Chuang et al., 2024）错误地使用了公式 (2) 的输出 $F$ 来直接聚合：

$$\text{Score}_{\text{错误}} = \sum_{t=1}^k F(q_L, q_P)_{x_t}$$

- **回顾公式 (2)：** $F(q_L, q_P)_{x_t} = \log \frac{q_L(x_t)}{q_P(x_t)}$
- **$F$ 的含义：** 它是一个**对数比值**，代表 **“深层相对于浅层的信心增强程度”**。它是一个**纯粹的引导信号**，而不是一个真实的对数概率。



### B. 错误的原因：$\log$ 比值与 $\log$ 概率的区别



当 $F$ 是一个正值时，它表示该 token 被增强了；当 $F$ 是一个负值时，它表示该 token 被抑制了。

但 $F$ **不等于** $\log \hat{p}$，因为 $\hat{p}$ 是经过 $\text{softmax}$ 归一化后的**真实概率**。



#### 导致 "长度偏差" 的原因：



1. **缺乏归一化：** 由于 $F$ 没有经过 $\text{softmax}$ 归一化（公式 1），它不是一个概率分布，其数值范围是任意的。
2. **累积效应失真：** 当你将 $k$ 个 $F$ 值（$k$ 为序列长度）相加时，你累积的只是**增强/抑制的幅度**，而不是**序列的真实发生可能性**。
   - **长度偏差：** 如果答案越长 ($k$ 越大)，$\sum F$ 的**绝对值**就会越大。
   - 如果 $F$ 多数是正值（增强），长答案会获得**过度奖励**。
   - 如果 $F$ 多数是负值（抑制），长答案会获得**过度惩罚**。



### C. 正确的做法（本研究的修正）



正确的做法是先将 $F$ 值通过 $\text{softmax}$ 归一化得到真实的校正后概率 $\hat{p}$（公式 1），然后再对这个**真实概率**取对数并聚合：

$$\text{Score}_{\text{正确}} = \sum_{t=1}^k \log \left[ \text{softmax}(F(q_L, q_P)) \right]_{x_t}$$

这样计算出的分数才真正代表了模型对整个答案序列的**信心**，并且消除了序列长度对分数的不公平影响。