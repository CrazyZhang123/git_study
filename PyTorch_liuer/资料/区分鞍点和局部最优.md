---
created: 2024-09-29T17:36
updated: 2024-09-29T18:25
---
[来自李宏毅老师](https://www.bilibili.com/video/BV1zP4y1p73V/?spm_id_from=333.337.search-card.all.click&vd_source=1ada3b7c2166492e6767c12fe3d6825a)

### 如何判断[驻点](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E9%A9%BB%E7%82%B9&zhida_source=entity)的类型

  

![](https://pic1.zhimg.com/80/v2-934465be8b8dacc3d7bce6046da85b68_720w.webp?source=1def8aca)

  

**为什么要知道到底是卡在local minima还是卡在saddle point呢？**

- 因为如果是**卡在[local minima](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=2&q=local+minima&zhida_source=entity)，那可能就没有路可以走了**。这是因为该点四周都比较高，你现在所在的位置已经是最低的点，此时loss最低。往四周走loss都会比较高，你不知道怎么走到其他地方。
- 但saddle point相比较就没有这个问题。如果你今天是**卡在[saddle point](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=3&q=saddle+point&zhida_source=entity)的话，saddle point旁边还是有路可以走的，**还是有方向可以让你的loss更低的。你只要逃离saddle point，你就有可能让你的loss更低。

所以判别驻点的类型是有实际意义的，需要借用[数学工具](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E6%95%B0%E5%AD%A6%E5%B7%A5%E5%85%B7&zhida_source=entity)。

### Tayler Series Approximation（[泰勒级数](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0&zhida_source=entity)近似法）

想要判明驻点的类型就需要知道loss function的形状，由于模型一般都十分复杂，无法具体描述loss function的形状，但是可以知道某一组参数附近的Loss function。

  

![](https://picx.zhimg.com/80/v2-7258e6ae61b72ed849547f3958721861_720w.webp?source=1def8aca)

  

如果给定某一组参数,比如说上图去献中的蓝色的这个θ'，在其附近的loss function是有办法被写出来的,它写出来就是上图中的样子。

所以L(θ)完整的样子写不出来，但是它在θ\`附近的式子可以写成上图中的公式。公式是**Tayler Series Appoximation（[泰勒级数展开](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0%E5%B1%95%E5%BC%80&zhida_source=entity)）**。

- 第一项的L（θ'）的含义就是当θ趋近于θ'的时候，L（θ）同时趋近于L（θ'）。
- 第二项是

![](https://pic1.zhimg.com/80/v2-98eeaadc38e3f61c905e3328590bde51_720w.webp?source=1def8aca)

，**g是一个向量，这个g就是我们的gradient**，绿色的g代表gradient，这个**gradient来弥补θ跟θ'之间的差距**。虽然θ跟θ'应该很接近，但是中间还是有一些差距的，这个差距用gradient来表示，有时候gradient会写成

![](https://pica.zhimg.com/80/v2-52095b79705bf80caaf163ef595dba57_720w.webp?source=1def8aca)

。注意g是一个向量，**它的第i个component（分量）,就是θ的第i个component对L的微分**。

- 第三项

![](https://pic1.zhimg.com/80/v2-170540ae5a94caac66dcad3ea7549f0d_720w.webp?source=1def8aca)

与Hessian Matrix（[黑塞矩阵](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5&zhida_source=entity))有关,这里边有一个H，叫做Hessian，为一个矩阵。整个第三项在加上gradient后就真正抵消了L（θ'）与L（θ）之间的差距。**H里放的是L的[二次微分](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E4%BA%8C%E6%AC%A1%E5%BE%AE%E5%88%86&zhida_source=entity)**

![](https://picx.zhimg.com/80/v2-9ed5161bf58041b9004543fa1b3f84b5_720w.webp?source=1def8aca)

，**它第i个row，第j个column的值就是把θ的第i个component对L作微分，再把θ的第j个component对L作微分，再把θ的第i个component,对L作微分，做两次微分以后的结果**就是这个。

### Hession

  

![](https://pic1.zhimg.com/80/v2-8c8fbf52dbd159e35f1d3af0c5b74157_720w.webp?source=1def8aca)

  

如果model训练到了一个critical point，意味著gradient=0.也就是上图公式中绿色框这一项为零。此时**g是一个[zero vector](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=zero+vector&zhida_source=entity)（零向量），绿色的这一项完全都不见了**，只剩下红色的这一项。

所以，当处于critical point的时候，这个loss function可以被近似为L（θ'）加上红色方框中的公式。因此，可以**根据红色的这一项来判断**在θ'附近的error surface（误差曲面）是什么样子的。只要能够判断误差曲面的形状就能判断驻点是局部最小还是[鞍点](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E9%9E%8D%E7%82%B9&zhida_source=entity)。判断的重点就是Hessian这个参数。

  

![](https://pica.zhimg.com/80/v2-e4acfc160fdf23f449d34c17f8f99e02_720w.webp?source=1def8aca)

  

为了方便起见，**把（θ-θ'）用向量v来表示**

- 如果对任何可能的v，**vᵀHv都大于零**；也就是说现在θ无论为任何值，**红色框里面都大于零**，也就意味着L（θ）>L（θ'）。L（θ）不管值为多少，只要在θ'附近，L（θ）都大于L（θ'）。也就说明**L（θ'）代表附近的一个最低点，所以它是local minima**。
- 如果反过来说，对所有的v而言，**vᵀ都小于零，也就是红色框里面永远都小于零**,也就是说θ无论为任何值，红色框里面都小于零。也就意味着L（θ）<L（θ'），**代表L（θ'）是附近最高的一个点，所以它是local maxima**。
- 第三个，假设**vᵀHv有时候大于零，有时候小于零**。你代入不同的v进去、代不同的θ进去，红色框里面有时候大于零，有时候小于零。意味着在θ'附近，有时候L(θ)>L(θ')、有时候L(θ)<L(θ')，也就是在L(θ')附近,有些地方高、有些地方低。**这说明这是一个saddle point**。

但是不可能把所有的v都代入来计算vᵀHv是大于零还是小于零，这有一个更方便的办法。

  

![](https://picx.zhimg.com/80/v2-ff6c5b13e5d6e0d43a1134d9a7b33d8f_720w.webp?source=1def8aca)

  

在线性代数中，如果对所有的v，vтHv都大于零，那这种矩阵叫做**positive definite（正定矩阵）**，这类矩阵的**所有的eigen value（特征值）都是正的**。

所以如果你今天算出一个hessian，不需要把它跟所有的v都相乘来判断符号，你只要去直接看这个H的eigen value即可。

如果你发现

- **hessian metric的所有eigen value都是正的**，即vᵀHv大于零，也就代表这是一个local minima。
- 反过来说也是一样。**如何hessian metric的所有eigen value都是负的**，即vᵀHv小于零，也就代表这是一个local maxima。
- **那如果eigen value有正有负**，那就代表是saddle point。

​ 总的来说，**你只要算出一个东西，这个东西的名字叫做hessian。它是一个矩阵，如果这个矩阵所有的eigen value都是正的，那就代表我们现在在local minima；如果它有正有负,就代表在saddle point。**

[[矩阵是否正定_负定、半正定_半负定的判断]]

---

再举一个例子。

  

![](https://picx.zhimg.com/80/v2-51298205c53b560f11f856aa27b1362a_720w.webp?source=1def8aca)

  

现在有一个史上最废的network。它只有一个neuron（神经元），输入一个x，乘上w₁。而且这个neuron还没有activation function（[激活函数](https://zhida.zhihu.com/search?content_id=441110369&content_type=Answer&match_order=1&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity)），所以x乘上w₁以后就输出；然后再乘上w₂，之后再输出就得到最终的数据，也就是y。

总之这个function非常的简单：

![](https://picx.zhimg.com/80/v2-970bedca49acdbcb76456af3b22599e1_720w.webp?source=1def8aca)

  

我们有一个史上最废的training set。这个data set只有一笔data，这个data是x=1的时候，它的level是1。所以输入1进去，你希望最终的输出跟1越接近越好。

而这个史上最废的training它的error surface也有办法直接画出来的。因为只有两个参数 w₁ w₂，连bias都没有。假设没有bias，只有w₁ w₂两个参数，那我们可以穷举所有w₁跟w₂的数值，算出所有w₁ w₂数值所代来的loss，然后就画出error surface，如下图。

  

![](https://picx.zhimg.com/80/v2-bdb207595826fc264bbd9aad8c9260ae_720w.webp?source=1def8aca)

  

误差曲面的四个角落的loss是高的，上图中有一些critical point，也就是黑点的地方，其中(0,0)也就是**原点的地方是critical point**，然后实际上**右上三个黑点也是一排critical point，左下三个点也是一排critical point**。

在以上的几个黑点中，**原点这个地方的驻点是saddle point**。因为以这个点为中心，往左上走loss会变大，往右下走loss会变大，往左下走loss会变小，往右下走loss会变小，因此它是一个saddle point。

而在误差曲面右上和左下的这两群critical point都是local minima。所以可以看成在右上的山沟里面有一排local minima，在左下这的山沟里面有一排local minima。然后在原点有一个saddle point。这个是采用暴力方法，把所有的参数代入loss function以后画出的error surface。

现在假设如果不暴力所有可能的loss，如果要直接算说一个点是local minima还是saddle point的话有什么样的方法？

  

![](https://picx.zhimg.com/80/v2-e2efaad769e39173137dfab288a21cce_720w.webp?source=1def8aca)

  

我们可以把loss的function写出来，这个L为：

![](https://picx.zhimg.com/80/v2-bd6a1fd6f879ff898b78379b21b13d32_720w.webp?source=1def8aca)

  

在这个L中，ŷ（实际数值）减掉model（也就是w₁ w₂x）然后取square error（平方差）。这边**只有一笔data,所以就不用summation over（求和）所有的training data**。因為反正只有一笔data，x代1，ŷ代1。因为只有一笔训练资料，是最废的，那你可以把这一个loss function它的gradient求出来，w₁对L的微分,w₂对L的微分写出来是这个样子：

![](https://picx.zhimg.com/80/v2-0d42778a3bd6ee7a3a40843eeb8a3385_720w.webp?source=1def8aca)

  

其中：

![](https://picx.zhimg.com/80/v2-dcda9b74b65b85f5b90e13003d6662ae_720w.webp?source=1def8aca)

就是gradient。

如果w₁=0 w₂=0就在圆心，并且w₁代0 w₂代0，w₁对L的微分 w₂对L的微分,算出来就都是零。这个时候就知道原点就是一个critical point，但**它是local maxima、local minima还是saddle point就要看hessian才能够知道**。

  

![](https://pica.zhimg.com/80/v2-4aaf248f60d0dbef53b467e78f5e1af7_720w.webp?source=1def8aca)

  

刚才已经遍历所有可能的w₁ w₂了，所以已经知道它显然是一个saddle point。但是假设还没有遍历所有可能的loss，我们要看看能不能够用Hessian看出它是什么样的critical point。

**H它是一个矩阵，这个矩阵里的元素就是L的二次微分**。所以这个矩阵中第一行第一列的位置,就是w₁对L微分两次

![](https://picx.zhimg.com/80/v2-59ce7e3e79b72b416aa1ef3d252f87c2_720w.webp?source=1def8aca)

；第一行第二列就是先用w₁对L作微分，再用w₂对L作微分

![](https://picx.zhimg.com/80/v2-1f04db1f54f8f6cba539a8f602032355_720w.webp?source=1def8aca)

。以此类推，得到的这四个值组合起来就是hessian，计算后H为

![](https://picx.zhimg.com/80/v2-ef2d8a2e137f6dc9a5cd028d5be5da78_720w.webp?source=1def8aca)

  

通过这个hessian矩阵如何可以知道它是local minima还是saddle point呢? 需要计算eigen value（特征值）。

经过计算，这个矩阵有两个eigen value：2、-2 。**eigen value有正有负代表saddle point**