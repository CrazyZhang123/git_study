{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcbc041",
   "metadata": {},
   "source": [
    "## 1、交叉熵损失实现\n",
    "\n",
    "### 1.1 Cross Entropy in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56149ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729189131256584\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# one hot编码\n",
    "y = np.array([1,0,0])\n",
    "z = np.array([0.2,0.1,-0.1])\n",
    "y_pred = np.exp(z) / np.exp(z).sum()\n",
    "loss = -(y*np.log(y_pred)).sum()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c8aa6",
   "metadata": {},
   "source": [
    "### 1.2 Cross Entropy in Pytorch\n",
    "\n",
    "**标签值y值在pytorch的CrossEntropyLoss中会自动转换为one hot编码!!!**\n",
    "\n",
    "CrossEntropyLoss == LogSoftmax + NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd966bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9729)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# y值在pytorch的CrossEntropyLoss中会自动转换为one hot编码!!!\n",
    "\n",
    "y = torch.tensor([0],dtype=torch.long)\n",
    "z = torch.tensor([[0.2,0.1,-0.1]],dtype=torch.float32)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(z,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ab5888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 0.7911708354949951, loss2:1.2461291551589966\n"
     ]
    }
   ],
   "source": [
    "# Mini-Batch\n",
    "import torch\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "Y = torch.tensor([2,0,1],dtype=torch.long)\n",
    "\n",
    "# 自动识别为torch.float32\n",
    "# 2 0 1\n",
    "Y_pred1 = torch.tensor([[0.1,0.2,0.9],[0.7,0.1,0.2],[0.2,0.5,0.3]]) \n",
    "# 0 1 1\n",
    "Y_pred2 = torch.tensor([[0.8,0.1,0.1],[0.2,0.8,0.1],[0.3,0.4,0.3]])\n",
    "\n",
    "print(f'loss1: {criterion(Y_pred1,Y)}, loss2:{criterion(Y_pred2,Y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550c9e1",
   "metadata": {},
   "source": [
    "## 2、In MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a774c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0、导包\n",
    "import torch \n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b75f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、准备数据集\n",
    "\n",
    "# 为了更高效的处理，比如卷积操作，做如下操作：\n",
    "# 通过cv读取图像进来是 w * H * c,转到pytorch 里面会转为 c * w * h\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # 标准化：第一个是mean,第二个是std。\n",
    "    transforms.Normalize((0.1307,),(0.3081))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/dataset/MNIST',train=True,transform=transform,download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/dataset/MNIST',train=False,transform=transform,download=True)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、设计网络\n",
    "class MNIST_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # 父类初始化!!!\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = torch.nn.Linear(784,512)\n",
    "        self.l2 = torch.nn.Linear(512,256)\n",
    "        self.l3 = torch.nn.Linear(256,128)\n",
    "        self.l4 = torch.nn.Linear(128,64)\n",
    "        self.l5 = torch.nn.Linear(64,10)\n",
    "        self.activate = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # 单个图像：28 * 28 = 784\n",
    "        x = x.view(-1,784)\n",
    "        x = self.activate(self.l1(x))\n",
    "        x = self.activate(self.l2(x))\n",
    "        x = self.activate(self.l3(x))\n",
    "        x = self.activate(self.l4(x))\n",
    "        return self.l5(x)\n",
    "model = MNIST_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fbfbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 数据集比较大，使用momentum来加速训练\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6267cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、训练和测试\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_id,data in enumerate(train_dataloader,0):\n",
    "        # 获取数据\n",
    "        inputs,targets = data\n",
    "\n",
    "        # 前向传播\n",
    "        y_pred = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criterion(y_pred,targets)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 损失求和\n",
    "        running_loss += loss.item()\n",
    "        # \n",
    "        if batch_id % 300 == 299:\n",
    "            print(f'[epoch: {epoch +1},batch_id:{batch_id + 1}] loss: {running_loss / 300 : .4f}')\n",
    "            # 损失清零\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不计算梯度\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            # 获取数据\n",
    "            inputs, targets = data\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            # 不同类别概率的最大值\n",
    "            _,predicted = torch.max(outputs.data,dim=1)\n",
    "            # 样本数量\n",
    "            total += targets.size(0) \n",
    "            # 计算预测正确的数量\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    print(f'Accuracy on test set:{100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807ed689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1,batch_id:300] loss:  0.0912\n",
      "[epoch: 1,batch_id:600] loss:  0.0921\n",
      "[epoch: 1,batch_id:900] loss:  0.0891\n",
      "Accuracy on test set:96.79%\n",
      "[epoch: 2,batch_id:300] loss:  0.0753\n",
      "[epoch: 2,batch_id:600] loss:  0.0705\n",
      "[epoch: 2,batch_id:900] loss:  0.0765\n",
      "Accuracy on test set:97.32%\n",
      "[epoch: 3,batch_id:300] loss:  0.0542\n",
      "[epoch: 3,batch_id:600] loss:  0.0629\n",
      "[epoch: 3,batch_id:900] loss:  0.0628\n",
      "Accuracy on test set:97.31%\n",
      "[epoch: 4,batch_id:300] loss:  0.0494\n",
      "[epoch: 4,batch_id:600] loss:  0.0489\n",
      "[epoch: 4,batch_id:900] loss:  0.0484\n",
      "Accuracy on test set:97.54%\n",
      "[epoch: 5,batch_id:300] loss:  0.0384\n",
      "[epoch: 5,batch_id:600] loss:  0.0404\n",
      "[epoch: 5,batch_id:900] loss:  0.0416\n",
      "Accuracy on test set:97.55%\n",
      "[epoch: 6,batch_id:300] loss:  0.0309\n",
      "[epoch: 6,batch_id:600] loss:  0.0348\n",
      "[epoch: 6,batch_id:900] loss:  0.0312\n",
      "Accuracy on test set:97.79%\n",
      "[epoch: 7,batch_id:300] loss:  0.0227\n",
      "[epoch: 7,batch_id:600] loss:  0.0289\n",
      "[epoch: 7,batch_id:900] loss:  0.0264\n",
      "Accuracy on test set:97.44%\n",
      "[epoch: 8,batch_id:300] loss:  0.0196\n",
      "[epoch: 8,batch_id:600] loss:  0.0199\n",
      "[epoch: 8,batch_id:900] loss:  0.0219\n",
      "Accuracy on test set:97.86%\n",
      "[epoch: 9,batch_id:300] loss:  0.0152\n",
      "[epoch: 9,batch_id:600] loss:  0.0166\n",
      "[epoch: 9,batch_id:900] loss:  0.0163\n",
      "Accuracy on test set:97.85%\n",
      "[epoch: 10,batch_id:300] loss:  0.0127\n",
      "[epoch: 10,batch_id:600] loss:  0.0138\n",
      "[epoch: 10,batch_id:900] loss:  0.0104\n",
      "Accuracy on test set:97.87%\n",
      "[epoch: 11,batch_id:300] loss:  0.0091\n",
      "[epoch: 11,batch_id:600] loss:  0.0091\n",
      "[epoch: 11,batch_id:900] loss:  0.0101\n",
      "Accuracy on test set:97.87%\n",
      "[epoch: 12,batch_id:300] loss:  0.0058\n",
      "[epoch: 12,batch_id:600] loss:  0.0091\n",
      "[epoch: 12,batch_id:900] loss:  0.0072\n",
      "Accuracy on test set:97.78%\n",
      "[epoch: 13,batch_id:300] loss:  0.0052\n",
      "[epoch: 13,batch_id:600] loss:  0.0067\n",
      "[epoch: 13,batch_id:900] loss:  0.0049\n",
      "Accuracy on test set:97.78%\n",
      "[epoch: 14,batch_id:300] loss:  0.0048\n",
      "[epoch: 14,batch_id:600] loss:  0.0036\n",
      "[epoch: 14,batch_id:900] loss:  0.0032\n",
      "Accuracy on test set:97.93%\n",
      "[epoch: 15,batch_id:300] loss:  0.0027\n",
      "[epoch: 15,batch_id:600] loss:  0.0030\n",
      "[epoch: 15,batch_id:900] loss:  0.0028\n",
      "Accuracy on test set:97.84%\n",
      "[epoch: 16,batch_id:300] loss:  0.0035\n",
      "[epoch: 16,batch_id:600] loss:  0.0022\n",
      "[epoch: 16,batch_id:900] loss:  0.0022\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 17,batch_id:300] loss:  0.0027\n",
      "[epoch: 17,batch_id:600] loss:  0.0018\n",
      "[epoch: 17,batch_id:900] loss:  0.0016\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 18,batch_id:300] loss:  0.0016\n",
      "[epoch: 18,batch_id:600] loss:  0.0014\n",
      "[epoch: 18,batch_id:900] loss:  0.0020\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 19,batch_id:300] loss:  0.0013\n",
      "[epoch: 19,batch_id:600] loss:  0.0012\n",
      "[epoch: 19,batch_id:900] loss:  0.0016\n",
      "Accuracy on test set:98.0%\n",
      "[epoch: 20,batch_id:300] loss:  0.0011\n",
      "[epoch: 20,batch_id:600] loss:  0.0011\n",
      "[epoch: 20,batch_id:900] loss:  0.0011\n",
      "Accuracy on test set:97.99%\n",
      "[epoch: 21,batch_id:300] loss:  0.0010\n",
      "[epoch: 21,batch_id:600] loss:  0.0009\n",
      "[epoch: 21,batch_id:900] loss:  0.0010\n",
      "Accuracy on test set:98.0%\n",
      "[epoch: 22,batch_id:300] loss:  0.0008\n",
      "[epoch: 22,batch_id:600] loss:  0.0008\n",
      "[epoch: 22,batch_id:900] loss:  0.0009\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 23,batch_id:300] loss:  0.0008\n",
      "[epoch: 23,batch_id:600] loss:  0.0007\n",
      "[epoch: 23,batch_id:900] loss:  0.0008\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 24,batch_id:300] loss:  0.0007\n",
      "[epoch: 24,batch_id:600] loss:  0.0007\n",
      "[epoch: 24,batch_id:900] loss:  0.0007\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 25,batch_id:300] loss:  0.0006\n",
      "[epoch: 25,batch_id:600] loss:  0.0006\n",
      "[epoch: 25,batch_id:900] loss:  0.0006\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 26,batch_id:300] loss:  0.0006\n",
      "[epoch: 26,batch_id:600] loss:  0.0006\n",
      "[epoch: 26,batch_id:900] loss:  0.0006\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 27,batch_id:300] loss:  0.0005\n",
      "[epoch: 27,batch_id:600] loss:  0.0006\n",
      "[epoch: 27,batch_id:900] loss:  0.0005\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 28,batch_id:300] loss:  0.0005\n",
      "[epoch: 28,batch_id:600] loss:  0.0005\n",
      "[epoch: 28,batch_id:900] loss:  0.0005\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 29,batch_id:300] loss:  0.0005\n",
      "[epoch: 29,batch_id:600] loss:  0.0005\n",
      "[epoch: 29,batch_id:900] loss:  0.0005\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 30,batch_id:300] loss:  0.0004\n",
      "[epoch: 30,batch_id:600] loss:  0.0004\n",
      "[epoch: 30,batch_id:900] loss:  0.0004\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 31,batch_id:300] loss:  0.0004\n",
      "[epoch: 31,batch_id:600] loss:  0.0004\n",
      "[epoch: 31,batch_id:900] loss:  0.0004\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 32,batch_id:300] loss:  0.0004\n",
      "[epoch: 32,batch_id:600] loss:  0.0004\n",
      "[epoch: 32,batch_id:900] loss:  0.0004\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 33,batch_id:300] loss:  0.0004\n",
      "[epoch: 33,batch_id:600] loss:  0.0004\n",
      "[epoch: 33,batch_id:900] loss:  0.0004\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 34,batch_id:300] loss:  0.0003\n",
      "[epoch: 34,batch_id:600] loss:  0.0004\n",
      "[epoch: 34,batch_id:900] loss:  0.0004\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 35,batch_id:300] loss:  0.0003\n",
      "[epoch: 35,batch_id:600] loss:  0.0003\n",
      "[epoch: 35,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 36,batch_id:300] loss:  0.0003\n",
      "[epoch: 36,batch_id:600] loss:  0.0003\n",
      "[epoch: 36,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 37,batch_id:300] loss:  0.0003\n",
      "[epoch: 37,batch_id:600] loss:  0.0003\n",
      "[epoch: 37,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 38,batch_id:300] loss:  0.0003\n",
      "[epoch: 38,batch_id:600] loss:  0.0003\n",
      "[epoch: 38,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 39,batch_id:300] loss:  0.0003\n",
      "[epoch: 39,batch_id:600] loss:  0.0003\n",
      "[epoch: 39,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 40,batch_id:300] loss:  0.0002\n",
      "[epoch: 40,batch_id:600] loss:  0.0003\n",
      "[epoch: 40,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 41,batch_id:300] loss:  0.0002\n",
      "[epoch: 41,batch_id:600] loss:  0.0003\n",
      "[epoch: 41,batch_id:900] loss:  0.0003\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 42,batch_id:300] loss:  0.0002\n",
      "[epoch: 42,batch_id:600] loss:  0.0002\n",
      "[epoch: 42,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 43,batch_id:300] loss:  0.0002\n",
      "[epoch: 43,batch_id:600] loss:  0.0002\n",
      "[epoch: 43,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 44,batch_id:300] loss:  0.0002\n",
      "[epoch: 44,batch_id:600] loss:  0.0002\n",
      "[epoch: 44,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 45,batch_id:300] loss:  0.0002\n",
      "[epoch: 45,batch_id:600] loss:  0.0002\n",
      "[epoch: 45,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.0%\n",
      "[epoch: 46,batch_id:300] loss:  0.0002\n",
      "[epoch: 46,batch_id:600] loss:  0.0002\n",
      "[epoch: 46,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 47,batch_id:300] loss:  0.0002\n",
      "[epoch: 47,batch_id:600] loss:  0.0002\n",
      "[epoch: 47,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 48,batch_id:300] loss:  0.0002\n",
      "[epoch: 48,batch_id:600] loss:  0.0002\n",
      "[epoch: 48,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.07%\n",
      "[epoch: 49,batch_id:300] loss:  0.0002\n",
      "[epoch: 49,batch_id:600] loss:  0.0002\n",
      "[epoch: 49,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 50,batch_id:300] loss:  0.0002\n",
      "[epoch: 50,batch_id:600] loss:  0.0002\n",
      "[epoch: 50,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 51,batch_id:300] loss:  0.0002\n",
      "[epoch: 51,batch_id:600] loss:  0.0002\n",
      "[epoch: 51,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 52,batch_id:300] loss:  0.0002\n",
      "[epoch: 52,batch_id:600] loss:  0.0002\n",
      "[epoch: 52,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 53,batch_id:300] loss:  0.0002\n",
      "[epoch: 53,batch_id:600] loss:  0.0002\n",
      "[epoch: 53,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 54,batch_id:300] loss:  0.0002\n",
      "[epoch: 54,batch_id:600] loss:  0.0002\n",
      "[epoch: 54,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 55,batch_id:300] loss:  0.0002\n",
      "[epoch: 55,batch_id:600] loss:  0.0002\n",
      "[epoch: 55,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.08%\n",
      "[epoch: 56,batch_id:300] loss:  0.0002\n",
      "[epoch: 56,batch_id:600] loss:  0.0002\n",
      "[epoch: 56,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 57,batch_id:300] loss:  0.0001\n",
      "[epoch: 57,batch_id:600] loss:  0.0002\n",
      "[epoch: 57,batch_id:900] loss:  0.0002\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 58,batch_id:300] loss:  0.0002\n",
      "[epoch: 58,batch_id:600] loss:  0.0001\n",
      "[epoch: 58,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 59,batch_id:300] loss:  0.0001\n",
      "[epoch: 59,batch_id:600] loss:  0.0001\n",
      "[epoch: 59,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 60,batch_id:300] loss:  0.0001\n",
      "[epoch: 60,batch_id:600] loss:  0.0002\n",
      "[epoch: 60,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.06%\n",
      "[epoch: 61,batch_id:300] loss:  0.0001\n",
      "[epoch: 61,batch_id:600] loss:  0.0001\n",
      "[epoch: 61,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 62,batch_id:300] loss:  0.0001\n",
      "[epoch: 62,batch_id:600] loss:  0.0001\n",
      "[epoch: 62,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 63,batch_id:300] loss:  0.0001\n",
      "[epoch: 63,batch_id:600] loss:  0.0001\n",
      "[epoch: 63,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 64,batch_id:300] loss:  0.0001\n",
      "[epoch: 64,batch_id:600] loss:  0.0001\n",
      "[epoch: 64,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 65,batch_id:300] loss:  0.0001\n",
      "[epoch: 65,batch_id:600] loss:  0.0001\n",
      "[epoch: 65,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 66,batch_id:300] loss:  0.0001\n",
      "[epoch: 66,batch_id:600] loss:  0.0001\n",
      "[epoch: 66,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 67,batch_id:300] loss:  0.0001\n",
      "[epoch: 67,batch_id:600] loss:  0.0001\n",
      "[epoch: 67,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 68,batch_id:300] loss:  0.0001\n",
      "[epoch: 68,batch_id:600] loss:  0.0001\n",
      "[epoch: 68,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.05%\n",
      "[epoch: 69,batch_id:300] loss:  0.0001\n",
      "[epoch: 69,batch_id:600] loss:  0.0001\n",
      "[epoch: 69,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 70,batch_id:300] loss:  0.0001\n",
      "[epoch: 70,batch_id:600] loss:  0.0001\n",
      "[epoch: 70,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 71,batch_id:300] loss:  0.0001\n",
      "[epoch: 71,batch_id:600] loss:  0.0001\n",
      "[epoch: 71,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 72,batch_id:300] loss:  0.0001\n",
      "[epoch: 72,batch_id:600] loss:  0.0001\n",
      "[epoch: 72,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 73,batch_id:300] loss:  0.0001\n",
      "[epoch: 73,batch_id:600] loss:  0.0001\n",
      "[epoch: 73,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 74,batch_id:300] loss:  0.0001\n",
      "[epoch: 74,batch_id:600] loss:  0.0001\n",
      "[epoch: 74,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 75,batch_id:300] loss:  0.0001\n",
      "[epoch: 75,batch_id:600] loss:  0.0001\n",
      "[epoch: 75,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 76,batch_id:300] loss:  0.0001\n",
      "[epoch: 76,batch_id:600] loss:  0.0001\n",
      "[epoch: 76,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 77,batch_id:300] loss:  0.0001\n",
      "[epoch: 77,batch_id:600] loss:  0.0001\n",
      "[epoch: 77,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 78,batch_id:300] loss:  0.0001\n",
      "[epoch: 78,batch_id:600] loss:  0.0001\n",
      "[epoch: 78,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 79,batch_id:300] loss:  0.0001\n",
      "[epoch: 79,batch_id:600] loss:  0.0001\n",
      "[epoch: 79,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 80,batch_id:300] loss:  0.0001\n",
      "[epoch: 80,batch_id:600] loss:  0.0001\n",
      "[epoch: 80,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 81,batch_id:300] loss:  0.0001\n",
      "[epoch: 81,batch_id:600] loss:  0.0001\n",
      "[epoch: 81,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 82,batch_id:300] loss:  0.0001\n",
      "[epoch: 82,batch_id:600] loss:  0.0001\n",
      "[epoch: 82,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 83,batch_id:300] loss:  0.0001\n",
      "[epoch: 83,batch_id:600] loss:  0.0001\n",
      "[epoch: 83,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 84,batch_id:300] loss:  0.0001\n",
      "[epoch: 84,batch_id:600] loss:  0.0001\n",
      "[epoch: 84,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 85,batch_id:300] loss:  0.0001\n",
      "[epoch: 85,batch_id:600] loss:  0.0001\n",
      "[epoch: 85,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 86,batch_id:300] loss:  0.0001\n",
      "[epoch: 86,batch_id:600] loss:  0.0001\n",
      "[epoch: 86,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 87,batch_id:300] loss:  0.0001\n",
      "[epoch: 87,batch_id:600] loss:  0.0001\n",
      "[epoch: 87,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 88,batch_id:300] loss:  0.0001\n",
      "[epoch: 88,batch_id:600] loss:  0.0001\n",
      "[epoch: 88,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 89,batch_id:300] loss:  0.0001\n",
      "[epoch: 89,batch_id:600] loss:  0.0001\n",
      "[epoch: 89,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 90,batch_id:300] loss:  0.0001\n",
      "[epoch: 90,batch_id:600] loss:  0.0001\n",
      "[epoch: 90,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 91,batch_id:300] loss:  0.0001\n",
      "[epoch: 91,batch_id:600] loss:  0.0001\n",
      "[epoch: 91,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 92,batch_id:300] loss:  0.0001\n",
      "[epoch: 92,batch_id:600] loss:  0.0001\n",
      "[epoch: 92,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 93,batch_id:300] loss:  0.0001\n",
      "[epoch: 93,batch_id:600] loss:  0.0001\n",
      "[epoch: 93,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.01%\n",
      "[epoch: 94,batch_id:300] loss:  0.0001\n",
      "[epoch: 94,batch_id:600] loss:  0.0001\n",
      "[epoch: 94,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 95,batch_id:300] loss:  0.0001\n",
      "[epoch: 95,batch_id:600] loss:  0.0001\n",
      "[epoch: 95,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.04%\n",
      "[epoch: 96,batch_id:300] loss:  0.0001\n",
      "[epoch: 96,batch_id:600] loss:  0.0001\n",
      "[epoch: 96,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 97,batch_id:300] loss:  0.0001\n",
      "[epoch: 97,batch_id:600] loss:  0.0001\n",
      "[epoch: 97,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 98,batch_id:300] loss:  0.0001\n",
      "[epoch: 98,batch_id:600] loss:  0.0001\n",
      "[epoch: 98,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.03%\n",
      "[epoch: 99,batch_id:300] loss:  0.0001\n",
      "[epoch: 99,batch_id:600] loss:  0.0001\n",
      "[epoch: 99,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.02%\n",
      "[epoch: 100,batch_id:300] loss:  0.0001\n",
      "[epoch: 100,batch_id:600] loss:  0.0001\n",
      "[epoch: 100,batch_id:900] loss:  0.0001\n",
      "Accuracy on test set:98.05%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train(epoch=epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
