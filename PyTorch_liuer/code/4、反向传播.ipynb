{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a385239c",
   "metadata": {},
   "source": [
    "## 1、使用pytorch实现反向传播\n",
    "\n",
    "SGD 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07687f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training) 4 4.0\n",
      "\tgrad: 1.0 2.0 1.0 -2.0\n",
      "\tgrad: 2.0 4.0 3.841600179672241 -7.840000152587891\n",
      "\tgrad: 3.0 6.0 7.315943717956543 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "\tgrad: 1.0 2.0 0.5465821623802185 -1.478623867034912\n",
      "\tgrad: 2.0 4.0 2.099749803543091 -5.796205520629883\n",
      "\tgrad: 3.0 6.0 3.9987640380859375 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "\tgrad: 1.0 2.0 0.2987521290779114 -1.0931644439697266\n",
      "\tgrad: 2.0 4.0 1.1476863622665405 -4.285204887390137\n",
      "\tgrad: 3.0 6.0 2.1856532096862793 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "\tgrad: 1.0 2.0 0.16329261660575867 -0.8081896305084229\n",
      "\tgrad: 2.0 4.0 0.6273048520088196 -3.1681032180786133\n",
      "\tgrad: 3.0 6.0 1.1946394443511963 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "\tgrad: 1.0 2.0 0.08925279974937439 -0.5975041389465332\n",
      "\tgrad: 2.0 4.0 0.34287363290786743 -2.3422164916992188\n",
      "\tgrad: 3.0 6.0 0.6529689431190491 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "\tgrad: 1.0 2.0 0.048784039914608 -0.4417421817779541\n",
      "\tgrad: 2.0 4.0 0.18740876019001007 -1.7316293716430664\n",
      "\tgrad: 3.0 6.0 0.35690122842788696 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "\tgrad: 1.0 2.0 0.02666448801755905 -0.3265852928161621\n",
      "\tgrad: 2.0 4.0 0.10243429243564606 -1.2802143096923828\n",
      "\tgrad: 3.0 6.0 0.195076122879982 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "\tgrad: 1.0 2.0 0.014574333094060421 -0.24144840240478516\n",
      "\tgrad: 2.0 4.0 0.055988773703575134 -0.9464778900146484\n",
      "\tgrad: 3.0 6.0 0.10662525147199631 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "\tgrad: 1.0 2.0 0.007966067641973495 -0.17850565910339355\n",
      "\tgrad: 2.0 4.0 0.030602457001805305 -0.699742317199707\n",
      "\tgrad: 3.0 6.0 0.0582793727517128 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "\tgrad: 1.0 2.0 0.004354109987616539 -0.1319713592529297\n",
      "\tgrad: 2.0 4.0 0.016726721078157425 -0.5173273086547852\n",
      "\tgrad: 3.0 6.0 0.03185431286692619 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "\tgrad: 1.0 2.0 0.002379868645220995 -0.09756779670715332\n",
      "\tgrad: 2.0 4.0 0.00914248451590538 -0.3824653625488281\n",
      "\tgrad: 3.0 6.0 0.017410902306437492 -0.7917022705078125\n",
      "progress: 10 0.017410902306437492\n",
      "\tgrad: 1.0 2.0 0.001300786156207323 -0.07213282585144043\n",
      "\tgrad: 2.0 4.0 0.004997097887098789 -0.2827606201171875\n",
      "\tgrad: 3.0 6.0 0.009516451507806778 -0.5853137969970703\n",
      "progress: 11 0.009516451507806778\n",
      "\tgrad: 1.0 2.0 0.000710982596501708 -0.053328514099121094\n",
      "\tgrad: 2.0 4.0 0.0027312987949699163 -0.2090473175048828\n",
      "\tgrad: 3.0 6.0 0.005201528314501047 -0.43272972106933594\n",
      "progress: 12 0.005201528314501047\n",
      "\tgrad: 1.0 2.0 0.0003886088088620454 -0.039426326751708984\n",
      "\tgrad: 2.0 4.0 0.0014928855234757066 -0.15455150604248047\n",
      "\tgrad: 3.0 6.0 0.0028430151287466288 -0.3199195861816406\n",
      "progress: 13 0.0028430151287466288\n",
      "\tgrad: 1.0 2.0 0.0002124064340023324 -0.029148340225219727\n",
      "\tgrad: 2.0 4.0 0.0008159824647009373 -0.11426162719726562\n",
      "\tgrad: 3.0 6.0 0.0015539465239271522 -0.23652076721191406\n",
      "progress: 14 0.0015539465239271522\n",
      "\tgrad: 1.0 2.0 0.0001160974134108983 -0.021549701690673828\n",
      "\tgrad: 2.0 4.0 0.00044599699322134256 -0.08447456359863281\n",
      "\tgrad: 3.0 6.0 0.0008493617060594261 -0.17486286163330078\n",
      "progress: 15 0.0008493617060594261\n",
      "\tgrad: 1.0 2.0 6.345591827994213e-05 -0.01593184471130371\n",
      "\tgrad: 2.0 4.0 0.0002437756775179878 -0.062453269958496094\n",
      "\tgrad: 3.0 6.0 0.00046424579340964556 -0.12927818298339844\n",
      "progress: 16 0.00046424579340964556\n",
      "\tgrad: 1.0 2.0 3.468381328275427e-05 -0.011778593063354492\n",
      "\tgrad: 2.0 4.0 0.00013324167230166495 -0.046172142028808594\n",
      "\tgrad: 3.0 6.0 0.0002537401160225272 -0.09557533264160156\n",
      "progress: 17 0.0002537401160225272\n",
      "\tgrad: 1.0 2.0 1.895835521281697e-05 -0.00870823860168457\n",
      "\tgrad: 2.0 4.0 7.282838487299159e-05 -0.03413581848144531\n",
      "\tgrad: 3.0 6.0 0.00013869594840798527 -0.07066154479980469\n",
      "progress: 18 0.00013869594840798527\n",
      "\tgrad: 1.0 2.0 1.0361248314438853e-05 -0.006437778472900391\n",
      "\tgrad: 2.0 4.0 3.980389010393992e-05 -0.025236129760742188\n",
      "\tgrad: 3.0 6.0 7.580435340059921e-05 -0.052239418029785156\n",
      "progress: 19 7.580435340059921e-05\n",
      "\tgrad: 1.0 2.0 5.663329375238391e-06 -0.004759550094604492\n",
      "\tgrad: 2.0 4.0 2.1756823116447777e-05 -0.018657684326171875\n",
      "\tgrad: 3.0 6.0 4.143271507928148e-05 -0.038620948791503906\n",
      "progress: 20 4.143271507928148e-05\n",
      "\tgrad: 1.0 2.0 3.0955231977713993e-06 -0.003518819808959961\n",
      "\tgrad: 2.0 4.0 1.1892057955265045e-05 -0.0137939453125\n",
      "\tgrad: 3.0 6.0 2.264650902361609e-05 -0.028553009033203125\n",
      "progress: 21 2.264650902361609e-05\n",
      "\tgrad: 1.0 2.0 1.692111254669726e-06 -0.00260162353515625\n",
      "\tgrad: 2.0 4.0 6.500706149381585e-06 -0.010198593139648438\n",
      "\tgrad: 3.0 6.0 1.2377059647405986e-05 -0.021108627319335938\n",
      "progress: 22 1.2377059647405986e-05\n",
      "\tgrad: 1.0 2.0 9.247925163435866e-07 -0.0019233226776123047\n",
      "\tgrad: 2.0 4.0 3.552988573574112e-06 -0.0075397491455078125\n",
      "\tgrad: 3.0 6.0 6.768445018678904e-06 -0.0156097412109375\n",
      "progress: 23 6.768445018678904e-06\n",
      "\tgrad: 1.0 2.0 5.056396048530587e-07 -0.0014221668243408203\n",
      "\tgrad: 2.0 4.0 1.9426645394560182e-06 -0.0055751800537109375\n",
      "\tgrad: 3.0 6.0 3.7000872907810844e-06 -0.011541366577148438\n",
      "progress: 24 3.7000872907810844e-06\n",
      "\tgrad: 1.0 2.0 2.763741235867201e-07 -0.0010514259338378906\n",
      "\tgrad: 2.0 4.0 1.0618171017995337e-06 -0.0041217803955078125\n",
      "\tgrad: 3.0 6.0 2.021880391112063e-06 -0.008531570434570312\n",
      "progress: 25 2.021880391112063e-06\n",
      "\tgrad: 1.0 2.0 1.5102727957128081e-07 -0.0007772445678710938\n",
      "\tgrad: 2.0 4.0 5.802590408165997e-07 -0.0030469894409179688\n",
      "\tgrad: 3.0 6.0 1.1044940038118511e-06 -0.006305694580078125\n",
      "progress: 26 1.1044940038118511e-06\n",
      "\tgrad: 1.0 2.0 8.253806527136476e-08 -0.0005745887756347656\n",
      "\tgrad: 2.0 4.0 3.171319349348778e-07 -0.0022525787353515625\n",
      "\tgrad: 3.0 6.0 6.041091182851233e-07 -0.0046634674072265625\n",
      "progress: 27 6.041091182851233e-07\n",
      "\tgrad: 1.0 2.0 4.512691020863713e-08 -0.0004248619079589844\n",
      "\tgrad: 2.0 4.0 1.73288071891875e-07 -0.0016651153564453125\n",
      "\tgrad: 3.0 6.0 3.296045179013163e-07 -0.003444671630859375\n",
      "progress: 28 3.296045179013163e-07\n",
      "\tgrad: 1.0 2.0 2.4648571184116008e-08 -0.0003139972686767578\n",
      "\tgrad: 2.0 4.0 9.473984619035036e-08 -0.0012311935424804688\n",
      "\tgrad: 3.0 6.0 1.805076408345485e-07 -0.0025491714477539062\n",
      "progress: 29 1.805076408345485e-07\n",
      "\tgrad: 1.0 2.0 1.3481496807798976e-08 -0.00023221969604492188\n",
      "\tgrad: 2.0 4.0 5.184261908652843e-08 -0.0009107589721679688\n",
      "\tgrad: 3.0 6.0 9.874406714516226e-08 -0.0018854141235351562\n",
      "progress: 30 9.874406714516226e-08\n",
      "\tgrad: 1.0 2.0 7.387384926005325e-09 -0.00017189979553222656\n",
      "\tgrad: 2.0 4.0 2.8413126074156025e-08 -0.0006742477416992188\n",
      "\tgrad: 3.0 6.0 5.4147676564753056e-08 -0.00139617919921875\n",
      "progress: 31 5.4147676564753056e-08\n",
      "\tgrad: 1.0 2.0 4.037147505187022e-09 -0.0001270771026611328\n",
      "\tgrad: 2.0 4.0 1.548892214486841e-08 -0.0004978179931640625\n",
      "\tgrad: 3.0 6.0 2.9467628337442875e-08 -0.00102996826171875\n",
      "progress: 32 2.9467628337442875e-08\n",
      "\tgrad: 1.0 2.0 2.2060362425690982e-09 -9.393692016601562e-05\n",
      "\tgrad: 2.0 4.0 8.469442036584951e-09 -0.0003681182861328125\n",
      "\tgrad: 3.0 6.0 1.6088051779661328e-08 -0.0007610321044921875\n",
      "progress: 33 1.6088051779661328e-08\n",
      "\tgrad: 1.0 2.0 1.2033893881380209e-09 -6.937980651855469e-05\n",
      "\tgrad: 2.0 4.0 4.617106696969131e-09 -0.00027179718017578125\n",
      "\tgrad: 3.0 6.0 8.734787115827203e-09 -0.000560760498046875\n",
      "progress: 34 8.734787115827203e-09\n",
      "\tgrad: 1.0 2.0 6.568967592102126e-10 -5.125999450683594e-05\n",
      "\tgrad: 2.0 4.0 2.5307258511020336e-09 -0.00020122528076171875\n",
      "\tgrad: 3.0 6.0 4.8466972657479346e-09 -0.0004177093505859375\n",
      "progress: 35 4.8466972657479346e-09\n",
      "\tgrad: 1.0 2.0 3.5926461805502186e-10 -3.790855407714844e-05\n",
      "\tgrad: 2.0 4.0 1.3833414413966238e-09 -0.000148773193359375\n",
      "\tgrad: 3.0 6.0 2.6520865503698587e-09 -0.000308990478515625\n",
      "progress: 36 2.6520865503698587e-09\n",
      "\tgrad: 1.0 2.0 1.978719410544727e-10 -2.8133392333984375e-05\n",
      "\tgrad: 2.0 4.0 7.648850441910326e-10 -0.000110626220703125\n",
      "\tgrad: 3.0 6.0 1.4551915228366852e-09 -0.0002288818359375\n",
      "progress: 37 1.4551915228366852e-09\n",
      "\tgrad: 1.0 2.0 1.1004885891452432e-10 -2.09808349609375e-05\n",
      "\tgrad: 2.0 4.0 4.204139258945361e-10 -8.20159912109375e-05\n",
      "\tgrad: 3.0 6.0 7.914877642178908e-10 -0.00016880035400390625\n",
      "progress: 38 7.914877642178908e-10\n",
      "\tgrad: 1.0 2.0 6.004086117172847e-11 -1.5497207641601562e-05\n",
      "\tgrad: 2.0 4.0 2.3283064365386963e-10 -6.103515625e-05\n",
      "\tgrad: 3.0 6.0 4.4019543565809727e-10 -0.000125885009765625\n",
      "progress: 39 4.4019543565809727e-10\n",
      "\tgrad: 1.0 2.0 3.2741809263825417e-11 -1.1444091796875e-05\n",
      "\tgrad: 2.0 4.0 1.255671122635249e-10 -4.482269287109375e-05\n",
      "\tgrad: 3.0 6.0 2.3283064365386963e-10 -9.1552734375e-05\n",
      "progress: 40 2.3283064365386963e-10\n",
      "\tgrad: 1.0 2.0 1.7408297026122455e-11 -8.344650268554688e-06\n",
      "\tgrad: 2.0 4.0 6.571099220309407e-11 -3.24249267578125e-05\n",
      "\tgrad: 3.0 6.0 1.2028067430946976e-10 -6.580352783203125e-05\n",
      "progress: 41 1.2028067430946976e-10\n",
      "\tgrad: 1.0 2.0 8.881784197001252e-12 -5.9604644775390625e-06\n",
      "\tgrad: 2.0 4.0 3.2741809263825417e-11 -2.288818359375e-05\n",
      "\tgrad: 3.0 6.0 5.820766091346741e-11 -4.57763671875e-05\n",
      "progress: 42 5.820766091346741e-11\n",
      "\tgrad: 1.0 2.0 4.604316927725449e-12 -4.291534423828125e-06\n",
      "\tgrad: 2.0 4.0 1.8417267710901797e-11 -1.71661376953125e-05\n",
      "\tgrad: 3.0 6.0 3.842615114990622e-11 -3.719329833984375e-05\n",
      "progress: 43 3.842615114990622e-11\n",
      "\tgrad: 1.0 2.0 2.7853275241795927e-12 -3.337860107421875e-06\n",
      "\tgrad: 2.0 4.0 1.1141310096718371e-11 -1.33514404296875e-05\n",
      "\tgrad: 3.0 6.0 2.2737367544323206e-11 -2.86102294921875e-05\n",
      "progress: 44 2.2737367544323206e-11\n",
      "\tgrad: 1.0 2.0 1.7195134205394424e-12 -2.6226043701171875e-06\n",
      "\tgrad: 2.0 4.0 6.87805368215777e-12 -1.049041748046875e-05\n",
      "\tgrad: 3.0 6.0 1.4551915228366852e-11 -2.288818359375e-05\n",
      "progress: 45 1.4551915228366852e-11\n",
      "\tgrad: 1.0 2.0 9.094947017729282e-13 -1.9073486328125e-06\n",
      "\tgrad: 2.0 4.0 3.637978807091713e-12 -7.62939453125e-06\n",
      "\tgrad: 3.0 6.0 5.6843418860808015e-12 -1.430511474609375e-05\n",
      "progress: 46 5.6843418860808015e-12\n",
      "\tgrad: 1.0 2.0 5.115907697472721e-13 -1.430511474609375e-06\n",
      "\tgrad: 2.0 4.0 2.0463630789890885e-12 -5.7220458984375e-06\n",
      "\tgrad: 3.0 6.0 3.637978807091713e-12 -1.1444091796875e-05\n",
      "progress: 47 3.637978807091713e-12\n",
      "\tgrad: 1.0 2.0 3.552713678800501e-13 -1.1920928955078125e-06\n",
      "\tgrad: 2.0 4.0 1.4210854715202004e-12 -4.76837158203125e-06\n",
      "\tgrad: 3.0 6.0 3.637978807091713e-12 -1.1444091796875e-05\n",
      "progress: 48 3.637978807091713e-12\n",
      "\tgrad: 1.0 2.0 2.2737367544323206e-13 -9.5367431640625e-07\n",
      "\tgrad: 2.0 4.0 9.094947017729282e-13 -3.814697265625e-06\n",
      "\tgrad: 3.0 6.0 2.0463630789890885e-12 -8.58306884765625e-06\n",
      "progress: 49 2.0463630789890885e-12\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 50 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 51 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 52 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 53 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 54 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 55 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 56 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 57 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 58 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 59 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 60 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 61 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 62 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 63 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 64 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 65 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 66 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 67 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 68 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 69 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 70 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 71 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 72 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 73 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 74 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 75 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 76 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 77 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 78 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 79 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 80 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 81 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 82 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 83 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 84 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 85 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 86 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 87 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 88 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 89 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 90 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 91 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 92 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 93 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 94 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 95 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 96 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 97 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 98 9.094947017729282e-13\n",
      "\tgrad: 1.0 2.0 1.2789769243681803e-13 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 5.115907697472721e-13 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 9.094947017729282e-13 -5.7220458984375e-06\n",
      "progress: 99 9.094947017729282e-13\n",
      "Predict (after training) 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]\n",
    "\n",
    "w = torch.Tensor([1.0])\n",
    "# 设置为True不会计算关于他的梯度\n",
    "w.requires_grad = True\n",
    "\n",
    "# 构建计算图\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)**2\n",
    "\n",
    "print('Predict (before training)',4,forward(4).item())\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        # 计算损失\n",
    "        l = loss(x,y)\n",
    "        # 反向传播——计算所有的梯度，保存到tensor的grad属性中\n",
    "        l.backward()\n",
    "        # .item()将tensor转换为python的标量\n",
    "\n",
    "        print('\\tgrad:',x,y,l.item(),w.grad.item())\n",
    "\n",
    "        # 更新权重\n",
    "        w.data -= 0.01 * w.grad.data\n",
    "        # 梯度清零\n",
    "        w.grad.data.zero_()\n",
    "    # 训练轮数\n",
    "    print('progress:',epoch,l.item())\n",
    "print('Predict (after training)',4,forward(4).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7386226",
   "metadata": {},
   "source": [
    "## 2、pytorch表示下面的计算图和反向传播过程\n",
    "<img src='https://gitee.com/zhang-junjie123/picture/raw/master/image/Capture_20240929_222225.jpg' style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbdc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training): 4,21.00\n",
      "\tgrad:x,y,w:2.00,w2:2.00,b:2.00\n",
      "\tgrad:x,y,w:22.88,w2:11.44,b:5.72\n",
      "\tgrad:x,y,w:77.05,w2:25.68,b:8.56\n",
      "progress: 0 loss: 18.32\n",
      "\tgrad:x,y,w:-1.15,w2:-1.15,b:-1.15\n",
      "\tgrad:x,y,w:-15.54,w2:-7.77,b:-3.88\n",
      "\tgrad:x,y,w:-30.43,w2:-10.14,b:-3.38\n",
      "progress: 1 loss: 2.86\n",
      "\tgrad:x,y,w:0.35,w2:0.35,b:0.35\n",
      "\tgrad:x,y,w:2.43,w2:1.21,b:0.61\n",
      "\tgrad:x,y,w:19.45,w2:6.48,b:2.16\n",
      "progress: 2 loss: 1.17\n",
      "\tgrad:x,y,w:-0.32,w2:-0.32,b:-0.32\n",
      "\tgrad:x,y,w:-5.85,w2:-2.92,b:-1.46\n",
      "\tgrad:x,y,w:-3.88,w2:-1.29,b:-0.43\n",
      "progress: 3 loss: 0.05\n",
      "\tgrad:x,y,w:0.01,w2:0.01,b:0.01\n",
      "\tgrad:x,y,w:-1.91,w2:-0.96,b:-0.48\n",
      "\tgrad:x,y,w:6.86,w2:2.29,b:0.76\n",
      "progress: 4 loss: 0.15\n",
      "\tgrad:x,y,w:-0.12,w2:-0.12,b:-0.12\n",
      "\tgrad:x,y,w:-3.66,w2:-1.83,b:-0.92\n",
      "\tgrad:x,y,w:1.75,w2:0.58,b:0.19\n",
      "progress: 5 loss: 0.01\n",
      "\tgrad:x,y,w:-0.03,w2:-0.03,b:-0.03\n",
      "\tgrad:x,y,w:-2.77,w2:-1.39,b:-0.69\n",
      "\tgrad:x,y,w:4.01,w2:1.34,b:0.45\n",
      "progress: 6 loss: 0.05\n",
      "\tgrad:x,y,w:-0.05,w2:-0.05,b:-0.05\n",
      "\tgrad:x,y,w:-3.12,w2:-1.56,b:-0.78\n",
      "\tgrad:x,y,w:2.85,w2:0.95,b:0.32\n",
      "progress: 7 loss: 0.03\n",
      "\tgrad:x,y,w:-0.02,w2:-0.02,b:-0.02\n",
      "\tgrad:x,y,w:-2.89,w2:-1.44,b:-0.72\n",
      "\tgrad:x,y,w:3.29,w2:1.10,b:0.37\n",
      "progress: 8 loss: 0.03\n",
      "\tgrad:x,y,w:-0.01,w2:-0.01,b:-0.01\n",
      "\tgrad:x,y,w:-2.92,w2:-1.46,b:-0.73\n",
      "\tgrad:x,y,w:2.99,w2:1.00,b:0.33\n",
      "progress: 9 loss: 0.03\n",
      "\tgrad:x,y,w:0.00,w2:0.00,b:0.00\n",
      "\tgrad:x,y,w:-2.84,w2:-1.42,b:-0.71\n",
      "\tgrad:x,y,w:3.04,w2:1.01,b:0.34\n",
      "progress: 10 loss: 0.03\n",
      "\tgrad:x,y,w:0.01,w2:0.01,b:0.01\n",
      "\tgrad:x,y,w:-2.82,w2:-1.41,b:-0.70\n",
      "\tgrad:x,y,w:2.93,w2:0.98,b:0.33\n",
      "progress: 11 loss: 0.03\n",
      "\tgrad:x,y,w:0.03,w2:0.03,b:0.03\n",
      "\tgrad:x,y,w:-2.77,w2:-1.38,b:-0.69\n",
      "\tgrad:x,y,w:2.89,w2:0.96,b:0.32\n",
      "progress: 12 loss: 0.03\n",
      "\tgrad:x,y,w:0.04,w2:0.04,b:0.04\n",
      "\tgrad:x,y,w:-2.73,w2:-1.37,b:-0.68\n",
      "\tgrad:x,y,w:2.82,w2:0.94,b:0.31\n",
      "progress: 13 loss: 0.02\n",
      "\tgrad:x,y,w:0.05,w2:0.05,b:0.05\n",
      "\tgrad:x,y,w:-2.69,w2:-1.35,b:-0.67\n",
      "\tgrad:x,y,w:2.78,w2:0.93,b:0.31\n",
      "progress: 14 loss: 0.02\n",
      "\tgrad:x,y,w:0.06,w2:0.06,b:0.06\n",
      "\tgrad:x,y,w:-2.66,w2:-1.33,b:-0.66\n",
      "\tgrad:x,y,w:2.72,w2:0.91,b:0.30\n",
      "progress: 15 loss: 0.02\n",
      "\tgrad:x,y,w:0.07,w2:0.07,b:0.07\n",
      "\tgrad:x,y,w:-2.62,w2:-1.31,b:-0.66\n",
      "\tgrad:x,y,w:2.67,w2:0.89,b:0.30\n",
      "progress: 16 loss: 0.02\n",
      "\tgrad:x,y,w:0.08,w2:0.08,b:0.08\n",
      "\tgrad:x,y,w:-2.59,w2:-1.29,b:-0.65\n",
      "\tgrad:x,y,w:2.62,w2:0.87,b:0.29\n",
      "progress: 17 loss: 0.02\n",
      "\tgrad:x,y,w:0.09,w2:0.09,b:0.09\n",
      "\tgrad:x,y,w:-2.56,w2:-1.28,b:-0.64\n",
      "\tgrad:x,y,w:2.58,w2:0.86,b:0.29\n",
      "progress: 18 loss: 0.02\n",
      "\tgrad:x,y,w:0.10,w2:0.10,b:0.10\n",
      "\tgrad:x,y,w:-2.53,w2:-1.26,b:-0.63\n",
      "\tgrad:x,y,w:2.53,w2:0.84,b:0.28\n",
      "progress: 19 loss: 0.02\n",
      "\tgrad:x,y,w:0.11,w2:0.11,b:0.11\n",
      "\tgrad:x,y,w:-2.50,w2:-1.25,b:-0.62\n",
      "\tgrad:x,y,w:2.49,w2:0.83,b:0.28\n",
      "progress: 20 loss: 0.02\n",
      "\tgrad:x,y,w:0.12,w2:0.12,b:0.12\n",
      "\tgrad:x,y,w:-2.47,w2:-1.23,b:-0.62\n",
      "\tgrad:x,y,w:2.45,w2:0.82,b:0.27\n",
      "progress: 21 loss: 0.02\n",
      "\tgrad:x,y,w:0.13,w2:0.13,b:0.13\n",
      "\tgrad:x,y,w:-2.44,w2:-1.22,b:-0.61\n",
      "\tgrad:x,y,w:2.41,w2:0.80,b:0.27\n",
      "progress: 22 loss: 0.02\n",
      "\tgrad:x,y,w:0.14,w2:0.14,b:0.14\n",
      "\tgrad:x,y,w:-2.41,w2:-1.21,b:-0.60\n",
      "\tgrad:x,y,w:2.37,w2:0.79,b:0.26\n",
      "progress: 23 loss: 0.02\n",
      "\tgrad:x,y,w:0.14,w2:0.14,b:0.14\n",
      "\tgrad:x,y,w:-2.39,w2:-1.19,b:-0.60\n",
      "\tgrad:x,y,w:2.34,w2:0.78,b:0.26\n",
      "progress: 24 loss: 0.02\n",
      "\tgrad:x,y,w:0.15,w2:0.15,b:0.15\n",
      "\tgrad:x,y,w:-2.36,w2:-1.18,b:-0.59\n",
      "\tgrad:x,y,w:2.30,w2:0.77,b:0.26\n",
      "progress: 25 loss: 0.02\n",
      "\tgrad:x,y,w:0.16,w2:0.16,b:0.16\n",
      "\tgrad:x,y,w:-2.34,w2:-1.17,b:-0.58\n",
      "\tgrad:x,y,w:2.27,w2:0.76,b:0.25\n",
      "progress: 26 loss: 0.02\n",
      "\tgrad:x,y,w:0.17,w2:0.17,b:0.17\n",
      "\tgrad:x,y,w:-2.32,w2:-1.16,b:-0.58\n",
      "\tgrad:x,y,w:2.23,w2:0.74,b:0.25\n",
      "progress: 27 loss: 0.02\n",
      "\tgrad:x,y,w:0.17,w2:0.17,b:0.17\n",
      "\tgrad:x,y,w:-2.29,w2:-1.15,b:-0.57\n",
      "\tgrad:x,y,w:2.20,w2:0.73,b:0.24\n",
      "progress: 28 loss: 0.01\n",
      "\tgrad:x,y,w:0.18,w2:0.18,b:0.18\n",
      "\tgrad:x,y,w:-2.27,w2:-1.14,b:-0.57\n",
      "\tgrad:x,y,w:2.17,w2:0.72,b:0.24\n",
      "progress: 29 loss: 0.01\n",
      "\tgrad:x,y,w:0.18,w2:0.18,b:0.18\n",
      "\tgrad:x,y,w:-2.25,w2:-1.13,b:-0.56\n",
      "\tgrad:x,y,w:2.14,w2:0.71,b:0.24\n",
      "progress: 30 loss: 0.01\n",
      "\tgrad:x,y,w:0.19,w2:0.19,b:0.19\n",
      "\tgrad:x,y,w:-2.23,w2:-1.12,b:-0.56\n",
      "\tgrad:x,y,w:2.11,w2:0.70,b:0.23\n",
      "progress: 31 loss: 0.01\n",
      "\tgrad:x,y,w:0.20,w2:0.20,b:0.20\n",
      "\tgrad:x,y,w:-2.21,w2:-1.11,b:-0.55\n",
      "\tgrad:x,y,w:2.09,w2:0.70,b:0.23\n",
      "progress: 32 loss: 0.01\n",
      "\tgrad:x,y,w:0.20,w2:0.20,b:0.20\n",
      "\tgrad:x,y,w:-2.19,w2:-1.10,b:-0.55\n",
      "\tgrad:x,y,w:2.06,w2:0.69,b:0.23\n",
      "progress: 33 loss: 0.01\n",
      "\tgrad:x,y,w:0.21,w2:0.21,b:0.21\n",
      "\tgrad:x,y,w:-2.18,w2:-1.09,b:-0.54\n",
      "\tgrad:x,y,w:2.04,w2:0.68,b:0.23\n",
      "progress: 34 loss: 0.01\n",
      "\tgrad:x,y,w:0.21,w2:0.21,b:0.21\n",
      "\tgrad:x,y,w:-2.16,w2:-1.08,b:-0.54\n",
      "\tgrad:x,y,w:2.01,w2:0.67,b:0.22\n",
      "progress: 35 loss: 0.01\n",
      "\tgrad:x,y,w:0.22,w2:0.22,b:0.22\n",
      "\tgrad:x,y,w:-2.14,w2:-1.07,b:-0.54\n",
      "\tgrad:x,y,w:1.99,w2:0.66,b:0.22\n",
      "progress: 36 loss: 0.01\n",
      "\tgrad:x,y,w:0.22,w2:0.22,b:0.22\n",
      "\tgrad:x,y,w:-2.13,w2:-1.06,b:-0.53\n",
      "\tgrad:x,y,w:1.97,w2:0.66,b:0.22\n",
      "progress: 37 loss: 0.01\n",
      "\tgrad:x,y,w:0.23,w2:0.23,b:0.23\n",
      "\tgrad:x,y,w:-2.11,w2:-1.06,b:-0.53\n",
      "\tgrad:x,y,w:1.95,w2:0.65,b:0.22\n",
      "progress: 38 loss: 0.01\n",
      "\tgrad:x,y,w:0.23,w2:0.23,b:0.23\n",
      "\tgrad:x,y,w:-2.10,w2:-1.05,b:-0.52\n",
      "\tgrad:x,y,w:1.93,w2:0.64,b:0.21\n",
      "progress: 39 loss: 0.01\n",
      "\tgrad:x,y,w:0.23,w2:0.23,b:0.23\n",
      "\tgrad:x,y,w:-2.08,w2:-1.04,b:-0.52\n",
      "\tgrad:x,y,w:1.91,w2:0.64,b:0.21\n",
      "progress: 40 loss: 0.01\n",
      "\tgrad:x,y,w:0.24,w2:0.24,b:0.24\n",
      "\tgrad:x,y,w:-2.07,w2:-1.03,b:-0.52\n",
      "\tgrad:x,y,w:1.89,w2:0.63,b:0.21\n",
      "progress: 41 loss: 0.01\n",
      "\tgrad:x,y,w:0.24,w2:0.24,b:0.24\n",
      "\tgrad:x,y,w:-2.06,w2:-1.03,b:-0.51\n",
      "\tgrad:x,y,w:1.87,w2:0.62,b:0.21\n",
      "progress: 42 loss: 0.01\n",
      "\tgrad:x,y,w:0.24,w2:0.24,b:0.24\n",
      "\tgrad:x,y,w:-2.04,w2:-1.02,b:-0.51\n",
      "\tgrad:x,y,w:1.85,w2:0.62,b:0.21\n",
      "progress: 43 loss: 0.01\n",
      "\tgrad:x,y,w:0.25,w2:0.25,b:0.25\n",
      "\tgrad:x,y,w:-2.03,w2:-1.02,b:-0.51\n",
      "\tgrad:x,y,w:1.83,w2:0.61,b:0.20\n",
      "progress: 44 loss: 0.01\n",
      "\tgrad:x,y,w:0.25,w2:0.25,b:0.25\n",
      "\tgrad:x,y,w:-2.02,w2:-1.01,b:-0.50\n",
      "\tgrad:x,y,w:1.82,w2:0.61,b:0.20\n",
      "progress: 45 loss: 0.01\n",
      "\tgrad:x,y,w:0.25,w2:0.25,b:0.25\n",
      "\tgrad:x,y,w:-2.01,w2:-1.00,b:-0.50\n",
      "\tgrad:x,y,w:1.80,w2:0.60,b:0.20\n",
      "progress: 46 loss: 0.01\n",
      "\tgrad:x,y,w:0.26,w2:0.26,b:0.26\n",
      "\tgrad:x,y,w:-2.00,w2:-1.00,b:-0.50\n",
      "\tgrad:x,y,w:1.79,w2:0.60,b:0.20\n",
      "progress: 47 loss: 0.01\n",
      "\tgrad:x,y,w:0.26,w2:0.26,b:0.26\n",
      "\tgrad:x,y,w:-1.99,w2:-0.99,b:-0.50\n",
      "\tgrad:x,y,w:1.77,w2:0.59,b:0.20\n",
      "progress: 48 loss: 0.01\n",
      "\tgrad:x,y,w:0.26,w2:0.26,b:0.26\n",
      "\tgrad:x,y,w:-1.98,w2:-0.99,b:-0.49\n",
      "\tgrad:x,y,w:1.76,w2:0.59,b:0.20\n",
      "progress: 49 loss: 0.01\n",
      "\tgrad:x,y,w:0.27,w2:0.27,b:0.27\n",
      "\tgrad:x,y,w:-1.97,w2:-0.98,b:-0.49\n",
      "\tgrad:x,y,w:1.74,w2:0.58,b:0.19\n",
      "progress: 50 loss: 0.01\n",
      "\tgrad:x,y,w:0.27,w2:0.27,b:0.27\n",
      "\tgrad:x,y,w:-1.96,w2:-0.98,b:-0.49\n",
      "\tgrad:x,y,w:1.73,w2:0.58,b:0.19\n",
      "progress: 51 loss: 0.01\n",
      "\tgrad:x,y,w:0.27,w2:0.27,b:0.27\n",
      "\tgrad:x,y,w:-1.95,w2:-0.97,b:-0.49\n",
      "\tgrad:x,y,w:1.72,w2:0.57,b:0.19\n",
      "progress: 52 loss: 0.01\n",
      "\tgrad:x,y,w:0.27,w2:0.27,b:0.27\n",
      "\tgrad:x,y,w:-1.94,w2:-0.97,b:-0.48\n",
      "\tgrad:x,y,w:1.71,w2:0.57,b:0.19\n",
      "progress: 53 loss: 0.01\n",
      "\tgrad:x,y,w:0.27,w2:0.27,b:0.27\n",
      "\tgrad:x,y,w:-1.93,w2:-0.97,b:-0.48\n",
      "\tgrad:x,y,w:1.69,w2:0.56,b:0.19\n",
      "progress: 54 loss: 0.01\n",
      "\tgrad:x,y,w:0.28,w2:0.28,b:0.28\n",
      "\tgrad:x,y,w:-1.92,w2:-0.96,b:-0.48\n",
      "\tgrad:x,y,w:1.68,w2:0.56,b:0.19\n",
      "progress: 55 loss: 0.01\n",
      "\tgrad:x,y,w:0.28,w2:0.28,b:0.28\n",
      "\tgrad:x,y,w:-1.91,w2:-0.96,b:-0.48\n",
      "\tgrad:x,y,w:1.67,w2:0.56,b:0.19\n",
      "progress: 56 loss: 0.01\n",
      "\tgrad:x,y,w:0.28,w2:0.28,b:0.28\n",
      "\tgrad:x,y,w:-1.91,w2:-0.95,b:-0.48\n",
      "\tgrad:x,y,w:1.66,w2:0.55,b:0.18\n",
      "progress: 57 loss: 0.01\n",
      "\tgrad:x,y,w:0.28,w2:0.28,b:0.28\n",
      "\tgrad:x,y,w:-1.90,w2:-0.95,b:-0.47\n",
      "\tgrad:x,y,w:1.65,w2:0.55,b:0.18\n",
      "progress: 58 loss: 0.01\n",
      "\tgrad:x,y,w:0.28,w2:0.28,b:0.28\n",
      "\tgrad:x,y,w:-1.89,w2:-0.95,b:-0.47\n",
      "\tgrad:x,y,w:1.64,w2:0.55,b:0.18\n",
      "progress: 59 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.88,w2:-0.94,b:-0.47\n",
      "\tgrad:x,y,w:1.63,w2:0.54,b:0.18\n",
      "progress: 60 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.88,w2:-0.94,b:-0.47\n",
      "\tgrad:x,y,w:1.62,w2:0.54,b:0.18\n",
      "progress: 61 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.87,w2:-0.94,b:-0.47\n",
      "\tgrad:x,y,w:1.61,w2:0.54,b:0.18\n",
      "progress: 62 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.86,w2:-0.93,b:-0.47\n",
      "\tgrad:x,y,w:1.60,w2:0.53,b:0.18\n",
      "progress: 63 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.86,w2:-0.93,b:-0.46\n",
      "\tgrad:x,y,w:1.60,w2:0.53,b:0.18\n",
      "progress: 64 loss: 0.01\n",
      "\tgrad:x,y,w:0.29,w2:0.29,b:0.29\n",
      "\tgrad:x,y,w:-1.85,w2:-0.93,b:-0.46\n",
      "\tgrad:x,y,w:1.59,w2:0.53,b:0.18\n",
      "progress: 65 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.85,w2:-0.92,b:-0.46\n",
      "\tgrad:x,y,w:1.58,w2:0.53,b:0.18\n",
      "progress: 66 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.84,w2:-0.92,b:-0.46\n",
      "\tgrad:x,y,w:1.57,w2:0.52,b:0.17\n",
      "progress: 67 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.84,w2:-0.92,b:-0.46\n",
      "\tgrad:x,y,w:1.57,w2:0.52,b:0.17\n",
      "progress: 68 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.83,w2:-0.92,b:-0.46\n",
      "\tgrad:x,y,w:1.56,w2:0.52,b:0.17\n",
      "progress: 69 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.83,w2:-0.91,b:-0.46\n",
      "\tgrad:x,y,w:1.55,w2:0.52,b:0.17\n",
      "progress: 70 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.82,w2:-0.91,b:-0.46\n",
      "\tgrad:x,y,w:1.55,w2:0.52,b:0.17\n",
      "progress: 71 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.82,w2:-0.91,b:-0.45\n",
      "\tgrad:x,y,w:1.54,w2:0.51,b:0.17\n",
      "progress: 72 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.81,w2:-0.91,b:-0.45\n",
      "\tgrad:x,y,w:1.53,w2:0.51,b:0.17\n",
      "progress: 73 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.81,w2:-0.90,b:-0.45\n",
      "\tgrad:x,y,w:1.53,w2:0.51,b:0.17\n",
      "progress: 74 loss: 0.01\n",
      "\tgrad:x,y,w:0.30,w2:0.30,b:0.30\n",
      "\tgrad:x,y,w:-1.80,w2:-0.90,b:-0.45\n",
      "\tgrad:x,y,w:1.52,w2:0.51,b:0.17\n",
      "progress: 75 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.80,w2:-0.90,b:-0.45\n",
      "\tgrad:x,y,w:1.52,w2:0.51,b:0.17\n",
      "progress: 76 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.80,w2:-0.90,b:-0.45\n",
      "\tgrad:x,y,w:1.51,w2:0.50,b:0.17\n",
      "progress: 77 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.79,w2:-0.90,b:-0.45\n",
      "\tgrad:x,y,w:1.51,w2:0.50,b:0.17\n",
      "progress: 78 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.79,w2:-0.89,b:-0.45\n",
      "\tgrad:x,y,w:1.50,w2:0.50,b:0.17\n",
      "progress: 79 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.78,w2:-0.89,b:-0.45\n",
      "\tgrad:x,y,w:1.50,w2:0.50,b:0.17\n",
      "progress: 80 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.78,w2:-0.89,b:-0.45\n",
      "\tgrad:x,y,w:1.49,w2:0.50,b:0.17\n",
      "progress: 81 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.78,w2:-0.89,b:-0.44\n",
      "\tgrad:x,y,w:1.49,w2:0.50,b:0.17\n",
      "progress: 82 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.77,w2:-0.89,b:-0.44\n",
      "\tgrad:x,y,w:1.48,w2:0.49,b:0.16\n",
      "progress: 83 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.77,w2:-0.89,b:-0.44\n",
      "\tgrad:x,y,w:1.48,w2:0.49,b:0.16\n",
      "progress: 84 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.77,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.48,w2:0.49,b:0.16\n",
      "progress: 85 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.76,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.47,w2:0.49,b:0.16\n",
      "progress: 86 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.76,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.47,w2:0.49,b:0.16\n",
      "progress: 87 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.76,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.46,w2:0.49,b:0.16\n",
      "progress: 88 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.75,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.46,w2:0.49,b:0.16\n",
      "progress: 89 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.75,w2:-0.88,b:-0.44\n",
      "\tgrad:x,y,w:1.46,w2:0.49,b:0.16\n",
      "progress: 90 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.75,w2:-0.87,b:-0.44\n",
      "\tgrad:x,y,w:1.45,w2:0.48,b:0.16\n",
      "progress: 91 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.75,w2:-0.87,b:-0.44\n",
      "\tgrad:x,y,w:1.45,w2:0.48,b:0.16\n",
      "progress: 92 loss: 0.01\n",
      "\tgrad:x,y,w:0.31,w2:0.31,b:0.31\n",
      "\tgrad:x,y,w:-1.74,w2:-0.87,b:-0.44\n",
      "\tgrad:x,y,w:1.45,w2:0.48,b:0.16\n",
      "progress: 93 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.74,w2:-0.87,b:-0.44\n",
      "\tgrad:x,y,w:1.44,w2:0.48,b:0.16\n",
      "progress: 94 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.74,w2:-0.87,b:-0.43\n",
      "\tgrad:x,y,w:1.44,w2:0.48,b:0.16\n",
      "progress: 95 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.74,w2:-0.87,b:-0.43\n",
      "\tgrad:x,y,w:1.44,w2:0.48,b:0.16\n",
      "progress: 96 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.73,w2:-0.87,b:-0.43\n",
      "\tgrad:x,y,w:1.44,w2:0.48,b:0.16\n",
      "progress: 97 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.73,w2:-0.87,b:-0.43\n",
      "\tgrad:x,y,w:1.43,w2:0.48,b:0.16\n",
      "progress: 98 loss: 0.01\n",
      "\tgrad:x,y,w:0.32,w2:0.32,b:0.32\n",
      "\tgrad:x,y,w:-1.73,w2:-0.86,b:-0.43\n",
      "\tgrad:x,y,w:1.43,w2:0.48,b:0.16\n",
      "progress: 99 loss: 0.01\n",
      "Predict (after training): 4,8.54\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_data = [1.0, 2.0,3.0]\n",
    "y_data = [2.0, 4.0,6.0]\n",
    "\n",
    "w = torch.Tensor([1.0])\n",
    "# 设置为True不会计算关于他的梯度，只有需要的时候才会计算\n",
    "w.requires_grad = True\n",
    "\n",
    "w2 = torch.Tensor([1.0])\n",
    "# 设置为True不会计算关于他的梯度，只有需要的时候才会计算\n",
    "w2.requires_grad = True\n",
    "\n",
    "b = torch.Tensor([1.0])\n",
    "# 设置为True不会计算关于他的梯度，只有需要的时候才会计算\n",
    "b.requires_grad = True\n",
    "\n",
    "# 构建计算图\n",
    "def forward(x):\n",
    "    return w * x**2 + w2 * x + b\n",
    "\n",
    "# 损失函数\n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y - y_pred) ** 2\n",
    "\n",
    "print(f'Predict (before training): 4,{forward(4).item():.2f}')\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        l = loss(x,y)\n",
    "        l.backward()\n",
    "        # 打印梯度\n",
    "        print(f'\\tgrad:x,y,w:{w.grad.item():.2f},w2:{w2.grad.item():.2f},b:{b.grad.item():.2f}')\n",
    "        # 更新权重\n",
    "        w.data -= 0.01 * w.grad.data\n",
    "        w2.data -= 0.01 * w2.grad.data\n",
    "        b.data -= 0.01 * b.grad.data\n",
    "\n",
    "        # 梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    print(f'progress: {epoch} loss: {l.item():.2f}')\n",
    "print(f'Predict (after training): 4,{forward(4).item():.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
