{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d963de2f",
   "metadata": {},
   "source": [
    "# GRPO + PPO算法demo\n",
    "\n",
    "## 一、整体结构说明\n",
    "1. **配置模块（Config）**：用于设置参数，其中包括选择算法（algo = \"ppo\" 或 \"grpo\"）。\n",
    "2. **损失函数（ppo_loss / grpo_loss）**：分别对应两种算法，接口一致，便于切换。\n",
    "3. **损失函数注册表（LOSS_FN_REGISTRY）**：根据配置信息cfg.algo自动选用对应的损失函数。\n",
    "4. **训练步骤（train_step）**：每步自动调用当前算法对应的loss函数进行优化。\n",
    "5. **主训练循环（main）**：训练主流程，只需在配置中更改cfg.algo即可实现算法的无缝切换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5de6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algo: grpo\n",
      "step 0: loss = -0.0000\n",
      "step 1: loss = -0.0000\n",
      "step 2: loss = -0.0000\n",
      "step 3: loss = -0.0000\n",
      "step 4: loss = -0.0000\n",
      "step 5: loss = -0.0000\n",
      "step 6: loss = -0.0000\n",
      "step 7: loss = 0.0000\n",
      "step 8: loss = -0.0000\n",
      "step 9: loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ======================\n",
    "# 1. 配置\n",
    "# ======================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    algo: str = \"ppo\"           # <<< 只改这一行: \"ppo\" 或 \"grpo\"\n",
    "    lr: float = 1e-4\n",
    "    clip_range: float = 0.2\n",
    "    beta_kl: float = 0.01       # GRPO 用\n",
    "    vf_coef: float = 0.5\n",
    "    ent_coef: float = 0.01\n",
    "    group_num: int = 4          # GRPO: 每个 prompt 有多少个候选 response\n",
    "    input_len: int = 8          # prompt 长度\n",
    "    seq_len: int = 32\n",
    "    vocab_size: int = 50257\n",
    "    hidden_size: int = 256\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2. 一个极简 policy 模型\n",
    "# ======================\n",
    "\n",
    "class TinyPolicy(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(cfg.vocab_size, cfg.hidden_size)\n",
    "        self.ln = nn.LayerNorm(cfg.hidden_size)\n",
    "        self.head = nn.Linear(cfg.hidden_size, cfg.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [bs, seq_len]\n",
    "        返回 logits: [bs, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        x = self.embed(input_ids)              # [bs, seq_len, hid]\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)                 # [bs, seq_len, vocab]\n",
    "        return logits\n",
    "\n",
    "    def log_prob(self, input_ids, actions):\n",
    "        \"\"\"\n",
    "        给定 input_ids，返回这些 action 的 log_prob，形状 [bs, seq_len]\n",
    "        actions: [bs, seq_len]\n",
    "        \"\"\"\n",
    "        logits = self.forward(input_ids)      # [bs, seq_len, vocab]\n",
    "        logprobs = logits.log_softmax(dim=-1)\n",
    "        return logprobs.gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3. PPO / GRPO Loss 实现（统一接口）\n",
    "# ======================\n",
    "\n",
    "def compute_mask(bs, seq_len, input_len, device, dtype):\n",
    "    mask = torch.zeros(bs, seq_len, device=device, dtype=dtype)\n",
    "    mask[:, input_len:] = 1.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def ppo_policy_loss(pi_logprob, pi_old_logprob, advantage, mask, clip_range):\n",
    "    \"\"\"\n",
    "    纯 policy 的 PPO loss（不含 value / entropy）\n",
    "    \"\"\"\n",
    "    bs, seq_len = pi_logprob.shape\n",
    "\n",
    "    ratio = torch.exp(pi_logprob - pi_old_logprob)            # [bs, seq_len]\n",
    "    ratio_clip = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "\n",
    "    advantage = advantage.view(bs, 1)                         # [bs, 1]\n",
    "    pg_unclipped = ratio * advantage\n",
    "    pg_clipped   = ratio_clip * advantage\n",
    "\n",
    "    pg_loss_token = -torch.minimum(pg_unclipped, pg_clipped)  # [bs, seq_len]\n",
    "    pg_loss_token = pg_loss_token * mask\n",
    "\n",
    "    valid_tokens = mask.sum().clamp_min(1.0)\n",
    "    pg_loss = pg_loss_token.sum() / valid_tokens\n",
    "    return pg_loss\n",
    "\n",
    "\n",
    "def grpo_kl(pi_logprob, pi_ref_logprob, mask):\n",
    "    \"\"\"\n",
    "    简单 KL 估计：E[ log π - log π_ref ]\n",
    "    返回样本级 [bs, 1]\n",
    "    \"\"\"\n",
    "    log_ratio = (pi_logprob - pi_ref_logprob) * mask          # [bs, seq_len]\n",
    "    valid_token = mask.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    kl_per_sample = log_ratio.sum(dim=1, keepdim=True) / valid_token\n",
    "    return kl_per_sample\n",
    "\n",
    "\n",
    "def grpo_loss(\n",
    "    pi_logprob,\n",
    "    pi_old_logprob,\n",
    "    pi_ref_logprob,\n",
    "    advantage,\n",
    "    mask,\n",
    "    len_oi,        # [bs]\n",
    "    group_num,\n",
    "    clip_range,\n",
    "    beta_kl,\n",
    "):\n",
    "    \"\"\"\n",
    "    GRPO-style loss:\n",
    "    - PPO clipped policy gradient\n",
    "    - minus beta * KL(π || π_ref)\n",
    "    - 只在 response token 上算（mask）\n",
    "    - 按 group / 长度归一化\n",
    "    \"\"\"\n",
    "    device = pi_logprob.device\n",
    "    dtype = pi_logprob.dtype\n",
    "    bs, seq_len = pi_logprob.shape\n",
    "\n",
    "    ratio = torch.exp(pi_logprob - pi_old_logprob)\n",
    "    ratio_clip = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "\n",
    "    advantage = advantage.view(bs, 1)\n",
    "    pg_unclipped = ratio * advantage\n",
    "    pg_clipped   = ratio_clip * advantage\n",
    "    policy_gradient = torch.minimum(pg_unclipped, pg_clipped) * mask  # [bs, seq_len]\n",
    "\n",
    "    # KL 部分（先按样本，再摊回 token）\n",
    "    kl_per_sample = grpo_kl(pi_logprob, pi_ref_logprob, mask)         # [bs, 1]\n",
    "    valid_token = mask.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    kl_token = (kl_per_sample / valid_token) * mask                   # [bs, seq_len]\n",
    "\n",
    "    # token 级目标\n",
    "    objective_token = policy_gradient - beta_kl * kl_token            # [bs, seq_len]\n",
    "\n",
    "    # 长度归一化\n",
    "    len_oi = len_oi.view(bs, 1).to(device=device, dtype=dtype).clamp_min(1.0)\n",
    "    loss_token = (-1.0 / group_num) * (1.0 / len_oi) * objective_token\n",
    "\n",
    "    loss = loss_token.sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 再包一层，把接口统一成一个函数\n",
    "def policy_loss_wrapper(\n",
    "    algo: str,\n",
    "    pi_logprob,\n",
    "    pi_old_logprob,\n",
    "    pi_ref_logprob,\n",
    "    advantage,\n",
    "    mask,\n",
    "    len_oi,\n",
    "    cfg: Config,\n",
    "):\n",
    "    \"\"\"\n",
    "    统一接口: 只改 cfg.algo = \"ppo\" / \"grpo\" 即可切换\n",
    "    \"\"\"\n",
    "    if algo == \"ppo\":\n",
    "        # PPO 不用 ref / len_oi / group_num\n",
    "        return ppo_policy_loss(pi_logprob, pi_old_logprob, advantage, mask, cfg.clip_range)\n",
    "\n",
    "    elif algo == \"grpo\":\n",
    "        return grpo_loss(\n",
    "            pi_logprob=pi_logprob,\n",
    "            pi_old_logprob=pi_old_logprob,\n",
    "            pi_ref_logprob=pi_ref_logprob,\n",
    "            advantage=advantage,\n",
    "            mask=mask,\n",
    "            len_oi=len_oi,\n",
    "            group_num=cfg.group_num,\n",
    "            clip_range=cfg.clip_range,\n",
    "            beta_kl=cfg.beta_kl,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algo: {algo}\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4. 假数据构造 & Advantage 计算示例\n",
    "# ======================\n",
    "\n",
    "def make_fake_batch(cfg: Config, bs: int):\n",
    "    \"\"\"\n",
    "    模拟一个 batch：\n",
    "    - input_ids: [bs, seq_len]\n",
    "    - actions:   [bs, seq_len]\n",
    "    - rewards:   [bs]  (这里只给一个 per-sample scalar reward)\n",
    "    \"\"\"\n",
    "    device = cfg.device\n",
    "    seq_len = cfg.seq_len\n",
    "    vocab = cfg.vocab_size\n",
    "\n",
    "    input_ids = torch.randint(0, vocab, (bs, seq_len), device=device)\n",
    "    actions   = torch.randint(0, vocab, (bs, seq_len), device=device)\n",
    "\n",
    "    # 简单点：随机 reward\n",
    "    rewards = torch.randn(bs, device=device)\n",
    "    return input_ids, actions, rewards\n",
    "\n",
    "\n",
    "def compute_advantage_from_reward(rewards: torch.Tensor):\n",
    "    \"\"\"\n",
    "    极简版 advantage: 直接用 reward - reward.mean()\n",
    "    实际使用时可以换成 GAE, baseline, critic 等。\n",
    "    \"\"\"\n",
    "    adv = rewards - rewards.mean()\n",
    "    return adv\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 5. 单次 train_step：只改 cfg.algo 即可切换\n",
    "# ======================\n",
    "\n",
    "def train_step(\n",
    "    cfg: Config,\n",
    "    policy: TinyPolicy,\n",
    "    ref_policy: TinyPolicy,   # PPO 可以不用 ref，GRPO 会用\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    input_ids,\n",
    "    actions,\n",
    "    rewards,\n",
    "):\n",
    "    \"\"\"\n",
    "    一次优化步骤：\n",
    "    - 用 ref_policy 估当前 batch 的 log_prob（相当于 behavior / ref）\n",
    "    - 复制一份作为 old_logprob（实际可缓存）\n",
    "    - 再用 policy 计算当前 log_prob\n",
    "    - 根据 cfg.algo 调用对应 loss\n",
    "    \"\"\"\n",
    "\n",
    "    device = cfg.device\n",
    "\n",
    "    # 不参与梯度的旧策略 & reference\n",
    "    with torch.no_grad():\n",
    "        pi_old_logprob = policy.log_prob(input_ids, actions)      # 这里当作 old（简单）\n",
    "        pi_ref_logprob = ref_policy.log_prob(input_ids, actions)  # GRPO 用\n",
    "\n",
    "    # 当前策略 log_prob\n",
    "    pi_logprob = policy.log_prob(input_ids, actions)\n",
    "\n",
    "    # advantage\n",
    "    advantage = compute_advantage_from_reward(rewards)            # [bs]\n",
    "\n",
    "    bs, seq_len = pi_logprob.shape\n",
    "    mask = compute_mask(bs, seq_len, cfg.input_len, device, pi_logprob.dtype)\n",
    "\n",
    "    # 这里假设每条样本有效 response 长度都一样\n",
    "    len_oi = torch.full((bs,), seq_len - cfg.input_len, device=device, dtype=torch.long)\n",
    "\n",
    "    # ✅ 只改这一行 cfg.algo，即可 PPO <-> GRPO\n",
    "    loss = policy_loss_wrapper(\n",
    "        algo=cfg.algo,\n",
    "        pi_logprob=pi_logprob,\n",
    "        pi_old_logprob=pi_old_logprob,\n",
    "        pi_ref_logprob=pi_ref_logprob,\n",
    "        advantage=advantage,\n",
    "        mask=mask,\n",
    "        len_oi=len_oi,\n",
    "        cfg=cfg,\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 6. 主训练循环\n",
    "# ======================\n",
    "\n",
    "def main():\n",
    "    # 切换算法\n",
    "    cfg = Config()\n",
    "    cfg.algo = \"grpo\"   # 或 \"ppo\"\n",
    "    print(\"Using algo:\", cfg.algo)\n",
    "\n",
    "    policy = TinyPolicy(cfg).to(cfg.device)\n",
    "    ref_policy = TinyPolicy(cfg).to(cfg.device)\n",
    "    # 通常 ref_policy 冻结参数，这里不需要 optimizer\n",
    "    ref_policy.load_state_dict(policy.state_dict())  # 初始化一样\n",
    "\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=cfg.lr)\n",
    "\n",
    "    # 假设总 batch size = group_num * num_prompts_per_batch\n",
    "    num_prompts_per_batch = 2\n",
    "    bs = cfg.group_num * num_prompts_per_batch\n",
    "\n",
    "    for step in range(10):\n",
    "        input_ids, actions, rewards = make_fake_batch(cfg, bs)\n",
    "        loss = train_step(\n",
    "            cfg=cfg,\n",
    "            policy=policy,\n",
    "            ref_policy=ref_policy,\n",
    "            optimizer=optimizer,\n",
    "            input_ids=input_ids,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "        )\n",
    "        print(f\"step {step}: loss = {loss:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
