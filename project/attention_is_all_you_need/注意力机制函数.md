
## 1、掩码矩阵的生成
```python
def generate_mask(self, src, tgt):
“”“
功能：生成源序列（src）和目标序列（tgt）的掩码，用于 Transformer 模型的注意力机制。
输入参数：
  src：源序列（编码器输入），形状通常为 (batch_size, src_seq_len)。
  tgt：目标序列（解码器输入），形状通常为 (batch_size, tgt_seq_len)。
输出：
  src_mask：源序列的掩码，用于屏蔽填充位置。
  tgt_mask：目标序列的掩码，用于屏蔽未来信息和填充位置。
”“”
	# 生成源序列掩码和目标序列掩码
	# (src != 0)：生成一个布尔张量，标记 src 中非填充位置（假设填充标记为 0）。
	# .unsqueeze(1).unsqueeze(2)：为张量增加两个维度，使其形状变为 (batch_size, 1, 1, src_seq_len)，以便与注意力权重广播。
    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
  # (tgt != 0)：生成一个布尔张量，标记 tgt 中非填充位置。
  # .unsqueeze(1).unsqueeze(3)：为张量增加两个维度，使其形状变为 (batch_size, 1, tgt_seq_len, 1)，用于后续与 nopeak_mask 结合。
tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)
    seq_length = tgt.size(1)
    # torch.ones(1, seq_length, seq_length)：创建一个全 1 的上三角矩阵。
    # torch.triu(..., diagonal=1)：提取上三角部分（不包括对角线），其余位置为 0。
    # 1 - ...：将上三角部分变为 0，其余位置变为 1。
    # .bool()：将张量转换为布尔类型。
    # nopeak_mask：形状为 (1, seq_length, seq_length)，用于屏蔽未来位置。
    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()
    # 结合目标序列掩码和未来信息掩码
    tgt_mask = tgt_mask & nopeak_mask
    # &：按位与操作，将填充位置和未来位置都屏蔽掉。
		# 最终 tgt_mask 的形状为 (batch_size, 1, tgt_seq_len, tgt_seq_len)。
    return src_mask, tgt_mask
```