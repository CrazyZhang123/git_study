{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0519a06",
   "metadata": {},
   "source": [
    "# 手写transformer decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d686d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38536810831500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math,torch\n",
    "\n",
    "torch.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5afbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math,torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self,hidden_dim,head_num,attn_dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_num = head_num\n",
    "        # 每个头的维度\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        # qkv,o映射+drop和laynorm\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout_rate)\n",
    "        self.attn_ln = nn.LayerNorm(hidden_dim,eps=0.00001)\n",
    "\n",
    "        # ffn\n",
    "        self.up_proj = nn.Linear(hidden_dim,hidden_dim*4)\n",
    "        self.ffn_act = nn.ReLU()\n",
    "        self.down_proj = nn.Linear(hidden_dim*4,hidden_dim)\n",
    "        self.ffn_dropout = nn.Dropout(attn_dropout_rate)\n",
    "        self.ffn_ln = nn.LayerNorm(hidden_dim,eps=0.00001)\n",
    "\n",
    "    def attention_output(self,q,k,v,attention_mask):\n",
    "         # 注意力分数\n",
    "        attn_score = torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # 掩码矩阵\n",
    "        if attention_mask is not None:\n",
    "            # 下三角矩阵，解码需要\n",
    "            attention_mask = attention_mask.tril()\n",
    "            attn_score = attn_score.masked_fill(\n",
    "                attention_mask==0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        else: # 没有padding，也就是没有attention_mask\n",
    "            # 下三角来自attention_score\n",
    "            attention_mask = torch.ones_like(attn_score).tril()\n",
    "            attn_score = attn_score.masked_fill(\n",
    "                attention_mask==0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attn_weight = torch.softmax(attn_score,dim=-1)\n",
    "        attn_weight = self.attn_dropout(attn_weight)\n",
    "        # output [batch_size,head_num,seq_len,head_dim]\n",
    "    \n",
    "        output = attn_weight@v\n",
    "        \n",
    "        # concat head\n",
    "        # output [batch_size,seq_len,head_num,head_dim]\n",
    "        output = output.transpose(1,2).contiguous()\n",
    "       \n",
    "        # output [batch_size,seq_len,head_num * head_dim]\n",
    "        # 传进来的q是转置过seq_len和head_num的，所以直接用输出的前两个维度\n",
    "        # batch,_,seq,_ = q.size()\n",
    "        batch,seq,_,_ = output.size()\n",
    "        output = output.view(batch,seq,-1)\n",
    "\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def mha(self,x,attention_mask=None):\n",
    "        # x [batch_size,seq_len,hidden_dim]\n",
    "        batch,seq,_ = x.size()\n",
    "        q = self.q_proj(x).view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        \n",
    "        k = self.k_proj(x).view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        v = self.v_proj(x).view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "\n",
    "        output = self.attention_output(q,k,v,attention_mask)\n",
    "        return self.attn_ln(output+x)\n",
    "\n",
    "    def ffn(self,x):\n",
    "        up = self.up_proj(x)\n",
    "        act = self.ffn_act(up)\n",
    "        down = self.down_proj(act)\n",
    "\n",
    "        # dropout\n",
    "        down = self.ffn_dropout(down)\n",
    "        # add + layernorm\n",
    "        return self.ffn_ln(down + x)\n",
    "\n",
    "    def forward(self,x,attention_mask=None):\n",
    "        x = self.mha(x,attention_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "# x = torch.rand(3, 4, 64)\n",
    "# net = SimpleDecoderLayer(64, 8)\n",
    "# mask = (\n",
    "#     torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "#     .unsqueeze(1)\n",
    "#     .unsqueeze(2)\n",
    "#     .repeat(1, 8, 4, 1)\n",
    "# )\n",
    "\n",
    "# net(x, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddab55",
   "metadata": {},
   "source": [
    "## chaofa用代码打点酱油 手写版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4636, 0.5364, 0.0000, 0.0000],\n",
      "          [0.3056, 0.3526, 0.3418, 0.0000],\n",
      "          [0.2186, 0.2720, 0.2724, 0.2370]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4781, 0.5219, 0.0000, 0.0000],\n",
      "          [0.3292, 0.3364, 0.3344, 0.0000],\n",
      "          [0.2520, 0.2495, 0.2418, 0.2566]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4706, 0.5294, 0.0000, 0.0000],\n",
      "          [0.3260, 0.3509, 0.3231, 0.0000],\n",
      "          [0.2512, 0.2583, 0.2446, 0.2460]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5406, 0.4594, 0.0000, 0.0000],\n",
      "          [0.3464, 0.3334, 0.3203, 0.0000],\n",
      "          [0.2658, 0.2430, 0.2423, 0.2489]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5143, 0.4857, 0.0000, 0.0000],\n",
      "          [0.3335, 0.3027, 0.3638, 0.0000],\n",
      "          [0.2503, 0.2514, 0.2570, 0.2414]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4990, 0.5010, 0.0000, 0.0000],\n",
      "          [0.3308, 0.3290, 0.3401, 0.0000],\n",
      "          [0.2246, 0.2428, 0.2548, 0.2778]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5165, 0.4835, 0.0000, 0.0000],\n",
      "          [0.3366, 0.3207, 0.3427, 0.0000],\n",
      "          [0.2465, 0.2506, 0.2530, 0.2500]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4659, 0.5341, 0.0000, 0.0000],\n",
      "          [0.3169, 0.3460, 0.3371, 0.0000],\n",
      "          [0.2255, 0.2647, 0.2557, 0.2541]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4702, 0.5298, 0.0000, 0.0000],\n",
      "          [0.4687, 0.5313, 0.0000, 0.0000],\n",
      "          [0.4718, 0.5282, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5037, 0.4963, 0.0000, 0.0000],\n",
      "          [0.5044, 0.4956, 0.0000, 0.0000],\n",
      "          [0.4948, 0.5052, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5410, 0.4590, 0.0000, 0.0000],\n",
      "          [0.5359, 0.4641, 0.0000, 0.0000],\n",
      "          [0.5473, 0.4527, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5239, 0.4761, 0.0000, 0.0000],\n",
      "          [0.5207, 0.4793, 0.0000, 0.0000],\n",
      "          [0.5428, 0.4572, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5046, 0.4954, 0.0000, 0.0000],\n",
      "          [0.5123, 0.4877, 0.0000, 0.0000],\n",
      "          [0.5133, 0.4867, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5200, 0.4800, 0.0000, 0.0000],\n",
      "          [0.4985, 0.5015, 0.0000, 0.0000],\n",
      "          [0.5035, 0.4965, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5057, 0.4943, 0.0000, 0.0000],\n",
      "          [0.4933, 0.5067, 0.0000, 0.0000],\n",
      "          [0.4945, 0.5055, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5183, 0.4817, 0.0000, 0.0000],\n",
      "          [0.5073, 0.4927, 0.0000, 0.0000],\n",
      "          [0.5124, 0.4876, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5098, 0.4902, 0.0000, 0.0000],\n",
      "          [0.3433, 0.3329, 0.3239, 0.0000],\n",
      "          [0.3402, 0.3358, 0.3240, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5072, 0.4928, 0.0000, 0.0000],\n",
      "          [0.3418, 0.3287, 0.3295, 0.0000],\n",
      "          [0.3242, 0.3378, 0.3380, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4952, 0.5048, 0.0000, 0.0000],\n",
      "          [0.3296, 0.3466, 0.3239, 0.0000],\n",
      "          [0.3221, 0.3302, 0.3477, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4887, 0.5113, 0.0000, 0.0000],\n",
      "          [0.3294, 0.3434, 0.3271, 0.0000],\n",
      "          [0.3328, 0.3408, 0.3265, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5311, 0.4689, 0.0000, 0.0000],\n",
      "          [0.3478, 0.3178, 0.3345, 0.0000],\n",
      "          [0.3430, 0.3237, 0.3334, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4755, 0.5245, 0.0000, 0.0000],\n",
      "          [0.3237, 0.3533, 0.3230, 0.0000],\n",
      "          [0.3180, 0.3499, 0.3320, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5175, 0.4825, 0.0000, 0.0000],\n",
      "          [0.3364, 0.3247, 0.3388, 0.0000],\n",
      "          [0.3221, 0.3235, 0.3544, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000, 0.0000],\n",
      "          [0.3209, 0.3570, 0.3221, 0.0000],\n",
      "          [0.3197, 0.3552, 0.3250, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# 写一个 Block\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主意有 残差链接\n",
    "        # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性）\n",
    "        # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1\n",
    "        # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$\n",
    "        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.drop_att = nn.Dropout(self.dropout)\n",
    "\n",
    "        # for ffn 准备\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "        self.drop_ffn = nn.Dropout(self.dropout)\n",
    "\n",
    "    def attention_output(self, query, key, value, attention_mask=None):\n",
    "        # 计算两者相关性\n",
    "        key = key.transpose(2, 3)  # (batch, num_head, head_dim, seq)\n",
    "        att_weight = torch.matmul(query, key) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # attention mask 进行依次调整；变成 causal_attention\n",
    "        if attention_mask is not None:\n",
    "            # 变成下三角矩阵\n",
    "            attention_mask = attention_mask.tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "        else:\n",
    "            # 人工构造一个下三角的 attention mask\n",
    "            attention_mask = torch.ones_like(att_weight).tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        print(att_weight)\n",
    "\n",
    "        att_weight = self.drop_att(att_weight)\n",
    "\n",
    "        mid_output = torch.matmul(att_weight, value)\n",
    "        # mid_output shape is: (batch, nums_head, seq, head_dim)\n",
    "\n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "        batch, seq, _, _ = mid_output.size()\n",
    "        mid_output = mid_output.view(batch, seq, -1)\n",
    "        output = self.o_proj(mid_output)\n",
    "        return output\n",
    "\n",
    "    def attention_block(self, X, attention_mask=None):\n",
    "        batch, seq, _ = X.size()\n",
    "        query = self.q_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        key = self.k_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        value = self.v_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "\n",
    "        output = self.attention_output(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return self.layernorm_att(X + output)\n",
    "\n",
    "    def ffn_block(self, X):\n",
    "        up = self.act_fn(\n",
    "            self.up_proj(X),\n",
    "        )\n",
    "        down = self.down_proj(up)\n",
    "\n",
    "        # 执行 dropout\n",
    "        down = self.drop_ffn(down)\n",
    "\n",
    "        # 进行 norm 操作\n",
    "        return self.layernorm_ffn(X + down)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X 一般假设是已经经过 embedding 的输入， (batch, seq, hidden_dim)\n",
    "        # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略\n",
    "        # shape 一般是： (batch, nums_head, seq)\n",
    "\n",
    "        att_output = self.attention_block(X, attention_mask=attention_mask)\n",
    "        ffn_output = self.ffn_block(att_output)\n",
    "        return ffn_output\n",
    "\n",
    "\n",
    "# 测试\n",
    "\n",
    "# x = torch.rand(3, 4, 64)\n",
    "# net = SimpleDecoder(64, 8)\n",
    "# mask = (\n",
    "#     torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "#     .unsqueeze(1)\n",
    "#     .unsqueeze(2)\n",
    "#     .repeat(1, 8, 4, 1)\n",
    "# )\n",
    "\n",
    "# net(x, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89547ef8",
   "metadata": {},
   "source": [
    "### 对比两个代码\n",
    "\n",
    "差异为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410ef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4997, 0.5003, 0.0000, 0.0000],\n",
      "          [0.3383, 0.3257, 0.3360, 0.0000],\n",
      "          [0.2631, 0.2395, 0.2341, 0.2633]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5132, 0.4868, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3299, 0.3315, 0.0000],\n",
      "          [0.2562, 0.2445, 0.2496, 0.2497]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4922, 0.5078, 0.0000, 0.0000],\n",
      "          [0.3637, 0.3182, 0.3181, 0.0000],\n",
      "          [0.2501, 0.2442, 0.2310, 0.2747]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4938, 0.5062, 0.0000, 0.0000],\n",
      "          [0.3514, 0.3171, 0.3316, 0.0000],\n",
      "          [0.2508, 0.2479, 0.2487, 0.2526]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5050, 0.4950, 0.0000, 0.0000],\n",
      "          [0.3271, 0.3270, 0.3459, 0.0000],\n",
      "          [0.2438, 0.2399, 0.2589, 0.2574]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4868, 0.5132, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3261, 0.3405, 0.0000],\n",
      "          [0.2390, 0.2543, 0.2483, 0.2584]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4918, 0.5082, 0.0000, 0.0000],\n",
      "          [0.3270, 0.3374, 0.3356, 0.0000],\n",
      "          [0.2359, 0.2592, 0.2434, 0.2615]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4603, 0.5397, 0.0000, 0.0000],\n",
      "          [0.3381, 0.3610, 0.3009, 0.0000],\n",
      "          [0.2661, 0.2574, 0.2320, 0.2445]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4815, 0.5185, 0.0000, 0.0000],\n",
      "          [0.5037, 0.4963, 0.0000, 0.0000],\n",
      "          [0.5148, 0.4852, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5192, 0.4808, 0.0000, 0.0000],\n",
      "          [0.5109, 0.4891, 0.0000, 0.0000],\n",
      "          [0.5108, 0.4892, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5266, 0.4734, 0.0000, 0.0000],\n",
      "          [0.5115, 0.4885, 0.0000, 0.0000],\n",
      "          [0.5106, 0.4894, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4932, 0.5068, 0.0000, 0.0000],\n",
      "          [0.4977, 0.5023, 0.0000, 0.0000],\n",
      "          [0.4985, 0.5015, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5010, 0.4990, 0.0000, 0.0000],\n",
      "          [0.4943, 0.5057, 0.0000, 0.0000],\n",
      "          [0.4959, 0.5041, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5161, 0.4839, 0.0000, 0.0000],\n",
      "          [0.5052, 0.4948, 0.0000, 0.0000],\n",
      "          [0.5119, 0.4881, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5015, 0.4985, 0.0000, 0.0000],\n",
      "          [0.4974, 0.5026, 0.0000, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4951, 0.5049, 0.0000, 0.0000],\n",
      "          [0.4987, 0.5013, 0.0000, 0.0000],\n",
      "          [0.5100, 0.4900, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4847, 0.5153, 0.0000, 0.0000],\n",
      "          [0.3276, 0.3409, 0.3316, 0.0000],\n",
      "          [0.3245, 0.3412, 0.3343, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5309, 0.4691, 0.0000, 0.0000],\n",
      "          [0.3473, 0.3190, 0.3337, 0.0000],\n",
      "          [0.3799, 0.3294, 0.2907, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5091, 0.4909, 0.0000, 0.0000],\n",
      "          [0.3406, 0.3330, 0.3264, 0.0000],\n",
      "          [0.3447, 0.3300, 0.3253, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4917, 0.5083, 0.0000, 0.0000],\n",
      "          [0.3328, 0.3398, 0.3274, 0.0000],\n",
      "          [0.3483, 0.3223, 0.3294, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5073, 0.4927, 0.0000, 0.0000],\n",
      "          [0.3385, 0.3438, 0.3177, 0.0000],\n",
      "          [0.3347, 0.3384, 0.3269, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5004, 0.4996, 0.0000, 0.0000],\n",
      "          [0.3299, 0.3228, 0.3473, 0.0000],\n",
      "          [0.3235, 0.3302, 0.3463, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5063, 0.4937, 0.0000, 0.0000],\n",
      "          [0.3472, 0.3268, 0.3261, 0.0000],\n",
      "          [0.3377, 0.3317, 0.3306, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5385, 0.4615, 0.0000, 0.0000],\n",
      "          [0.3767, 0.3071, 0.3161, 0.0000],\n",
      "          [0.3612, 0.3165, 0.3222, 0.0000]]]])\n",
      "输出是否相同： True\n",
      "输出差异： tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import random, numpy as np\n",
    "\n",
    "# ========== 固定随机种子 ==========\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "# ========== 输入与mask ==========\n",
    "x = torch.rand(3, 4, 64)\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "\n",
    "# ========== 初始化模型 ==========\n",
    "\n",
    "\n",
    "set_seed(1234)\n",
    "net1 = SimpleDecoderLayer(64, 8)\n",
    "set_seed(1234)\n",
    "net2 = SimpleDecoder(64, 8)\n",
    "# Dropout 一定要关掉（或固定随机数）使用net.eval() 模式\n",
    "net1.eval()\n",
    "net2.eval()\n",
    "\n",
    "# ========== 验证输出 ==========\n",
    "with torch.no_grad():\n",
    "    out1 = net1(x, mask)\n",
    "    out2 = net2(x, mask)\n",
    "\n",
    "print(\"输出是否相同：\", torch.allclose(out1, out2, atol=1e-6))\n",
    "print(\"输出差异：\", torch.abs(out1 - out2).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378c0d5",
   "metadata": {},
   "source": [
    "# 完整Decoder层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9cece43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0493, 0.0713, 0.1049, 0.1347, 0.1210, 0.0952, 0.0727, 0.0426,\n",
       "          0.0845, 0.0876, 0.0280, 0.1082],\n",
       "         [0.0637, 0.0771, 0.1638, 0.1294, 0.0771, 0.0827, 0.0429, 0.0499,\n",
       "          0.0515, 0.0619, 0.1050, 0.0950],\n",
       "         [0.0520, 0.0907, 0.0710, 0.1408, 0.0933, 0.1512, 0.0784, 0.0479,\n",
       "          0.0252, 0.1195, 0.0292, 0.1007],\n",
       "         [0.1514, 0.1203, 0.0526, 0.0758, 0.0534, 0.0576, 0.0628, 0.1585,\n",
       "          0.0442, 0.1429, 0.0401, 0.0404]],\n",
       "\n",
       "        [[0.0508, 0.0223, 0.1029, 0.2019, 0.1459, 0.0300, 0.0330, 0.0841,\n",
       "          0.0943, 0.1302, 0.0474, 0.0573],\n",
       "         [0.1275, 0.0979, 0.0654, 0.0678, 0.0346, 0.0635, 0.0599, 0.1868,\n",
       "          0.0745, 0.1091, 0.0613, 0.0516],\n",
       "         [0.0632, 0.0465, 0.0421, 0.1633, 0.0864, 0.0526, 0.1039, 0.1056,\n",
       "          0.0694, 0.0764, 0.1070, 0.0836],\n",
       "         [0.0586, 0.0563, 0.0470, 0.0998, 0.1076, 0.2248, 0.0673, 0.0567,\n",
       "          0.0461, 0.1066, 0.0428, 0.0863]],\n",
       "\n",
       "        [[0.0249, 0.0410, 0.1306, 0.2135, 0.0965, 0.0464, 0.0271, 0.0926,\n",
       "          0.0815, 0.0414, 0.1108, 0.0937],\n",
       "         [0.0236, 0.0290, 0.0627, 0.2283, 0.0981, 0.0365, 0.1203, 0.1189,\n",
       "          0.0537, 0.0554, 0.0865, 0.0871],\n",
       "         [0.0377, 0.0585, 0.0621, 0.1173, 0.1309, 0.1874, 0.0804, 0.0772,\n",
       "          0.0298, 0.0898, 0.0275, 0.1015],\n",
       "         [0.0704, 0.1049, 0.0865, 0.0851, 0.0662, 0.0546, 0.0802, 0.1847,\n",
       "          0.0535, 0.1432, 0.0275, 0.0432]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [   # hidden_dim,head_num 64,8\n",
    "                SimpleDecoderLayer(64,8) for i in range(5)\n",
    "            ]\n",
    "        )\n",
    "        # hidden_dim 64\n",
    "        self.emb = nn.Embedding(12,64)\n",
    "        self.out = nn.Linear(64,12)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        # x (b,s)\n",
    "        x = self.emb(x)\n",
    "        for i,l in enumerate(self.layer_list):\n",
    "            x = l(x,mask)\n",
    "        print(x.shape)\n",
    "        output = self.out(x)\n",
    "        return torch.softmax(output,dim=-1)\n",
    "\n",
    "# 完整测试代码\n",
    "x = torch.randint(low=0,high=12,size=(3,4))\n",
    "\n",
    "net = Decoder()\n",
    "mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1,1,1,1],\n",
    "            [1,1,0,0],\n",
    "            [1,1,1,0]\n",
    "        ]\n",
    "    )\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1,8,4,1) # head_num,seq_len: 8,4\n",
    "\n",
    ")\n",
    "\n",
    "net(x,mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
