
## 1ã€åŠ è½½dataset
åŠ è½½dataset = load_dataset("truthfulqa/truthful_qa", "multiple_choice")\['validation'\]

ç„¶åtokenized_tqa ->(Q:{} A:{})æ ¼å¼åŒ–ï¼Œç»è¿‡tokenizerï¼Œæ”¶é›†promptså’Œlabels
> def format_truthfulqa(question, choice):
    return f"Q: {question} A: {choice}"

## 2ã€åˆ›å»ºæ”¶é›†å™¨collectorå’ŒIntervenableModel
åˆ›å»ºæ”¶é›†æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰ æ³¨æ„åŠ›å¤´æ¿€æ´»å€¼çš„**æ”¶é›†å™¨collector**,åˆ›å»ºå¹²é¢„æ¨¡å‹ã€‚
- éå†æ¯ä¸€å±‚çš„ç¼–å·ï¼ˆ`layer`ï¼‰ã€‚
- ä¸ºæ¯å±‚åˆ›å»ºä¸€ä¸ªÂ `Collector`Â å¯¹è±¡ï¼Œå®ƒæ˜¯ä¸€ä¸ª**å¯è°ƒç”¨å¯¹è±¡**ï¼ˆå³å®ç°äº†Â `__call__`Â æ–¹æ³•çš„ç±»ï¼‰ã€‚
- ç„¶åæ„å»ºä¸€ä¸ªÂ `pv_config`Â é…ç½®ï¼š
    - `"component"`Â æŒ‡å‘æ¨¡å‹ä¸­è¦â€œæ³¨å…¥â€çš„å…·ä½“æ¨¡å—ï¼ˆåœ¨è¿™é‡Œæ˜¯ self-attn çš„ output projection çš„è¾“å…¥ï¼‰ã€‚
    - `"intervention"`Â æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¡¨ç¤ºå½“è¯¥ç»„ä»¶æ‰§è¡Œæ—¶ï¼Œè¯¥å‡½æ•°ä¼šè¢«è°ƒç”¨ã€‚
- ä¸€ä¸ªå…¸å‹çš„Â **â€œé’©å­ï¼ˆhookï¼‰æ³¨å…¥â€æ¨¡å¼**ï¼Œç”¨äº**åœ¨æ¨¡å‹å‰å‘ä¼ æ’­æ—¶æ‹¦æˆªä¸­é—´å±‚æ¿€æ´»**ã€‚
è¯¦è§[[hook é’©å­]]
ğŸ’¡è¿™å…¶å®æ˜¯ä¸ºÂ **`IntervenableModel`**Â å‡†å¤‡é…ç½®ï¼Œç”¨æ¥å‘Šè¯‰å®ƒâ€œåœ¨å“ªä¸€å±‚æ³¨å…¥ä»€ä¹ˆå‡½æ•°â€ã€‚

>    for layer in range(model.config.num_hidden_layers): 
        collector = Collector(multiplier=0, head=-1) \#head=-1 to collect all head activations, multiplier doens't matter
        collectors.append(collector)
        pv_config.append({
            "component": f"model.layers[{layer}].self_attn.o_proj.input",
            "intervention": wrapper(collector),
        })
    collected_model = pv.IntervenableModel(pv_config, model)

**wrapperå‡½æ•°**
- å®ƒåªæ˜¯è¿”å›äº†ä¸€ä¸ª**é—­åŒ…ï¼ˆclosureï¼‰**ã€‚
- `intervener`Â æ˜¯ä¼ å…¥çš„Â `Collector`Â å®ä¾‹ã€‚
- æ‰€ä»¥Â `wrapped`Â å…¶å®å°±æ˜¯è°ƒç”¨Â `Collector.__call__`ã€‚
- å³ `wrapper(collector) <=> lambda *a, **kw: collector(*a, **kw)`

**ä¸ºä»€ä¹ˆè¦æœ‰?**
==å› ä¸ºæœ‰çš„æ¡†æ¶ï¼ˆæ¯”å¦‚Â `patched-vision`ã€`TransformerLens`ã€`HookedTransformer`ï¼‰è¦æ±‚ hook å¿…é¡»æ˜¯**çº¯å‡½æ•°**ï¼ˆä¸å¸¦çŠ¶æ€çš„å‡½æ•°ï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼ ç±»å¯¹è±¡ã€‚  
é€šè¿‡Â `wrapper`ï¼Œå¯ä»¥ç¡®ä¿è¿”å›çš„å¯¹è±¡æ˜¯ä¸€ä¸ªå¯è°ƒç”¨å‡½æ•°ã€‚==
```
def wrapper(intervener):
    def wrapped(*args, **kwargs):
        return intervener(*args, **kwargs)
    return wrapped

```

### 3ã€è·å–æ¿€æ´»å€¼
- éå†å‰é¢æ”¶é›†çš„tokenizeråŒ–çš„æç¤ºè¯
- è°ƒç”¨get_llama_activations_pyveneæ”¶é›†æ¿€æ´»å€¼
- all_layer_wise_activationsæ”¶é›†æ‰€æœ‰å±‚æœ€åä¸€ä¸ªtokençš„hiddenæ¿€æ´»å€¼ã€‚
- all_head_wise_activationsæ”¶é›†batch\[0]çš„æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰æ³¨æ„åŠ›å¤´æ¿€æ´»å€¼ã€‚
```python
    # prompt (1,seq_len)
    for prompt in tqdm(prompts):
        # è°ƒç”¨get_llama_activations_pyveneå‡½æ•°ï¼Œä¼ å…¥å¯å¹²é¢„æ¨¡å‹ã€æ”¶é›†å™¨ã€å½“å‰æç¤ºå’Œè®¾å¤‡ä¿¡æ¯
        # å±‚çº§åˆ«æ¿€æ´»å€¼ã€å¤´éƒ¨çº§åˆ«æ¿€æ´»å€¼
        layer_wise_activations, head_wise_activations, _ = get_llama_activations_pyvene(collected_model, collectors, prompt, device)
        # [å±‚ç´¢å¼•, tokenç´¢å¼•, ç‰¹å¾ç»´åº¦]
        # ä½¿ç”¨åˆ‡ç‰‡[:,-1,:]æå–æ‰€æœ‰å±‚çš„æœ€åä¸€ä¸ªtokenä½ç½®çš„æ¿€æ´»å€¼
        # æ³¨æ„è¿™ä¸ªç»´åº¦å°±æ²¡äº†
        # layer_wise_activations shape = (33, 25, 4096) 
        # layer_wise_activations[:,-1,:] shapeæ˜¯ [layer_num, hidden_dim]
        all_layer_wise_activations.append(layer_wise_activations[:,-1,:].copy())
        # å› ä¸ºä½¿ç”¨collectoræ”¶é›†çš„å°±æ˜¯last tokençš„æ³¨æ„åŠ›å¤´çš„æ¿€æ´»å€¼åˆ†æ•°
        # ç»´åº¦ï¼š[layer_num,hidden_dim]  head_num*D_head
        all_head_wise_activations.append(head_wise_activations.copy())
```

#### 3.1 get_llama_activations_pyvene
å°†promptæ”¾å…¥deviceï¼Œç„¶åæ”¾å…¥éœ€è¦æ”¶é›†çš„æ¨¡å‹ä¸­ï¼Œæ˜¾å¼è¾“å‡ºhidden_statesã€‚
- å°†æ‰€æœ‰çš„hidden_statesæŒ‰è¡Œè¿›è¡Œstackå †å ï¼Œç„¶åå»æ‰ç»´åº¦ä¸º0çš„å†—ä½™éƒ¨åˆ†ï¼Œç„¶åä¸¢åˆ°cpuä¸Šã€‚
- éå†æ”¶é›†å™¨æ¥æ”¶é›†çš„æ˜¯b\[0,-1\]ï¼Œå³batchä¸­ç¬¬ä¸€ä¸ªçš„æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰æ³¨æ„åŠ›å¤´ã€‚
	-  collector.collect_stateé»˜è®¤éƒ½æ˜¯trueï¼Œcollected_model()è¿è¡Œåï¼Œæ¯ä¸ªlayeræ¿€æ´»å€¼éƒ½æ”¾åœ¨å¯¹åº”çš„collector.statesåˆ—è¡¨ä¸­ï¼Œç„¶åæŒ‰è¡Œè¿›è¡Œå †å ,ç„¶åç§»åŠ¨åˆ°cpuä¸Š,é€šè¿‡numpy()å°†å¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä¾¿äºåç»­å¤„ç†ï¼Œæ”¾åˆ°head_wise_hidden_statesåˆ—è¡¨ä¸­ã€‚
	- é‡ç½®collectorå®ä¾‹â€”â€”æ¸…ç©ºcollector.statesåˆ—è¡¨
- å°†head_wise_hidden_statesåˆ—è¡¨çš„æ‰€æœ‰npæ•°ç»„è½¬ä¸ºå¼ é‡ï¼ŒæŒ‰è¡Œå †å ã€‚
- è¿”å›hidden_states,head_wise_hidden_states,mlpè¾“å‡º(ç©º)
```python
get_llama_activations_pyvene
    def get_llama_activations_pyvene(collected_model, collectors, prompt, device):
    with torch.no_grad():
        prompt = prompt.to(device)
        output = collected_model({"input_ids": prompt, "output_hidden_states": True})[1]
    hidden_states = output.hidden_states
    hidden_states = torch.stack(hidden_states, dim = 0).squeeze()
    hidden_states = hidden_states.detach().cpu().numpy()
    head_wise_hidden_states = []
    for collector in collectors:
        if collector.collect_state: \#é»˜è®¤éƒ½æ˜¯True 
            states_per_gen = torch.stack(collector.states, axis=0).cpu().numpy()
            head_wise_hidden_states.append(states_per_gen)
        else:
            head_wise_hidden_states.append(None)
        collector.reset()
    mlp_wise_hidden_states = []
    head_wise_hidden_states = torch.stack([torch.tensor(h) for h in head_wise_hidden_states], dim=0).squeeze().numpy()
    return hidden_states, head_wise_hidden_states, mlp_wise_hidden_states
```
