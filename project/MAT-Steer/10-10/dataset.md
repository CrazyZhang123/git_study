
bbq
```python
            # ä»ç£ç›˜åŠ è½½BBQæ•°æ®é›†
            file_path = '/webdav/Storage(default)/MyData/datasets/BBQ'
            datasets = load_from_disk(file_path)

            # # éå†BBQæ•°æ®é›†çš„æ‰€æœ‰å­ç±»åˆ«ï¼ˆå¦‚ageã€gender_identityç­‰ï¼‰
            # for category in datasets.keys():
            #     # è·å–å½“å‰ç±»åˆ«çš„æ•°æ®åˆ†ç‰‡ï¼ˆå¦‚datasets['age']ï¼‰
            # category = 'age'
            # 
            TOTAL_SAMPLES = 5000
            categories = list(datasets.keys())
            samples_per_category = TOTAL_SAMPLES // len(categories)
            print(f"å…±æœ‰ {len(categories)} ä¸ªç±»åˆ«ï¼Œæ¯ç±»é‡‡æ · {samples_per_category} æ¡ï¼Œæ€»è®¡çº¦ {TOTAL_SAMPLES} æ¡æ ·æœ¬")

            processed_data = []

            # ========== 5. éå†æ¯ä¸ªåˆ†ç±»å¹¶å‡è¡¡é‡‡æ · ==========
            for category in categories:
                data = datasets[category]
                print(f"\nğŸ“‚ å¤„ç†ç±»åˆ«: {category}ï¼Œæ ·æœ¬æ€»æ•°: {len(data)}")

                category_samples = []

                for item in data:
                    context = item.get("context", "")
                    question = item.get("question", "")
                    base_text = f"{context} {question}" if context else question
                    base_text = base_text.strip()

                    # æå–3ä¸ªé€‰é¡¹
                    answers = [item.get(f"ans{i}", "").strip() for i in range(3)]
                    correct_label = item.get("answer_label", item.get("label", 0))

                    # è·³è¿‡æ— æ•ˆæ ·æœ¬
                    if not isinstance(correct_label, int) or correct_label not in [0, 1, 2]:
                        continue

                    # éå†é€‰é¡¹
                    for i, answer in enumerate(answers):
                        if not answer:
                            continue
                        full_text = format_bbq(base_text, answer)
                        label = 1 if i == correct_label else 0
                        category_samples.append({
                            "text": full_text,
                            "label": label,
                            "category": category
                        })

                # ä»è¯¥ç±»åˆ«ä¸­éšæœºé‡‡æ ·å›ºå®šæ•°é‡
                if len(category_samples) > samples_per_category:
                    category_samples = random.sample(category_samples, samples_per_category)
                else:
                    print(f"âš ï¸ ç±»åˆ« {category} æ ·æœ¬ä¸è¶³ï¼Œä»… {len(category_samples)} æ¡")

                processed_data.extend(category_samples)

```

toxgen
```python
 elif dataset_name == "toxigen":
            # Expect format with text/prompt and toxicity_score or label
            version = 'annotations'
            file_path = f'/webdav/Storage(default)/MyData/datasets/toxigen/{version}'
            # ä»ç£ç›˜åŠ è½½æ•°æ®é›†
            datasets = load_from_disk(file_path)
            import re
            import numpy as np
            from sklearn.metrics import accuracy_score

            def fix_bytes_prefix(s):
                """å»é™¤ b'...' å‰ç¼€å¹¶è§£ç """
                if not isinstance(s, str):
                    return s
                s = s.strip()
                if s.startswith("b'") or s.startswith('b"'):
                    s = s[2:]
                    if (s.endswith("'") or s.endswith('"')):
                        s = s[:-1]
                    # è§£ç  \\\\n ç­‰è½¬ä¹‰å­—ç¬¦
                    s = bytes(s, "utf-8").decode("unicode_escape")
                return s.strip()

            def convert_toxigen_to_mcq(dataset, text_field='Input.text', label_field='Input.prompt_label'):
                """
                å°† Toxigen æ•°æ®é›†è½¬æ¢ä¸ºå¤šé€‰é¢˜ï¼ˆå•é€‰ï¼‰æ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªé—®é¢˜å¤šä¸ªé€‰é¡¹
                """
                # 1. æ”¶é›†æ‰€æœ‰æ ‡ç­¾
                labels = set()
                for item in dataset:
                    if label_field in item and item[label_field] is not None:
                        labels.add(item[label_field].strip())
                # å¦‚æœæ ‡ç­¾å°‘äº2ä¸ªï¼Œåˆ™è¡¥ä¸€ä¸ª neutral ä»¥ç¡®ä¿æ˜¯å¤šé€‰é¢˜
                if len(labels) < 2:
                    labels.add("neutral")
                label_set = sorted(list(labels))
                label2id = {l: i for i, l in enumerate(label_set)}

                examples = []
                for item in dataset:
                    if text_field not in item or label_field not in item:
                        continue
                    text = fix_bytes_prefix(item[text_field])
                    label = item[label_field].strip()
                    if label not in label2id:
                        continue
                    examples.append({
                        "question": text,
                        "choices": label_set,
                        "label": label2id[label],
                        "raw_label": label
                    })
                return examples, label2id

            def format_truthfulqa(question, choice):
                return f"Q: {question} A: {choice}"

            def build_mcq_records(examples):
                """
                å°†æ¯é“é¢˜è½¬æ¢ä¸ºå¤šæ¡è®°å½•ï¼šæ¯ä¸ªé€‰é¡¹ä¸€æ¡
                """
                processed_data = []
                for ex in examples:
                    q = ex["question"]
                    choices = ex["choices"]
                    correct_label = ex["label"]
                    for i, choice in enumerate(choices):
                        label = 1 if i == correct_label else 0
                        processed_data.append({
                            "text": format_truthfulqa(q, choice),
                            "label": label
                        })
                return processed_data

            # ========== ç¤ºä¾‹ ==========

            # å‡è®¾ä½ å·²æœ‰ HuggingFace DatasetDictï¼š
            # dataset = datasets['train']  # ä¾‹å¦‚ HuggingFace çš„æ•°æ®é›†å¯¹è±¡
            # è‹¥ä¸æ˜¯åˆ—è¡¨ï¼Œå¯è½¬æ¢ï¼š dataset = dataset.to_list()
            dataset = datasets['train'] # ä¿è¯æ˜¯ list

            examples, label2id = convert_toxigen_to_mcq(dataset)

            # éšæœºæŠ½æ ·
            sample_size = min(5000, len(examples))
            random_indices = np.random.choice(len(examples), size=sample_size, replace=False)
            sampled_examples = [examples[i] for i in random_indices]

            # è½¬æ¢ä¸º Q/A æ ¼å¼
            processed_data = build_mcq_records(sampled_examples)
```